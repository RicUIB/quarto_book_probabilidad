{
  "hash": "0a792ea8c58a7fe2cab0b68f05fe94ae",
  "result": {
    "markdown": "# Variables Aleatorias. Complementos\n\n## Momentos de variables aleatorias\n\n### Momento de orden $n$\n\n<l class=\"definition\"> **Definición.** </l> \nSea $X$ una variable aleatoria. Definimos el **momento de orden $n$** como \n$m_n = E\\left(X^n\\right)$.\n\n<l class=\"observ\"> **Observación.**</l>\nEl momento de orden $1$ de una variable aleatoria es su valor medio o $E(X)$.\n\nLos momentos de orden $n$ caracterizan una variable $X$. O sea, que si conocemos todos los momentos de orden $n$, podemos deducir cuál es la distribución de $X$.\n\nEn general, el cálculo de los momentos de orden $n$ para una variable $X$ es bastante tedioso. \n\n**Ejemplos de momentos de orden $n$**\n\n<div class=\"example\">\n**Ejemplo: momento de orden $n$ de una variable de Bernoulli de parámetro $p$**\n\nSea $X$ una variable de Bernoulli de parámetro $p$. Recordemos que su función de probabilidad es:\n$$\nP_X(0)=q=1-p,\\ p_X(1)=p.\n$$\nSu momento de orden $n$ será:\n$$\nm_n = E\\left(X^n\\right)=p\\cdot 1^n+(1-p)\\cdot 0^n = p.\n$$\nEn este caso, todos los momentos de orden $n$ valen $p$.\n</div>\n\n<div class=\"example\">\n**Ejemplo: momento de orden $n$ de una variable exponencial de parámetro $\\lambda$**\n\nConsideremos ahora una variable $X$ exponencial de parámetro $\\lambda$.\n\nRecordemos que su función de densidad es: $f_X(x)=\\lambda \\mathrm{e}^{-\\lambda x},$ para $x\\geq 0$ y $0$, en caso contrario.\n\nSu momento de orden $n$ será:\n$$\nm_n = E\\left(X^n\\right)=\\int_0^\\infty \\lambda \\mathrm{e}^{-\\lambda x} x^n\\, dx =\\frac{n!}{\\lambda^n}.\n$$\n\nLa expresión anterior se puede obtener integrando por partes $n$ veces y resolviendo los límites correspondientes. Dejámos al lector los cálculos correspondientes.\n\nFijémonos que los momentos de orden $n$ tienden a infinito a medida que $n$ crece: $\\lim\\limits_{n\\to\\infty}m_n = \\lim\\limits_{n\\to\\infty}\\frac{n!}{\\lambda^n}=\\infty$.\n\n</div>\n\n<div class=\"example\">\n**Ejemplo: momento de orden $n$ de una variable normal de parámetros $m=0$ y $\\sigma =1$**\n\nRecordemos que su función de densidad es: $f_X(x)=\\frac{1}{\\sqrt{2\\pi}}\\mathrm{e}^{-\\frac{x^2}{2}},$ para $x\\in \\mathbb{R}$.\n\nSu momento de orden 1 será la esperanza de $X$: $m_1 = 0$ i su momento de orden 2 será: \n$m_2 = E\\left(X^2\\right)=\\int_{-\\infty}^\\infty \\frac{1}{\\sqrt{2\\pi}}\\mathrm{e}^{-\\frac{x^2}{2}}\\cdot x^2\\, dx = 1.$\nLa integral anterior se resuelve usando técnicas de integrales de dos variables. \nDicho valor también se puede obtener usando que su varianza vale 1:\n$m_2 = \\mathrm{Var}(X)+E(X)^2 = \\sigma^2 +0^2 = 1.$\n\nLos momentos de orden impar $n$ serán cero ya que integramos una función impar:\n$m_n = E\\left(X^n\\right)=\\int_{-\\infty}^\\infty \\frac{1}{\\sqrt{2\\pi}}\\mathrm{e}^{-\\frac{x^2}{2}}\\cdot x^n\\, dx = 0.$\nO sea, si consideramos $g(x)=\\frac{1}{\\sqrt{2\\pi}}\\mathrm{e}^{-\\frac{x^2}{2}}\\cdot x^n$, se verifica $g(-x)=-g(x)$, para todo $x\\in\\mathbb{R}$.\n\nSi intentamos calcular el momento de orden 4, obtenemos:\n$m_4 = E\\left(X^4\\right)=\\int_{-\\infty}^\\infty \\frac{1}{\\sqrt{2\\pi}}\\mathrm{e}^{-\\frac{x^2}{2}}\\cdot x^4\\, dx = 3,$ usando técnicas de integración de dos variables otra vez.\n\n</div>\n\n\n### Momento central de orden $n$ \n\n<l class=\"definition\"> **Definición.** </l> \nSea $X$ una variable aleatoria. Definimos el **momento central de orden $n$** como \n$\\mu_n = E\\left((X-\\mu)^n\\right)$, donde $\\mu =E(X)$ es la media o la esperanza de la variable aleatoria $X$.\n\n<l class=\"observ\"> **Observación.**</l>\nEl momento central de orden $1$ de una variable aleatoria es siempre 0:\n$$\n\\mu_1 = E\\left((X-\\mu)\\right)=E(X)-E(\\mu)=E(X)-E(X)=0.\n$$\n\n<l class=\"observ\"> Observación.</l>\nEl momento central de orden $2$ de una variable aleatoria es la varianza:\n$$\n\\mu_2 = E\\left((X-\\mu)^2\\right):= \\mathrm{Var}(X).\n$$\n\nLos momentos centrales de orden $n$ caracterizan también una variable $X$. O sea, que si conocemos todos los momentos centrales de orden $n$, podemos deducir cuál es la distribución de $X$. \n\n<l class=\"prop\"> **Proposición.**</l>\nLa relación que hay entre los momentos centrales y los momentos de una variable aleatoria es la siguiente:\n$$\n\\mu_n = \\sum_{k=0}^n (-1)^{n-k} \\binom{n}{k} \\mu^{n-k} m_k = \\sum_{k=0}^n (-1)^{k} \\binom{n}{k} \\mu^{k} m_{n-k},\n$$\ndonde $\\mu =E(X)$ recordemos que es la esperanza de la variable aleatoria $X$.\n\n<div class=\"dem\">\n**Demostración**\n\nRecordemos la definición de momento central de orden $n$ y desarrollemos su expresión aplicando el **binomio de Newton**:\n$$\n\\mu_n = E\\left((X-\\mu)^n\\right) =E\\left(\\sum_{k=0}^n (-1)^{n-k} \\binom{n}{k} X^k\\mu^{n-k}\\right).\n$$\nAplicando la propiedad de la esperanza que la esperanza de la suma es la suma de esperanzas, obtenemos la expresión dada por la proposición:\n$$\n\\mu_n =\\sum_{k=0}^n (-1)^{n-k} \\binom{n}{k} \\mu^{n-k} E\\left(X^k\\right) = \\sum_{k=0}^n (-1)^{n-k} \\binom{n}{k} \\mu^{n-k} m_k.\n$$\n</div>\n\n\n**Ejemplos de momentos centrales de orden $n$**\n\n<div class=\"example\">\n**Ejemplo: momento central de orden $n$ de una variable de Bernoulli de parámetro $p$**\n\nSea $X$ una variable de Bernoulli de parámetro $p$. Recordemos que su función de probabilidad es:\n$$\nP_X(0)=q=1-p,\\ P_X(1)=p.\n$$\nUsando que $E(X)=p$, su momento central de orden $n$ será:\n$$\n\\mu_n = E\\left((X-p)^n\\right)=p\\cdot (1-p)^n+(1-p)\\cdot (0-p)^n = p(1-p)^n + (-1)^n (1-p) p^n.\n$$\n</div> \n\n<div class=\"exercise\">\n**Ejercicio**\n\nDemostrar que la expresión anterior corresponde a un polinomio de grado $n$.\n</div>\n\n\n<div class=\"example\">\n**Ejemplo: momento central de orden $n$ de una variable exponencial de parámetro $\\lambda$**\n\nConsideremos ahora una variable $X$ exponencial de parámetro $\\lambda$.\n\nRecordemos que su función de densidad es: $f_X(x)=\\lambda \\mathrm{e}^{-\\lambda x},$ para $x\\geq 0$.\n\nUsando que $E(X)=\\frac{1}{\\lambda}$, su momento central de orden $n$ será:\n$$\n\\mu_n = E\\left(\\left(X-\\frac{1}{\\lambda}\\right)^n\\right)=\\int_0^\\infty \\lambda \\mathrm{e}^{-\\lambda x} \\left(x-\\frac{1}{\\lambda}\\right)^n\\, dx =\\frac{a_n}{\\lambda^n},\n$$\ndonde $a_n = n!\\sum\\limits_{k=0}^n \\frac{(-1)^k}{k!}.$\n\n\nLa expresión anterior fijado $n$ se puede obtener integrando por partes $n$ veces y resolviendo los límites correspondientes. Dejámos al lector los cálculos correspondientes. Sin embargo, la obtención de la fórmula general para $n$ se sale del nivel del curso.\n\nFijémonos que los momentos centrales de orden $n$ también tienden a infinito a medida que $n$ crece: $\\lim\\limits_{n\\to\\infty}\\mu_n = \\lim\\limits_{n\\to\\infty}\\frac{a_n}{\\lambda^n}=\\infty$: \n$$\n\\lim_{n\\to\\infty}\\mu_n =\\lim_{n\\to\\infty} \\frac{n!\\sum\\limits_{k=0}^n \\frac{(-1)^k}{k!}}{\\lambda^n}= \n\\lim_{n\\to\\infty}\\sum\\limits_{k=0}^n \\frac{(-1)^k}{k!}\\cdot \\lim_{n\\to\\infty} \\frac{n!}{\\lambda^n}= \\mathrm{e}^{-1}\\cdot \\infty = \\infty.\n$$\n</div>\n\n<div class=\"example\">\n**Ejemplo: momento central de orden $n$ de una variable normal de parámetros $\\mu$ y $\\sigma$**\n\nRecordemos que su función de densidad es: $f_X(x)=\\frac{1}{\\sqrt{2\\pi}\\sigma}\\mathrm{e}^{-\\frac{(x-\\mu)^2}{2\\sigma^2}},$ para $x\\in \\mathbb{R}$.\n\nSu momento central de orden 2 será la varianza $\\sigma^2$: $\\mu_2 =\\sigma^2.$\n\nLos momentos centrales de orden impar $n$ serán cero ya que integramos una función impar respecto $x=\\mu$:\n$\\mu_n = E\\left((X-\\mu)^n\\right)=\\int_{-\\infty}^\\infty \\frac{1}{\\sqrt{2\\pi}\\sigma}\\mathrm{e}^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\cdot (x-\\mu)^n\\, dx = 0.$\nO sea, si consideramos $g(x)=\\frac{1}{\\sqrt{2\\pi}\\sigma}\\mathrm{e}^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\cdot (x-\\mu)^n$, se verifica $g(\\mu-x)=-g(\\mu +x)$, para todo $x\\in\\mathbb{R}$.\n\nSi intentamos calcular el momento central de orden 4, obtenemos:\n$\\mu_4 = E\\left((X-\\mu)^4\\right)=\\int_{-\\infty}^\\infty \\frac{1}{\\sqrt{2\\pi}\\sigma}\\mathrm{e}^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\cdot (x-\\mu)^4\\, dx = 3\\sigma^4.$ La integral anterior puede resolverse con el cambio de variable $t=\\frac{x-\\mu}{\\sigma}$ y usando que:  $\\int_{-\\infty}^\\infty \\frac{1}{\\sqrt{2\\pi}}\\mathrm{e}^{-\\frac{x^2}{2}}\\cdot x^4\\, dx = 3.$\n</div>\n\n## Asimetría de una variable aleatoria\n\n\nUna variable aleatoria tiene **asimetría positiva** si su función de densidad o de probabilidad presenta una cola a la \n**derecha** y **asimetría negativa**, si su función de densidad o de probabilidad presenta cola a la **izquierda**.\n\nPor ejemplo, en la figura siguiente, vemos la gráfica de la función de probabilidad de una variable aleatoria que presenta **asimetría negativa** a la izquierda y una función de densidad de una variable aleatoria que presenta **asimetría positiva** a la derecha:\n\n<div class=\"center\">\n\n::: {.cell fig='true'}\n::: {.cell-output-display}\n![](4_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n</div>\n\n**¿Cómo calcular la asimetría de una variable aleatoria?**\n\nLa asimetría de una variable aleatoria $X$ se calcula a partir de sus momentos centrales de segundo y tercer orden:\n$$\n\\gamma_1 = E\\left({\\left(\\frac{X-\\mu}{\\sigma}\\right)}^3\\right)=\\frac{\\mu_3}{\\sigma^3},\n$$\ndonde $\\mu = E(X)$ y $\\sigma^2 =\\mathrm{Var}(X)$.\n\nDicho valor se denomina **coeficiente de asimetría de Pearson**.\n\n\nUsando la relación ya vista entre los momentos centrales y los momentos, podemos expresar el **coeficiente de asimetría** en función de los momentos:\n$$\n\\gamma_1 = \\frac{m_3 -3\\mu\\sigma^2-\\mu^3}{\\sigma^3}.\n$$\nDejamos al lector la comprobación de la expresión anterior.\n\nPor tanto, una variable aleatoria $X$ tendrá simetría positiva o a la derecha si $\\gamma_1 >0$ y tendrá asimetría negativa o a la izquierda, si $\\gamma_1 <0$.\n\n<div class=\"example\">\n**Ejemplo: cálculo del coeficiente de asimetría para una variable de Bernoulli de parámetro $p$**\n\nSea $X$ una variable de Bernoulli de parámetro $p$. Usando que $m_n =p$, para todo $n$ y que $\\mu_2 = \\sigma^2 = p-p^2$, el coeficiente de asimetría $\\gamma_1$ será:\n$$\n\\gamma_1 = \\frac{p-3p(p-p^2)-p^3}{\\sqrt{(p-p^2)^3}} = \\frac{p (1-p) (1-2p)}{{\\sqrt{(p-p^2)^3}}}.\n$$\nPor tanto, la variable de Bernoulli de parámetro $p$ tendrá simetria negativa si $p>\\frac{1}{2}$ y positiva, si $p<\\frac{1}{2}$:\n\n<div class=\"center\">\n\n::: {.cell fig='true'}\n::: {.cell-output-display}\n![](4_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n</div>\n</div>\n\n<div class=\"example\">\n**Ejemplo: cálculo del coeficiente de asimetría para una variable exponencial de parámetro $\\lambda$**\n\nSea $X$ una variable exponencial de parámetro $\\lambda$. \nUsando que $\\sigma^2=\\frac{1}{\\lambda^2}$ y $\\mu_3 =\\frac{a_3}{\\lambda^3}=\\frac{2}{\\lambda^3}$, su coeficiente de asimetría de Pearson será:\n$\\gamma_1 = \\frac{\\frac{2}{\\lambda^3}}{\\frac{1}{\\lambda^3}}=2.$\n\nEntonces presenta asimetría positiva o a la derecha tal como se observa en su función de densidad:\n\n<div class=\"center\">\n\n::: {.cell fig='true'}\n::: {.cell-output-display}\n![](4_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n</div>\n</div>\n\n<div class=\"example\">\n**Ejemplo: cálculo del coeficiente de asimetría para una variable normal de parámetros $\\mu$ y $\\sigma$**\n\nSea $X$ una variable aleatoria normal de parámetros $\\mu$ y $\\sigma$. \n\nTal como se ha indicado anteriormente, los momentos centrales de orden impar son nulos. \n\nPor tanto, en este caso $\\mu_3=0$ y, por tanto, $\\gamma_1=0$. \n\nDeducimos que la distribución normal es totalmente simétrica.\n\nDe hecho, usando que su función de densidad es $f_X(x)=\\frac{1}{\\sqrt{2\\pi}\\sigma}\\mathrm{e}^{-\\frac{(x-\\mu)^2}{2\\sigma^2}},$ para $x\\in \\mathbb{R}$, se puede comprobar que $f_X(\\mu-x)=f_X(\\mu +x)$, o sea, tiene el eje de simetría $x=\\mu$:\n\n<div class=\"center\">\n\n::: {.cell fig='true'}\n::: {.cell-output-display}\n![](4_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n</div>\n\n</div>\n\n## Curtosis o apuntamiento de una variable aleatoria\n\nLa curtosis de una variable aleatoria $X$ es una medida de cómo son las colas de su función de densidad. \n\nDicho en otras palabras, queremos medir de alguna manera la *tendencia* que tiene la variable aleatoria a tener valores atípicos o *outliers*.\n\nLa manera estándard de medir la curtosis de una variable aleatoria $X$ es a partir de su **momento central de cuarto orden**:\n$$\n\\gamma_2 = E\\left(\\left(\\frac{X-\\mu}{\\sigma}\\right)^4\\right) = \\frac{\\mu_4}{\\sigma^4},\n$$\ndonde recordemos que $\\mu=E(X)$ y $\\sigma^2 =\\mathrm{Var}(X)$.\n\nA la expresión anterior se le denomina **medida de curtosis de Pearson**.\n\n* Diremos que una variable aleatoria no tiene exceso de curtosis o **mesocúrtica** si $\\gamma_2 \\approx 3$.\n\n* Diremos que una variable aleatoria tiene exceso positivo de curtosis o **leptocúrtica** si $\\gamma_2 >3$.\n\n* Diremos que una variable aleatoria tiene exceso negativo de curtosis o **platicúrtica** si $\\gamma_2 <3$.\n\n\n<div class=\"example\">\n**Ejemplo: cálculo del coeficiente de curtosis para una variable de Bernoulli de parámetro $p$**\n\nSea $X$ una variable aleatoria de parámetro $p$.\n\nEl momento central de cuarto orden de $X$ será:\n$$\n\\mu_4 = p (1-p)^4 +(1-p)p^4 = p (1-p) (3 p^2-3p+1).\n$$\nLa medida de curtosis de Pearson será:\n$$\n\\gamma_2 = \\frac{p (1-p) (3 p^2-3p+1)}{p^2 (1-p)^2} = \\frac{3 p^2-3p+1}{p(1-p)}.\n$$\nSe puede comprobar (ejercicio para el lector) que si $p\\in \\left(\\frac{3-\\sqrt{3}}{6},\\frac{3+\\sqrt{3}}{6}\\right)\\approx (0.211,0.789)$, $\\gamma_2 <3$ y, por tanto $X$ será platicúrtica y en caso contrario, si $p\\in \\left(0,\\frac{3-\\sqrt{3}}{6}\\right)\\cup \\left(\\frac{3+\\sqrt{3}}{6},1\\right)$, $\\gamma_2 >3$ y, por tanto, $X$ será leptocúrtica.\n</div>\n\n<div class=\"example\">\n\n**Ejemplo: cálculo del coeficiente de curtosis para una variable exponencial de parámetro $\\lambda$**\n\nSea $X$ una variable exponencial de parámetro $\\lambda$. \nUsando que $\\sigma^2=\\frac{1}{\\lambda^2}$ y $\\mu_4 =\\frac{a_4}{\\lambda^3}=\\frac{9}{\\lambda^4}$, su coeficiente de asimetría de Pearson será:\n$\\gamma_2 = \\frac{\\frac{9}{\\lambda^4}}{\\frac{1}{\\lambda^4}}=9.$\n\nComo $\\gamma_2 >3$, se trataría de una distribución leptocúrtica.\n\n</div>\n\n<div class=\"example\">\n**Ejemplo: cálculo del coeficiente de curtosis para una variable normal de parámetros $\\mu$ y $\\sigma$**\n\nSea $X$ una variable aleatoria normal de parámetros $\\mu$ y $\\sigma$. \n\nTal como se ha indicado anteriormente, el momento central de orden 4 vale: $\\mu_4 = 3\\sigma^4$.\n\nSu coeficiente de curtosis será:\n$$\n\\gamma_2 =\\frac{\\mu_4}{\\sigma^4}=\\frac{3\\sigma^4}{\\sigma^4}=3.\n$$\nDeducimos, por tanto, que toda distribución normal es mesocúrtica o no tiene exceso (ni positivo ni negativo) de curtosis.\n\n\n</div>\n\n## Métodos de transformación\n\nHemos visto anteriormente que el cálculo de los **momentos** o los **momentos centrados** de una variable aleatoria $X$ puede ser muy complicado y muy tedioso.\n\nPor dicho motivo, vamos a introducir un conjunto de funciones que nos permitirán calcular los **momentos** de la variable $X$ de forma relativamente sencilla.\n\n### Función generatriz de momentos\n\n<l class=\"definition\">**Definición de función generatriz de momentos:** </l>\nSea $X$ una variable aleatoria $X$ con función de probabilidad $P_X$ en el caso discreto o función\nde densidad $f_X$ en el caso continuo. \n\nSea $t\\in\\mathbb{R}$ un valor real cualquiera. \n\nDefinimos la función generatriz de momentos $m_X(t)$ en el valor $t$ como: $m_X(t)=E\\left(\\mathrm{e}^{tX}\\right).$\n\n<div class=\"example\">\n**Ejemplo: cálculo de la función generatriz de momentos para una variable de Bernoulli de parámetro $p$**\n\nSea $X$ una variable aleatoria de Bernoulli de parámetro $p$. Recordemos que su función de probabilidad es:\n$$\nP_X(0)=q=1-p,\\ p_X(1)=p.\n$$\nSu función generatriz de momentos será:\n$$\nm_X (t)=E\\left(\\mathrm{e}^{tX}\\right) =p\\mathrm{e}^{t\\cdot 1}+(1-p)\\mathrm{e}^{t\\cdot 0}=p\\mathrm{e}^t+(1-p)=1+p\\left(\\mathrm{e}^t -1 \\right).\n$$\n</div>\n\n<div class=\"example\">\n\n**Ejemplo: cálculo de la función generatriz de momentos para una variable exponencial de parámetro $\\lambda$**\n\nSea $X$ una variable aleatoria exponencial de parámetro $\\lambda$. Recordemos que su función de densidad es: $f_X(x)=\\lambda \\mathrm{e}^{-\\lambda x},$ para $x\\geq 0$ y $0$, en caso contrario.\n\nSu función generatriz de momentos será:\n$$\nm_X (t)=E\\left(\\mathrm{e}^{tX}\\right)=\\int_0^\\infty \\mathrm{e}^{t x}\\lambda \\mathrm{e}^{-\\lambda x}\\, dx = \\lambda \\int_0^\\infty\\mathrm{e}^{(t-\\lambda)x}\\, dx = \\lambda\\left[\\frac{\\mathrm{e}^{(t-\\lambda)x}}{t-\\lambda}\\right]_{x=0}^{x=\\infty} = \\frac{\\lambda}{\\lambda -t},\\ \\mbox{si } t<\\lambda. \n$$\nEn este caso vemos que el dominio de la función generatriz de momentos $m_X$ es $(-\\infty,\\lambda)$, ya que si $t\\geq \\lambda$, la integral anterior no es convergente.\n\nFijémonos por lo que vendrá más adelante que, como $\\lambda >0$, el valor $0$ pertenece al dominio de $m_X$.\n</div>\n\n<div class=\"example\">\n**Ejemplo: cálculo de la función generatriz de momentos para una variable normal de parámetros $\\mu$ y $\\sigma$**\n\nSea $X$ una variable normal de parámetros $\\mu$ y $\\sigma$. \n\nRecordemos que su función de densidad es: $f_X(x)=\\frac{1}{\\sqrt{2\\pi}\\sigma}\\mathrm{e}^{-\\frac{(x-\\mu)^2}{2\\sigma^2}},$ para $x\\in \\mathbb{R}$.\n\nSu función generatriz de momentos será:\n\n$$\n\\begin{array}{rl}\nm_X (t) & =E\\left(\\mathrm{e}^{tX}\\right)=\\displaystyle\\int_{-\\infty}^\\infty \\mathrm{e}^{tx}\\frac{1}{\\sqrt{2\\pi}\\sigma}\\mathrm{e}^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\, dx = \\frac{1}{\\sqrt{2\\pi}\\sigma} \\int_{-\\infty}^\\infty \\mathrm{e}^{tx-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\, dx \\\\[1ex]  & =  \\displaystyle\\frac{1}{\\sqrt{2\\pi}\\sigma} \\int_{-\\infty}^\\infty \\mathrm{e}^{-\\frac{1}{2\\sigma^2}\\left((x-(\\sigma^2 t+\\mu))^2-2\\sigma^2 t \\mu-\\sigma^4t^2\\right)}\\, dx = \\frac{1}{\\sqrt{2\\pi}\\sigma} \\mathrm{e}^{\\frac{1}{2}(2 t \\mu +\\sigma^2 t^2)}\\int_{-\\infty}^\\infty \\mathrm{e}^{-\\frac{1}{2\\sigma^2}(x-(\\sigma^2 t+\\mu))^2}\\, dx\\\\[1ex] &  = \\displaystyle\\mathrm{e}^{\\frac{1}{2}(2 t \\mu +\\sigma^2 t^2)} \\left( \\frac{1}{\\sqrt{2\\pi}\\sigma} \\int_{-\\infty}^\\infty \\mathrm{e}^{-\\frac{1}{2\\sigma^2}(x-(\\sigma^2 t+\\mu))^2}\\, dx\\right) =  \\mathrm{e}^{ t \\mu +\\frac{\\sigma^2 t^2}{2}}.\n\\end{array}\n$$\nLa integral del último paréntesis se resuelve haciento el cambio de variable $u=x-\\sigma^2 t$ y usando que la integral de la función de densidad de $X$ sobre todo $\\mathbb{R}$ vale 1.\n</div>\n\n**Relación entre la función generatriz de momentos y los momentos**\n\nLa razón del nombre que lleva la **función generatriz de momentos** es que podemos obtener todos los momentos de la variable a partir de ella:\n\n<l class=\"prop\"> **Proposición.** </l>\nSean $X$ una variable aleatoria con **función generatriz de momentos** $m_X(t)$. Entonces, el momento de orden $n$ de $X$ se puede obtener de la forma siguiente:\n$$\nm_n =E\\left(X^n\\right)=\\frac{d}{d t^n}m_X(t)|_{t=0} =m_X^{(n)}(0).\n$$\nO sea, el momento de orden $n$ de $X$ es la derivada $n$-ésima de la función generatriz de momentos evaluada en $t=0$.\n\n\n<div class=\"dem\">\n**Demostración**\n\nRecordemos la definición de la función generatriz de momentos: $m_X(t)=E\\left(\\mathrm{e}^{tX}\\right).$\n\nLa idea de la demostración es probar por inducción que $m_X^{(n)}(t) =E\\left(\\mathrm{e}^{tX}\\cdot X^n\\right)$.\n\nVeámoslo para $n=1$: $m_X'(t)=E\\left(\\mathrm{e}^{tX}\\cdot X\\right)$. \n\nSeguidamente, apliquemos inducción sobre $n$. Supongamos que $m_X^{(n)}(t) =E\\left(\\mathrm{e}^{tX}\\cdot X^n\\right)$ y veamos que $m_X^{(n+1)}(t) =E\\left(\\mathrm{e}^{tX}\\cdot X^{n+1}\\right)$:\n$m_X^{(n+1)}(t) =\\frac{d}{dt}(m_X^{(n)}(t)) =\\frac{d}{dt}E\\left(\\mathrm{e}^{tX}\\cdot X^n\\right) = E\\left(\\mathrm{e}^{tX}\\cdot X^{n+1}\\right),$\ntal como queríamos demostrar.\n\nAhora si aplicamos la expresión demostrada $m_X^{(n)}(t) =E\\left(\\mathrm{e}^{tX}\\cdot X^n\\right)$ a $t=0$, obtenemos:\n$m_X^{(n)}(0) =E\\left(X^n\\right)=m_n,$\ntal como dice la proposición.\n</div>\n\n<div class=\"example\">\n**Ejemplo: aplicación de la proposición en el caso en que $X$ es una variable de Bernoulli de parámetro $p$**\n\nEn este caso, recordemos que: $m_X (t)=1+p\\left(\\mathrm{e}^t -1 \\right).$\n\nSe puede comprobar que $m_X^{(n)}(t)=p\\mathrm{e}^t$. Por tanto:\n$$\nm_n = m_X^{(n)}(0)=p,\n$$\ntal como habíamos calculado anteriormente.\n</div>\n\n<div class=\"example\">\n\n**Ejemplo: aplicación de la proposición en el caso en que $X$ es una variable exponencial de parámetro $\\lambda$**\n\nEn este caso, recordemos que: $m_X (t)=\\frac{\\lambda}{\\lambda -t},$ para $t<\\lambda$ pero como $\\lambda >0$, $t=0$ cumple la expresión anterior.\n\nDejamos como ejercicio para el lector comprobar que: $m_X^{(n)}(t)=\\frac{\\lambda n!}{(\\lambda-t)^{n+1}}$.\n\nPor tanto: \n$$\nm_n = m_X^{(n)}(0) = \\frac{\\lambda n!}{\\lambda^{n+1}}=\\frac{n!}{\\lambda^n},\n$$\nexpresión que ya habíamos obtenido anteriormente.\n</div>\n\n<div class=\"example\">\n**Ejemplo: aplicación de la proposición en el caso en que $X$ es una variable normal de parámetros $\\mu$ y $\\sigma$**\n\nEn este caso, recordemos que: $m_X (t)=\\mathrm{e}^{ t \\mu +\\frac{\\sigma^2 t^2}{2}}.$\n\nAplicando la fórmula de los momentos para $n=1$ obtenemos:\n$m'(t)=\\mathrm{e}^{ t \\mu +\\frac{\\sigma^2 t^2}{2}} \\left(\\mu+t\\sigma^2\\right)$, que en $t=0$ vale:\n$m'(0)=\\mu=E(X)$, tal como ya sabemos.\n\nSi la aplicamos para $n=2$, obtenemos:\n$m''(t)=\\mathrm{e}^{ t \\mu +\\frac{\\sigma^2 t^2}{2}} \\left((\\mu+t\\sigma^2)^2+ \\sigma^2 \\right) =\\mathrm{e}^{ t \\mu +\\frac{\\sigma^2 t^2}{2}} \\left(t^2\\sigma^4+\\mu^2+\\sigma^2+ 2t\\mu\\sigma^2 \\right)$, que en $t=0$ vale:\n$m''(0)=\\mu^2+\\sigma^2=E\\left(X^2\\right)$, tal como ya sabemos.\n\nPara $n=3$m obtenemos:\n$m'''(t)=e^{\\mu  t+\\frac{\\sigma ^2 t^2}{2}}\\left(\\mu +\\sigma ^2 t\\right)\n\\left(\\left(\\mu +\\sigma ^2 t\\right)^2+3 \\sigma ^2\\right)$, que en $t=0$ vale: $m'''(0)=3\\sigma^2\\mu = E\\left(X^3\\right)$, valor que correspondería al momento de tercer orden de $X$.\n\nPor último, para $n=4$, obtenemos:\n$m^{(iv)}(t)=e^{\\mu  t+\\frac{\\sigma ^2 t^2}{2}}\n   \\left(6 \\sigma ^2 \\left(\\mu\n   +\\sigma ^2 t\\right)^2+\\left(\\mu\n   +\\sigma ^2 t\\right)^4+3 \\sigma\n   ^4\\right)$, que en $t=0$ vale: $m^{(iv)}(0)=6\\sigma^2\\mu^2+\\mu^4+3\\sigma^4=E\\left(X^4\\right)$, valor que correspondería al momento de cuarto orden de $X$.\n\n</div>\n\n### Función característica\n<l class=\"definition\">**Definición de función característica:** </l>\nSea $X$ una variable aleatoria $X$ con función de probabilidad $P_X$ en el caso discreto o función\nde densidad $f_X$ en el caso continuo. \n\nSea $w\\in\\mathbb{R}$ un valor real cualquiera. \n\nDefinimos la función característica $\\phi_X(w)$ en el valor $w$ como: $\\phi_X(w)=E\\left(\\mathrm{e}^{\\mathrm{i} w X}\\right),$  donde $\\mathrm{i}$ es el número complejo $\\mathrm{i}=\\sqrt{-1}$.\n\n<l class=\"observ\">Observación: </l>\nSi $X$ es una variable continua, la **función característica** $\\phi_X(w)$ puede interpretarse como la **transformada de Fourier** de la **función de densidad** de $X$:\n$\\phi(w)=\\int_{-\\infty}^\\infty f_X(x)\\mathrm{e}^{\\mathrm{i}w x}\\, dx.$\n\nPor tanto, usando la fórmula de la **antitransformada de Fourier**, podemos escribir la **función de densidad** $f_X(x)$ como función de la **función característica** de $X$, $\\phi(w)$:\n$f_X(x)=\\frac{1}{2\\pi}\\int_{-\\infty}^\\infty \\phi_X(w)\\mathrm{e}^{-\\mathrm{i}w x}\\, dw.$\n\n<l class=\"observ\">Observación: </l>\nEn el caso discreto, o sea, Si $X$ es una variable discreta, la **función característica** $\\phi_X(w)$ se escribe como función de la **función de probabilidad** $P_X(x_k)$ con **Dominio** $D_X=\\{x_k,\\ k\\}$ como:\n$\\phi(w)=\\sum_{k} P_X(x_k)\\mathrm{e}^{\\mathrm{i}w x_k}.$\n\nEn los casos en que los $x_k$ sean enteros, $x_k=k$, que son la mayoría, la ecuación anterior es la **tranformada de Fourier de la secuencia** $P_X(k)$. Dicha función es una *función periódica* en $w$ de periodo $2\\pi$ ya que $\\mathrm{e}^{\\mathrm{i}(w+2\\pi)k}=\\mathrm{e}^{\\mathrm{i}wk}.$\n\nPor tanto, usando la fórmula de **inversión**,  podemos escribir la **función de probabilidad** $P_X(k)$ como función de la función característica de $X$, $\\phi(w)$:\n$P_X(k)=\\frac{1}{2\\pi}\\int_{0}^{2\\pi} \\phi_X(w)\\mathrm{e}^{-\\mathrm{i}w k}\\, dw.$\n\n<div class=\"example\">\n**Ejemplo: cálculo de la función característica para una variable de Bernoulli de parámetro $p$**\n\nSea $X$ una variable aleatoria de Bernoulli de parámetro $p$. Recordemos que su función de probabilidad es:\n$$\nP_X(0)=q=1-p,\\ p_X(1)=p.\n$$\nSu función característica será:\n$$\n\\phi_X (w)=E\\left(\\mathrm{e}^{\\mathrm{i}wX}\\right) =p\\mathrm{e}^{\\mathrm{i}w\\cdot 1}+(1-p)\\mathrm{e}^{\\mathrm{i}w\\cdot 0}=p\\mathrm{e}^{\\mathrm{i}w}+(1-p)=1+p\\left(\\mathrm{e}^{\\mathrm{i}w} -1 \\right).\n$$\nComprobemos la fórmula de la inversión:\n$$\n\\begin{array}{rl}\nP_X(1) & = \\frac{1}{2\\pi}\\int_0^{2\\pi} \\left(1+p\\left(\\mathrm{e}^{\\mathrm{i}w} -1 \\right)\\right) e^{-\\mathrm{i}w\\cdot 1}\\, dw =\\frac{1}{2\\pi}\\left(\\int_0^{2\\pi} (1-p)e^{-\\mathrm{i}w}\\, dw + \\int_0^{2\\pi} p\\, dw\\right) \\\\ & = \\frac{1}{2\\pi}\\left( (1-p) \\left[\\frac{\\mathrm{e}^{-\\mathrm{i}w}}{-\\mathrm{i}}\\right]_0^{2\\pi} +2\\pi p\\right)=\\frac{1}{2\\pi}\\left((1-p)\\cdot 0 +2\\pi p\\right)=p, \\\\\nP_X(0) & = \\frac{1}{2\\pi}\\int_0^{2\\pi} \\left(1+p\\left(\\mathrm{e}^{\\mathrm{i}w} -1 \\right)\\right) e^{-\\mathrm{i}w\\cdot 0}\\, dw =\\frac{1}{2\\pi}\\left(\\int_0^{2\\pi} (1-p) \\, dw + \\int_0^{2\\pi} p \\mathrm{e}^{\\mathrm{i}w}\\, dw\\right) \\\\ & = \\frac{1}{2\\pi}\\left( (1-p) \\cdot 2\\pi  +p \\left[\\frac{\\mathrm{e}^{\\mathrm{i}w}}{\\mathrm{i}}\\right]_0^{2\\pi}\\right)=\\frac{1}{2\\pi}\\left((1-p)\\cdot 2\\pi + p\\cdot 0\\right)=1-p.\n\\end{array}\n$$\n</div>\n\n<div class=\"example\">\n\n**Ejemplo: cálculo de la función característica para una variable exponencial de parámetro $\\lambda$**\n\nSea $X$ una variable aleatoria exponencial de parámetro $\\lambda$. Recordemos que su función de densidad es: $f_X(x)=\\lambda \\mathrm{e}^{-\\lambda x},$ para $x\\geq 0$ y $0$, en caso contrario.\n\nSu función característica será:\n$$\n\\phi_X (w)=E\\left(\\mathrm{e}^{\\mathrm{i}wX}\\right)=\\int_0^\\infty \\mathrm{e}^{\\mathrm{i}w x}\\lambda \\mathrm{e}^{-\\lambda x}\\, dx = \\lambda \\int_0^\\infty\\mathrm{e}^{(\\mathrm{i}w-\\lambda)x}\\, dx = \\lambda\\left[\\frac{\\mathrm{e}^{(\\mathrm{i}w-\\lambda)x}}{\\mathrm{i}w-\\lambda}\\right]_{x=0}^{x=\\infty} = \\frac{\\lambda}{\\lambda -\\mathrm{i} w}. \n$$\nLa expresión anterior es válida para todo $w\\in\\mathbb{R}$ ya que su valor sería:\n$\\phi_X (w)=\\frac{\\lambda}{\\lambda -\\mathrm{i} w}\\cdot \\frac{\\lambda +\\mathrm{i} w}{\\lambda +\\mathrm{i} w}=\\frac{\\lambda^2+\\mathrm{i}\\lambda w}{\\lambda^2+w^2}=\\frac{\\lambda^2}{\\lambda^2+w^2}+\\mathrm{i}\\frac{\\lambda w}{\\lambda^2+w^2}.$\nEn la última expresión hemos separado la parte real de la imaginaria.\n\nCalculemos la función de densidad a partir de la función característica:\n$$\nf_X(x)=\\frac{1}{2\\pi}\\int_{-\\infty}^\\infty \\frac{\\lambda}{\\lambda -\\mathrm{i} w}\\mathrm{e}^{-\\mathrm{i}wx}\\, dw = a\\mathrm{e}^{-a x},\n$$\nsi $x>0$ y $0$ en caso contrario. El cálculo de la integral anterior debe realizarse usando el *Teorema de los Residuos*,  [Residue theorem](https://en.wikipedia.org/wiki/Residue_theorem) y se sale de los objetivos de este curso.\n</div>\n\n<div class=\"example\">\n\n**Ejemplo: cálculo de la función característica para una variable normal de parámetros $\\mu$ y $\\sigma$**\n\nSea $X$ una variable normal de parámetros $\\mu$ y $\\sigma$. \n\nRecordemos que su función de densidad es: $f_X(x)=\\frac{1}{\\sqrt{2\\pi}\\sigma}\\mathrm{e}^{-\\frac{(x-\\mu)^2}{2\\sigma^2}},$ para $x\\in \\mathbb{R}$.\n\nSu función característica será:\n\n$$\n\\begin{array}{rl}\n\\phi_X (w) & =\\displaystyle E\\left(\\mathrm{e}^{\\mathrm{i}w X}\\right)=\\int_{-\\infty}^\\infty \\mathrm{e}^{\\mathrm{i}w x}\\frac{1}{\\sqrt{2\\pi}\\sigma}\\mathrm{e}^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\, dx = \\frac{1}{\\sqrt{2\\pi}\\sigma} \\int_{-\\infty}^\\infty \\mathrm{e}^{\\mathrm{i}wx-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\, dx \\\\[1ex]  & =\\displaystyle  \\frac{1}{\\sqrt{2\\pi}\\sigma} \\int_{-\\infty}^\\infty \\mathrm{e}^{-\\frac{1}{2\\sigma^2}\\left((x-(\\sigma^2 \\mathrm{i}w+\\mu))^2-2\\sigma^2 \\mathrm{i}w \\mu+\\sigma^4 w^2\\right)}\\, dx = \\frac{1}{\\sqrt{2\\pi}\\sigma} \\mathrm{e}^{\\frac{1}{2}(2 \\mathrm{i}w \\mu -\\sigma^2 w^2)}\\int_{-\\infty}^\\infty \\mathrm{e}^{-\\frac{1}{2\\sigma^2}(x-(\\sigma^2 \\mathrm{i}w+\\mu))^2}\\, dx\\\\[1ex] &  = \\displaystyle\\mathrm{e}^{\\frac{1}{2}(2 \\mathrm{i}w \\mu -\\sigma^2 w^2)} \\left( \\frac{1}{\\sqrt{2\\pi}\\sigma} \\int_{-\\infty}^\\infty \\mathrm{e}^{-\\frac{1}{2\\sigma^2}(x-(\\sigma^2 \\mathrm{i}w+\\mu))^2}\\, dx\\right) =  \\mathrm{e}^{ \\mathrm{i}w \\mu -\\frac{\\sigma^2 w^2}{2}}.\n\\end{array}\n$$\nLa integral del último paréntesis se resuelve haciento el cambio de variable $u=x-\\sigma^2 \\mathrm{i}w$ y usando que la integral de la función de densidad de $X$ sobre todo $\\mathbb{R}$ vale 1.\n\nCalculemos la función de densidad a partir de la función característica:\n$$\n\\begin{array}{rl}\nf_X(x) & =\\displaystyle\\frac{1}{2\\pi}\\int_{-\\infty}^\\infty \\mathrm{e}^{ \\mathrm{i}w \\mu -\\frac{\\sigma^2 w^2}{2}}\\mathrm{e}^{-\\mathrm{i} w x}\\, dw = \\frac{1}{2\\pi}\\int_{-\\infty}^\\infty \\mathrm{e}^{\\left(\\frac{\\mathrm{i}w\\sigma}{\\sqrt{2}}+\\frac{\\mu-x}{\\sigma\\sqrt{2}}\\right)^2-\\frac{(\\mu-x)^2}{2\\sigma^2}}\\, dw =\\frac{1}{2\\pi}\\mathrm{e}^{-\\frac{(\\mu-x)^2}{2\\sigma^2}}\\int_{-\\infty}^\\infty \\mathrm{e}^{\\left(\\frac{\\mathrm{i}w\\sigma}{\\sqrt{2}}+\\frac{\\mu-x}{\\sigma\\sqrt{2}}\\right)^2}\\, dw \\\\[1ex] & =\\displaystyle \\frac{1}{2\\pi}\\mathrm{e}^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\int_{-\\infty}^\\infty \\mathrm{e}^{-\\left(\\frac{w\\sigma}{\\sqrt{2}}+\\frac{\\mu-x}{\\mathrm{i}\\sigma\\sqrt{2}}\\right)^2}\\, dw \\stackrel{\\mbox{cambio de variable } u=\\frac{w\\sigma}{\\sqrt{2}}+\\frac{\\mu-x}{\\mathrm{i}\\sigma\\sqrt{2}}}{=} \\frac{1}{2\\pi}\\mathrm{e}^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\int_{-\\infty}^\\infty \\frac{\\sqrt{2}}{\\sigma}\\mathrm{e}^{-u^2}\\, du \\\\[1ex] & \\displaystyle\\stackrel{\\int_{-\\infty}^\\infty \\mathrm{e}^{-u^2}\\, du =\\sqrt{\\pi}}{=} \\frac{1}{\\sqrt{2}\\pi\\sigma} \\mathrm{e}^{-\\frac{(x-\\mu)^2}{2\\sigma^2}} \\sqrt{\\pi} = \\frac{1}{\\sqrt{2\\pi}\\sigma}\\mathrm{e}^{-\\frac{(x-\\mu)^2}{2\\sigma^2}},\n\\end{array}\n$$\nfunción que coincide con la densidad de la distribución $N(\\mu,\\sigma)$.\n\n</div>\n\n**Relación entre la función característica y los momentos**\n\nLa relación entre la **función característica** y los **momentos** es la siguiente:\n\n<l class=\"prop\"> **Proposición.** </l>\nSean $X$ una variable aleatoria con **función característica** $\\phi_X(w)$. Entonces, el momento de orden $n$ de $X$ se puede obtener de la forma siguiente:\n$$\nm_n =E\\left(X^n\\right)=\\frac{1}{\\mathrm{i}^n}\\frac{d}{d w^n}\\phi_X(w)|_{w=0} =\\frac{1}{\\mathrm{i}^n}\\phi_X^{(n)}(0).\n$$\nO sea, el momento de orden $n$ de $X$ es la derivada $n$-ésima de la función característica evaluada en $w=0$ dividido por $\\mathrm{i}^n$.\n\n<div class=\"exercise\">\n**Ejercicio**\n\nLa demostración se realiza de forma similar a la demostración de la proposición que relaciona la función generatriz de momentos y los momentos. \n\nSe deja como ejercicio al lector.\n</div>\n\n\n<div class=\"exercise\">\n**Ejercicio**\n\nRealizar los mismos ejemplos que los realizados para la función generatriz de momentos. O sea:\n\n* Si $X$ es una variable de Bernoulli de parámetro $p$, demostrar usando la función característica que para todo $n$, $m_n = E\\left(X^n\\right)=p$.\n\n* Si $X$ es una variable exponencial de parámetro $\\lambda$, demostrar usando la función característica que para todo $n$, $m_n =  E\\left(X^n\\right)=\\frac{n!}{\\lambda^n}$.\n\n* Si $X$ es una variable normal de parámetros $\\mu$ y $\\sigma$, demostrar usando la función característica que $E(X)=\\mu$, $E\\left(X^2\\right)=\\mu^2+\\sigma^2$, $E\\left(X^3\\right)=3\\sigma^2\\mu$ y $E\\left(X^4\\right)=6\\sigma^2\\mu^2+\\mu^4+3\\sigma^4$.\n\n\n</div>\n\n\n## Fiabilidad\n\nSea $T\\geq 0$ una variable aleatoria que nos da, por ejemplo, el tiempo de vida de cierto componente o dispositivo.\n\nVamos a definir medidas para estudiar la fiabilidad de este tipo de variables aleatorias.\n\n\n\n<l class=\"definition\">**Definición:**</l>\nSea $T\\geq 0$ una variable aleatoria. La **fiabilidad** de $T$ en el tiempo $t$ se define como la probabilidad que el sistema, componente o dispositivo funcione en el tiempo $t$: $R(t)=P(T>t)$.\n\n<l class=\"observ\">**Observación:**</l>\nDada una variable $T\\geq 0$, la relación existente entre la **fiabilidad** $R$ y la **función de distribución** $F_T$ es la siguiente:\n$$\nR(t)=P(T>t)=1-P(T\\leq t)=1-F_T (t)\n$$\n\n### Tiempo medio de vida\n\n<l class=\"observ\">Observación:</l>\nDada una variable $T\\geq 0$ continua, el **tiempo medio de vida** de la variable $T$ sería $E(T)$. Entonces, este **tiempo medio de vida** se puede calcular como: \n$E(T)=\\int_0^\\infty R(t)\\, dt.$\n\nVeámoslo. Para ello basta ver que $E(T)=\\int_0^\\infty (1-F_T(t))\\, dt$, donde $F_T(t)$ es la función de distribución de la variable $T$:\n$$\n\\begin{array}{rl}\nE(T) & =\\displaystyle\\int_{t=0}^{t=\\infty} 1-F_T(t)\\, dt=\\int_{t=0}^{t=\\infty}\\int_{u=t}^{u=\\infty} f_T(u)\\,du\\,dt \\\\[1ex] & =\\displaystyle\\int_{u=0}^{u=\\infty} f_T(u)\\int_{t=0}^{t=u} \\, dt\\, du =\\int_{u=0}^{u=\\infty} f_T(u)\\cdot u\\, du = E(T),\n\\end{array}\n$$\ndonde $f_T(u)$ seria la función de densidad de la variable $T$ en el valor $u$.\n\n\n<div class=\"example\">\n**Ejemplo**\n\nSea $T$ una variable aleatoria exponencial de parámetro $\\lambda$.\n\nLa fiabilidad de $T$ sería: $R(t)=P(T>t)=1-F_T(t)=\\mathrm{e}^{-\\lambda t}$:\n<div class=\"center\">\n\n::: {.cell fig='true'}\n::: {.cell-output-display}\n![](4_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n</div>\n\n</div>\n\n## Generación de muestras de variables aleatorias por ordenador\n\nLa simulación por **computadora** de cualquier fenómeno aleatorio implica la **generación de variables aleatorias** con distribuciones prefijadas de antemano. \n\nPor ejemplo, la simulación de un sistema de colas implica generar el tiempo entre las llegadas de los clientes, así como los tiempos de servicio de cada cliente.\n\nFijémonos que fijar la variable aleatoria $X$ es equivalente a fijar la **función de distribución $F_X(x)$** o la **función de densidad $f_X(x)$** en el caso continuo o la **función de probabilidad $P_X(x)$** en el caso discreto.\n\nTodos los métodos que vamos a describir presuponen que podemos generar **números aleatorios** que se distribuyen **uniformemente** entre 0 y 1. En `R` se puede hacer usando la función `runif(n)`, donde `n` es la cantidad de números aleatorios entre 0 y 1 a generar.\n\n\n### Método de transformación\n\nEl **método de transformación** se basa en el resultado siguiente:\n\n<l class=\"prop\">**Proposición.** </l>\nSea $X$ una variable aleatoria con función de distribución $F_X(x)$. Supongamos que $F_X(x)$ es estrictamente creciente o que existe $F_X^{-1}(y)$, para todo $y\\in [0,1]$. Sea $Y$ la variable aleatoria definida como: $Y=F_X(X)$. Entonces la distribución de $Y$ es uniforme en el intervalo $[0,1]$.\n\n<div class=\"dem\">\n**Demostración:**\n\nClaramente, por propia definición de $Y$, tenemos que el dominio de $Y$ es $[0,1]$ ya que el conjunto recorrido de la función de distribución de cualquier variable es el intervalo $[0,1]$. \n\nPara ver que la distribución de $Y$ es $U[0,1]$ basta comprobar que $F_Y(y)=y$, para todo $y\\in [0,1]$:\n$$\n\\begin{array}{rl}\nF_Y(y) & =P(Y\\leq y)=P(F_X(X)\\leq y)\\stackrel{\\mbox{usando que $F_X$ es estrictamente creciente}}{=} P(X\\leq F_X^{-1}(y)) \\\\ & =F_X(F_X^{-1}(y))=y.\n\\end{array}\n$$\n</div>\n\nUsando la proposición anterior, dada una variable $X$, como la distribución de la variable aleatoria $Y=F_X(X)$ es $U[0,1]$, si hacemos $X=F_X^{-1}(Y)$, tendremos que si sabemos generar una muestra de $Y$, aplicándole a la muestra la función $F_X^{-1}$ tendremos generada una muestra de $X$.\n\n\n<div class=\"example\">\n**Ejemplo: generar una muestra de una variable exponencial de parámetro $\\lambda$**\n\nRecordemos que si $X$ es exponencial de parámetro $\\lambda$, su función de distribución es: $F_X(x)=1-\\mathrm{e}^{-\\lambda x}$.\n\nHallemos a continuación $F_X^{-1}$:\n$$\ny=1-\\mathrm{e}^{-\\lambda x},\\ \\Leftrightarrow 1-y=\\mathrm{e}^{-\\lambda x},\\ \\Leftrightarrow \\ln(1-y)=-\\lambda x,\\ \\Leftrightarrow x=-\\frac{1}{\\lambda}\\ln(1-y).\n$$\nPor tanto, $F_X^{-1}(y)=-\\frac{1}{\\lambda}\\ln(1-y)$.\n\nGeneremos una muestra con `R` de 25 valores de una variable exponencial de parámetro $\\lambda=2$ usando el método anterior:\n\n::: {.cell}\n\n```{.r .cell-code}\nn=25\nlambda=2\nmuestra.y = runif(n)\nmuestra.x = -(1/lambda)*log(1-muestra.y)\nmuestra.x\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] 0.13799140 0.15490036 0.46718000 0.24777673 0.37288593 0.53707384\n [7] 0.20424784 0.01341613 0.35239861 0.12185498 0.16318543 1.43255749\n[13] 1.46046044 1.66240461 0.21902599 0.64754837 0.36208134 1.00767113\n[19] 0.01420178 0.14325276 0.95543495 0.27688037 0.08044269 0.18568357\n[25] 0.45466954\n```\n:::\n:::\n\n\nVamos a testear si nuestro método funciona. \n\nPara ello generaremos una muestra de 500 valores usando el método de transformación y dibujaremos su **histograma de frecuencias relativas**.\n\nSeguidamente dibujaremos la **función de densidad de la variable exponencial de parámetro $\\lambda$** y compararemos los resultados:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn=500\nlambda=2\nmuestra.y = runif(n)\nmuestra.x = -(1/lambda)*log(1-muestra.y)\nhist(muestra.x,freq=FALSE,main=\"Histograma de la muestra\")\nx2=seq(from=0,to=2.5,by=0.01)\nlines(x2,dexp(x2,lambda),col=\"red\")\n```\n:::\n\n\n<div class=\"center\">\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](4_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n</div>\n\n</div>\n\n### Método de rechazo\n\nSea $X$ una variable aleatoria continua tal que su función de densidad verifica:\n\n* Existen valores $a$ y $b$ tal que $f_X(x)= 0$ si $x\\not\\in [a,b]$.\n* Existen valores $c$ y $d$ tal que $f_X(x)\\in [c,d]$, si $x\\in [a,b]$.\n\nEn resumen, los puntos $(x,f(x))$ pertenecen al rectángulo $[a,b]\\times [c,d]$ y en caso contrario $f_X(x)=0$.\n\nEn el gráfico siguiente, $a=0$, $b=2$, $c=0$ y $d=1$.\n\n<div class=\"center\">\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](4_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n</div>\n\nPara generar una **muestra aleatoria** de la variable $X$, hacemos lo siguiente:\n\n1) generamos un valor aleatorio $x$ entre $a$ y $b$.\n\n2) generamos un valor aleatorio $y$ entre $c$ y $d$.\n\n3) si $y\\leq f_X(x)$, aceptamos $x$ como valor de la muestra. En caso contrario, volvemos a empezar en 1.\n\n\n<div class=\"example\">\n**Ejemplo**\n\nEl gráfico de la figura anterior corresponde a la función de densidad siguiente:\n$$\nf_X(x)=\\begin{cases}\nx, & \\mbox{ si }0\\leq x\\leq 1,\\\\\n2-x, & \\mbox{ si }1\\leq x\\leq 2,\\\\\n0, & \\mbox{en caso contrario.}\n\\end{cases}\n$$\n\nVamos a generar una muestra de $25$ valores usando el **método del rechazo**:\n\n::: {.cell}\n\n```{.r .cell-code}\na=0; b=2; c=0; d=1; n=25; i=1;\nf = function(x){ifelse(x>=0 & x<=1,x,ifelse(x>=1&x<=2,2-x,0))}\nmuestra=c()\nwhile(i <=n){\n  x=runif(1,a,b)\n  y=runif(1,c,d)\n  if(y <= f(x)){muestra=c(muestra,x); i=i+1}\n}\nmuestra\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] 0.8934245 1.0241151 0.7485943 1.1318743 1.1174486 0.8956145 1.6270601\n [8] 0.6737312 0.9584882 0.5762514 0.7390811 0.4276671 1.5210227 0.8506008\n[15] 0.4126257 0.9897587 1.2166050 1.6098125 0.8727734 0.8756381 1.4853939\n[22] 1.1539542 1.2353779 0.5157756 1.2873756\n```\n:::\n:::\n\n\nComo hicimos con el ejemplo del **método de transformación**, vamos a generar una muestra de 500 valores de la variable $X$, vamos a dibujar el **histograma de frecuencias relativas** junto con la función de densidad para ver si ésta se aproxima a dicho histograma:\n\n::: {.cell}\n\n```{.r .cell-code}\na=0; b=2; c=0; d=1; n=500; i=1;\nf = function(x){ifelse(x>=0 & x<=1,x,ifelse(x>=1&x<=2,2-x,0))}\nmuestra=c()\nwhile(i <=n){\n  x=runif(1,a,b)\n  y=runif(1,c,d)\n  if(y <= f(x)){muestra=c(muestra,x); i=i+1}\n}\nhist(muestra,freq=FALSE,main=\"Histograma de la muestra\")\nx2=seq(from=0,to=2,by=0.01)\nlines(x2,f(x2),col=\"red\")\n```\n:::\n\n\n<div class=\"center\">\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](4_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n</div>\n</div>\n\n## Entropía\n\nLa **entropía** es una medida de la **incertidumbre** en un experimento aleatorio. \n\nVeremos cómo la **entropía** cuantifica la **incertidumbre** por la cantidad de **información** requerida para especificar el resultado de un experimento aleatorio. \n\n\n### Entropía de una variable aleatoria\nSupongamos que tenemos una variable aleatoria $X$ discreta con valores enteros: $D_X=\\{1,2,\\ldots,N\\}$.\n\nSea $k\\in D_X$ un valor de la variable. Estamos interesados en cuantificar la **incertidumbre** del suceso $A_k =\\{X=k\\}$. \n\nO sea, cuánta **menos incertidumbre** tenga $A_k$, más **alta será su probabilidad**, y cuánta **más incertidumbre**, **menos probabilidad** de aparecer $A_k$.\n\nUna medida que cumple las condiciones anteriores es la siguiente: $I(A_k)=I(\\{X=k\\})=\\ln\\left(\\frac{1}{P(X=k)}\\right)=-\\ln\\left(P(X=k)\\right).$\n\nPor ejemplo, si $P(A_k)=1$, o sea, $A_k$ aparece \"**seguro**\", entonces tiene incertidumbre **nula**, $I(A_k)=0$, y si $P(A_k)=0$, o sea, $A_k$ no aparece \"**nunca**\", tiene incertidumbre **máxima**, $I(A_k)=\\infty$.\n\nLa motivación anterior hace que definamos la **entropía** de una variable aleatoria de la forma siguiente:\n\n<l class=\"definition\">**Definición:**</l>\nSea $X$ una variable aleatoria con función de densidad $f_X(x)$ en el caso continuo o función de probabilidad $P_X(x)$ en el caso discreto. Definimos **entropía de X** como: \n$H_X = \\displaystyle E\\left(-\\ln(f_X)\\right)=\\int_{-\\infty}^\\infty -\\ln(f_X(x)) f_X(x)\\, dx,$ en el caso continuo y,\n$H_X = \\displaystyle E\\left(-\\ln(P_X)\\right)=\\sum_{x_k\\in D_X} -\\ln(P_X(x_k)) P_X(x_k),$ en el caso discreto.\n\n<div class=\"example\">\n**Ejemplo: entropía de una variable de Bernoulli de parámetro $p$**\n\nSea $X$ una variable de Bernoulli de parámetro $p$.\n\nRecordemos que su función de probabilidad $P_X$ es: $P_X(0)=1-p=q,$ $P_X(1)=p$.\n\nLa entropía de $X$ será:\n$$\nH_X = E\\left(-\\ln(P_X)\\right) = -(1-p)\\cdot \\ln(1-p)-p\\cdot \\ln p.\n$$\nEl gráfico de la entropía se puede observar en el gráfico siguiente donde $X$ tiene entropía máxima cuando $p=\\frac{1}{2}$ que sería cuando $X$ tiene incertidumbre máxima al tratar de adivinar el resultado de $X$ y $X$ tiene entropía mínima cuando $p=0$ o $p=1$ ya que en estos casos el resultado de $X$ sería siempre $0$ o $1$, respectivamente.\n\n<div class=\"center\">\n\n::: {.cell fig='true'}\n::: {.cell-output-display}\n![](4_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n</div>\n</div>\n\n<div class=\"example\">\n**Ejemplo: Entropía de una variable aleatoria exponencial de parámetro $\\lambda$**\n\nSea $X$ una variable aleatoria exponencial de parámetro $\\lambda$. \n\nRecordemos que su función de densidad es: $f_X(x)=\\lambda \\mathrm{e}^{-\\lambda x}$, si $x\\geq 0$ y $f_X(x)=0$, en caso contrario.\n\nSu entropía será:\n$$\n\\begin{array}{rl}\nH_X & = \\displaystyle E\\left(-\\ln(f_X)\\right)=-\\int_0^\\infty \\ln\\left(\\lambda\\mathrm{e}^{-\\lambda x}\\right)\\lambda\\mathrm{e}^{-\\lambda x}\\, dx = -\\lambda \\int_0^\\infty (\\ln(\\lambda) -\\lambda x)\\mathrm{e}^{-\\lambda x}\\, dx \\\\[1ex] & =\\displaystyle -\\ln (\\lambda)\\int_0^\\infty \\lambda\\mathrm{e}^{-\\lambda x}\\, dx+\\lambda \\int_0^\\infty \\lambda x \\mathrm{e}^{-\\lambda x}\\, dx =-\\ln(\\lambda)\\int_0^\\infty f_X(x)\\, dx +\\lambda E(X)\\\\[1ex] & =\\displaystyle -\\ln(\\lambda)+\\lambda \\frac{1}{\\lambda} =1-\\ln(\\lambda).\n\\end{array}\n$$\nEl gráfico de la entropía se puede observar en el gráfico siguiente donde $X$ tiene entropía máxima cuando $\\lambda=0$ que sería cuando $X$ tiene incertidumbre máxima al tratar de adivinar el resultado de $X$ al tener media $E(X)=\\frac{1}{\\lambda}=\\infty$ y $X$ tiene entropía mínima cuando $\\lambda$ tiende a $\\infty$ ya que su media $E(X)=\\frac{1}{\\lambda}$ tendería a 0.\n\n<div class=\"center\">\n\n::: {.cell fig='true'}\n::: {.cell-output-display}\n![](4_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n</div>\n\n</div>\n",
    "supporting": [
      "4_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}