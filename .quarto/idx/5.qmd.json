{"title":"Vectores aleatorios bidimensionales","markdown":{"headingText":"Vectores aleatorios bidimensionales","containsRefs":false,"markdown":"\n## Dos variables aleatorias\n\nMuchos experimentos aleatorios involucran varias variables aleatorias. \n\nPor ejemplo, dado un individuo de 30 años escogido al azar de una cierta población, medir su altura y su peso conjuntamente.\n\nOtro ejemplo más complejo es la medición continuada de un *fenómeno aleatorio* que se repite en el tiempo, como sería medir la temperatura media un día determinado del año, por ejemplo el día 1 de enero en un cierto lugar. \n\nLa variable aleatoria que nos da la medición en 10 años es una variable aleatoria de varias variables que involucra 10 variables aleatorias supuestas independientes e idénticamente distribuidas, lo que en **estadística inferencial** se le llama una **muestra aleatoria simple**.\n\n### Definición\n\nRecordemos que una **variable aleatoria** $X$ es una aplicación que toma valores numéricos para cada resultado de un experimento aleatorio:\n$$\n\\begin{array}{rl}\nX: \\Omega & \\longrightarrow \\mathbb{R}\\\\\nw & \\longrightarrow X(w).\n\\end{array}\n$$\nA partir de la definición anterior, generalizamos la noción de **variable aleatoria unidimensional** a **variable aleatoria bidimensional**:\n\n<l class=\"definition\">Definición de variable aleatoria bidimensional:</l>\nDado un experimento aleatorio con **espacio muestral** $\\Omega$, definimos **variable aleatoria bidimensional** $(X,Y)$ a toda aplicación \n$$\n\\begin{array}{rl}\n(X,Y): \\Omega & \\longrightarrow \\mathbb{R}^2\\\\\nw & \\longrightarrow (X(w),Y(w)).\n\\end{array}\n$$\n\n<div class=\"example\">\n**Ejemplo: lanzamiento dos dados**\n\nConsideremos el experimento aleatorio de lanzar un dado no trucado dos veces.\n\nSea $S$ la suma de los resultados obtenidos y $P$ el producto de los mismos. \n\nLa variable aleatoria $(S,P)$ que asigna a cada resultado $w=(x_1,x_2)$ donde $x_1$ es el resultado obtenido en el primer lanzamiento y $x_2$, el resultado obtenido en el segundo, los valores: $S(w)=x_1+x_2$ y $P(w)=x_1\\cdot x_2$ es una variable aleatoria bidimensional.\n\n<div class=\"example-sol\">\nEl suceso $\\{2\\leq S\\leq 4,\\ 3\\leq P\\leq 6\\}$ seria:\n$$\n\\{2\\leq S\\leq 4,\\ 3\\leq P\\leq 6\\} = \\{(1,3),(3,1),(2,2)\\}.\n$$\n\n</div>\n</div>\n\n<div class=\"example\">\n**Ejemplo**\n\nConsideremos el experimento aleatorio de elegir al azar un estudiante de primer curso de grado. Sea $w$ el estudiante elegido. Consideremos la variable aleatoria $(H,W)$ que asigna a dicho estudiante $w$, $H(w):$ la altura de dicho estudiante en cm. y $W(w):$ el peso de dicho estudiante en kg.\n<div class=\"example-sol\">\nEstamos interesado en sucesos del tipo $A=\\{H\\leq 176,\\ W\\leq 85\\}$, es decir, el conjunto de estudiantes que miden menos de 1.76 m. y que pesan menos de 85 kg.\n</div>\n</div>\n\n### Representación del dominio de una variable aleatoria bidimensional\n\nLos sucesos que se derivan de una **variable aleatoria bidimensional** estan especificados por regiones del plano.\nVeamos algunos ejemplos:\n\nSuceso: $\\{X+Y\\leq 1\\}$. Es la zona sombreada del gráfico siguiente:\n\n<div class=\"center\">\n\n```{r, echo=FALSE, label=bid1_1,fig.cap=\"\"}\nknitr::include_graphics(\"Images/Bidim1.png\",dpi=1200)\n```\n</div>\n\nSuceso: $\\{X^2+Y^2\\leq 4\\}$. Es la zona sombreada del gráfico siguiente:\n\n<div class=\"center\">\n\n```{r, echo=FALSE, label=bid2,fig.cap=\"\", fig.align='center',fig.height=7}\nknitr::include_graphics(\"Images/Bidim2.png\",dpi=1200)\n```\n</div>\n\nSuceso: $\\{\\max\\{X,Y\\}\\geq 1\\}$. Esta zona es la sombreada del gráfico siguiente:\n\n<div class=\"center\">\n\n```{r, echo=FALSE, label=bid3,fig.cap=\"Representación suceso\"}\nknitr::include_graphics(\"Images/Bidim3.png\",dpi=1200)\n```\n</div>\n\n\nLa probabilidad de que la **variable bidimensional** pertenezca a una cierta **región del plano $B$** se define de la forma siguiente:\n$$\nP((X,Y)\\in B)=P\\{w\\in \\Omega,\\ |\\ (X(w),Y(w))\\in B\\},\n$$\nes decir, la probabilidad anterior es la probabilidad del suceso formado por los elementos de $w\\in\\Omega$ que cumplen que su **imagen** por la **variable aleatoria bidimensional $(X,Y)$** esté en $B$.\n\n\nPor ejemplo, si consideramos $B=\\{X+Y\\leq 1\\}$, $P((X,Y)\\in B)$ es la probabilidad del suceso formado por los elementos $w$ de $\\Omega$ tal que la suma de las imágenes por $X$ e $Y$ sea menor o igual que 1: $X(w)+Y(w)\\leq 1$.\n\n## Función de distribución conjunta\n\n### Definición\n\nDada una **variable aleatoria bidimensional** $(X,Y)$, queremos estudiar cómo se distribuye la probabilidad de sucesos cualesquiera de la forma $\\{(X,Y)\\in B\\}$, donde $B$ es una región del plano.\n\nPara ello, definimos la **función de distribución conjunta**:\n\n<l class=\"definition\">Definición de función de distribución conjunta:</l>\nDada una variable bidimensional $(X,Y)$, definimos su **función de distribución conjunta** $F_{XY}$ a la función definida sobre $\\mathbb{R}^2$ de la manera siguiente:\n$$\n\\begin{array}{rl}\nF_{XY}: \\mathbb{R}^2 & \\longrightarrow \\mathbb{R}\\\\\n(x,y) & \\longrightarrow F_{XY}(x,y)=P(X\\leq x,\\ Y\\leq y).\n\\end{array}\n$$\n\n\nPor lo tanto, dado un valor $(x,y)\\in \\mathbb{R}^2$, consideramos la región del plano $(-\\infty,x]\\times (-\\infty,y]$:\n<div class=\"center\">\n\n```{r, echo=FALSE, label=bid4,fig.cap=\"\"}\nknitr::include_graphics(\"Images/Fxy.png\",dpi=1200)\n```\n</div>\n\nEntonces la **función de distribución conjunta** en el valor $(x,y)$ es la probabilidad del suceso formado por aquellos elementos tal que la imagen por la **variable aleatoria bidimensional** $(X,Y)$ caen dentro de la región sombreada en el gráfico anterior:\n\n$$\n\\begin{array}{rl}\nF_{XY}(x,y) &= P\\{w\\in\\Omega,\\ |\\ (X(w),Y(w))\\in (-\\infty,x]\\times (-\\infty,y]\\} \\\\ \n&= P\\{w\\in\\Omega,\\ |\\ X(w)\\leq x,\\ Y(w)\\leq y\\}.\n\\end{array}\n$$\n\n### Propiedades\n\nSea $(X,Y)$ una variable bidimensional. y sea $F_{XY}$ su **función de distribución conjunta**. Dicha función satisface las propiedades siguientes:\n\n* La función de distribución conjunta es no decreciente en cada una de las variables:\n$$\n\\mbox{Si }x_1\\leq x_2, \\mbox{ y }y_1\\leq y_2,\\mbox{ entonces, }F_{XY}(x_1,y_1)\\leq F_{XY}(x_2,y_2).\n$$\n\n* $F_{XY}(x,-\\infty)=F_{XY}(-\\infty,y)=0,$ $F_{XY}(\\infty,\\infty)=1$, para todo $x,y\\in\\mathbb{R}$.\n\n\n* Las variables aleatorias $X$ e $Y$ se llaman **variables aleatorias marginales** y sus funciones de distribución $F_X$ y $F_Y$ pueden hallarse de la forma siguiente como función de la **función de distribución conjunta** $F_{XY}$:\n$$\nF_X(x)=F_{XY}(x,\\infty),\\ F_Y(y)=F_{XY}(\\infty,y),\n$$\npara todo $x,y\\in\\mathbb{R}$.\n\n* La función de distribución conjunta es continua por el \"*norte*\" y por el \"*este*\":\n$$\n\\begin{array}{rl}\n\\lim_{x\\to a^+}F_{XY}(x,y) & =\\lim_{x\\to a, x> a}F_{XY}(x,y)=F_{XY}(a,y), \\\\\n\\lim_{y\\to b^+}F_{XY}(x,y) & =\\lim_{y\\to b, y> b}F_{XY}(x,y)=F_{XY}(x,b),\n\\end{array}\n$$\npara todo $a,b\\in\\mathbb{R}$. Ver la siguiente figura.\n<div class=\"center\">\n\n\n```{r, echo=FALSE, label=bid5,fig.cap=\"Función de distribución conjunta\"}\nknitr::include_graphics(\"Images/Fxy2.png\",dpi=1200)\n```\n</div>\n\n\n* Dados $x_1<x_2$ y $y_1<y_2$, consideramos $B$ el rectángulo de vértices $(x_1,y_1)$, $(x_1,y_2)$, $(x_2,y_1)$ y $(x_2,y_2)$: $(x_1,x_2]\\times (y_1,y_2]$. Entonces,\n$$\n\\begin{array}{rl}\nP((X,Y)\\in B)  = & F_{XY}(x_2,y_2)-F_{XY}(x_2,y_1)-F_{XY}(x_1,y_2)\\\\ & +F_{XY}(x_1,y_1).\n\\end{array}\n$$\n\n<div class=\"center\">\n\n```{r, echo=FALSE, label=bid6,fig.cap=\"\"}\nknitr::include_graphics(\"Images/Fxy3.png\",dpi=1200)\n```\n</div>\n\n<div class=\"example\">\n**Ejemplo: Función de distribución uniforme**\n\nConsideremos una variable aleatoria bidimensional $(X,Y)$ con **función de distribución conjunta**:\n$$\nF_{XY}(x,y)=\\begin{cases}\n0, & \\mbox{si }x<0,\\mbox{ o }y<0,\\\\\nxy, & \\mbox{si }0\\leq x\\leq 1,\\ 0\\leq y\\leq 1, \\\\\nx, & \\mbox{si }0\\leq x\\leq 1,\\ y> 1, \\\\\ny, & \\mbox{si }0\\leq y\\leq 1,\\ x> 1, \\\\\n1, & x\\geq 1,\\ y\\geq 1.\n\\end{cases}\n$$\nEn la figura siguiente, hemos representado por zonas cómo está definida $F_{XY}$.\n\n<div class=\"center\">\n\n```{r, echo=FALSE, label=bid7,fig.cap=\"Funciń de distribución uniforme bidimensional.\"}\nknitr::include_graphics(\"Images/FxyEx.png\",dpi=1200)\n```\n\nComprobemos algunas de las propiedades que hemos enunciado anteriormente:\n\n</div>\n\n\n<div class=\"example-sol\">\n\n* Claramente $F_{XY}(x,-\\infty)=F_{XY}(-\\infty,y)=0$ ya que $F_{XY}(x,y)=0$ si $x<0$ o $y<0$. Por tanto, si hacemos tender $x$ o $y$ hacia $-\\infty$, obtendremos que $F_{XY}(x,-\\infty)=F_{XY}(-\\infty,y)=0$.\n\n* De la misma manera $F_{XY}(\\infty,\\infty)=1$ ya que $F_{XY}(x,y)=1$ para $x>1$ e $y>1$. Por tanto, si hacemos tender $x$ e $y$ hacia $\\infty$, obtendremos $F_{XY}(\\infty,\\infty)=1$.\n\n* Hallemos las marginales:\n$$\nF_X(x)=F_{XY}(x,\\infty)=\\begin{cases}\n0, & \\mbox{ si }x<0,\\\\\nx, & \\mbox{ si } 0\\leq x\\leq 1,\\\\\n1, & \\mbox{ si } x>1.\n\\end{cases}\n$$\nPara ver la expresión anterior basta trazar la recta vertical $X=x$ en el gráfico anterior y ver hacia dónde tiende a medida que la $y$ se va hacia $\\infty$. \n\n¿Habéis averiguado cuál es la distribución de $X$?\n\n¡Efectivamente!, $X$ es la uniforme en el intervalo $(0,1)$.\n\nDejamos como ejercicio hallar la distribución marginal para la variable $Y$.\n\n* Comprobemos que $F_{XY}$ es continua por el \"norte\" y el \"este\" en el punto $(1,1)$ que es un punto problemático:\n$$\n\\lim_{x\\to 1,x> 1} F_{XY}(x,1)=\\lim_{x\\to 1,x> 1} 1  = F_{XY}(1,1),\n$$\n\n$$\n\\lim_{y\\to 1,y> 1} F_{XY}(1,y)=\\lim_{y\\to 1,y> 1} 1  = F_{XY}(1,1).\n$$\n\n\nRepresentemos en  un gráfico tridimensional la **función de distribución conjunta** usando la función `persp` de `R` para $x$ e $y$ entre -2 y 2.\n\nPrimero definimos la **función** y luego la dibujamos:\n\n\n```{r,fig.align='center',fig.height=6.5}\nf.dist.con = function(x,y){ifelse(x<0 | y<0,0,\n                           ifelse(x>=0 & x<=1 & y>=0 & y<=1,x*y,\n                           ifelse(x>=0 & x<=1 & y >1,x,ifelse(y>=0 & y<=1 & x>1,y,1))))}\nx=seq(from=-2,to=2,by=0.1)\ny=seq(from=-2,to=2,by=0.1)\nz=outer(x,y,f.dist.con)\npersp(x,y,z,theta=50,phi=40,col=\"blue\",shade=0.25,ticktype=\"detailed\")\n```\n</div>\n</div>\n\n\n<div class=\"example\">\n**Ejemplo: dos lanzamientos de un dado no trucado**\n\nConsideremos el experimento aleatorio de lanzar dos veces un dado no trucado. \n\nSea $(S,P)$ la **variable aleatoria bidimensional** que nos da la suma y el producto de los resultados obtenidos, respectivamente.\n\n<div class=\"example-sol\">\n\nLa **función de distribución conjunta** en el valor $(3,4)$ es:\n$$\nF_{XY}(3,4) = P(S\\leq 3,\\ P\\leq 4)=P\\{(1,1), (1,2), (2,1) \\}=\\frac{3}{36}=\\frac{1}{12}\\approx `r round(1/12,3)`, \n$$\nya que $\\Omega$ tiene en total $36$ resultados:\n$$\n\\Omega =\\{(1,1),(1,2).\\ldots, (6,6)\\}.\n$$\ny los únicos resultados en los que la suma es menor o igual que 3 y el producto menor o igual que 4 son $(1,1)$ (suma 2 producto 1), $(1,2)$ (suma 3 y producto 2) y $(2,1)$ (suma 3 y producto 2).\n</div>\n</div>\n\n<div class=\"exercise\">\n**Ejercicio**\n\nHallar el valor de la **función de distribución conjunta** para la **variable aleatoria bidimensional** anterior $(S,P)$ en los valores $(i,j)$ siguientes: $(4,5),\\ (4,9),\\ (5,9),\\ (6,10)$.\n</div>\n\n## Variables aleatorias bidimensionales discretas\n\n<l class=\"definition\">Definición de variable aleatoria bidimensional discreta:</l>\nSea $(X,Y)$ una **variable aleatoria bidimensional**. Diremos \nque es discreta cuando su conjunto de valores en $\\mathbb{R}^2$, $(X,Y)(\\Omega)$ es un conjunto finito o numerable. \n\nEn la mayoría de los casos, dicho conjunto es un subconjunto de los enteros naturales.\n\n<div class=\"example\">\n**Ejemplo: lanzamiento de dos dados (continuación)**\n\nLa variable aleatoria bidimensional anterior que nos daba la suma y el producto de los resultados obtenidos por los dos lanzamientos, respectivamente es discreta ya que: \n$$\n\\begin{array}{rl}\n(S,P)(\\Omega) =&\\{(2,1),(3,2),(4,3),(4,4),(5,4),(5,6),(6,5),(6,8),(6,9),(7,6),\\\\ & \n(7,10),(7,12),(8,12), (8,15),(8,16),(9,18),(9,20),(10,24),\\\\ & (10,25),(11,30), (12,36)\\}.\n\\end{array}\n$$\n\nComprobar que el conjunto $(S,P)(\\Omega)$ dado por el ejemplo coincide con la expresión dada. \nO lo que es lo mismo, hallar el conjunto $(S,P)(\\Omega)$:\n\n<div class=\"example-sol\">\n\n$$\n\\begin{array}{rl}\n(S,P): \\Omega & \\longrightarrow \\mathbb{R}^2\\\\\n(1,1) & \\longrightarrow (S(1,1),P(1,1))=(2,1),\\\\\n(1,2) & \\longrightarrow (S(1,2),P(1,2))=(3,2),\\\\\n\\vdots & \\vdots \\\\\n(6,6) & \\longrightarrow (S(6,6),P(6,6))=(12,36).\n\\end{array}\n$$\n</div>\n</div>\n\n### Función de probabilidad conjunta\n\n<l class=\"definition\">Definición de función de probabilidad conjunta:</l>\nDada una **variable aleatoria bidimensional discreta** $(X,Y)$ con $(X,Y)(\\Omega)=\\{(x_i,y_j),\\ i=1,2,\\ldots,\\ j=1,2,\\ldots,\\}$, definimos la función de probabilidad discreta $P_{XY}$ para un valor $(x,y)\\in\\mathbb{R}^2$ de la siguiente forma:\n\n$$\n\\begin{array}{rl}\nP_{XY}: \\mathbb{R}^2 & \\longrightarrow \\mathbb{R}\\\\\n(x,y) & \\longrightarrow P_{XY}(x,y)=P(X= x,\\ Y= y).\n\\end{array}\n$$\n\n<l class=\"observ\">Observación:</l>\nSi $(x,y)\\not\\in (X,Y)(\\Omega)$, el valor de la **función de probabilidad conjunta** en $(x,y)$ es nula: $P_{XY}(x,y)=0$. El motivo es que, en este caso,  el conjunto $\\{w\\in\\Omega,\\ | (X(w),Y(w))=(x,y)\\}=\\emptyset$ es vacío  pues $(x,y)\\not\\in (X,Y)(\\Omega)$.\n\n\n<div class=\"example\">\n**Ejemplo: lanzamiento de dos dados (continuación)**\nPor tanto, de cara a calcular $P_{XY}$ basta  conocer los valores de $P_{XY}(x_i,y_j)$ para $(x_i,y_j)\\in (X,Y)(\\Omega)$:\n\n<div class=\"example-sol\">\n<div class=\"center\">\n| $X/Y$| $y_1$    | $y_2$  | $\\ldots$ | $y_N$ |\n|----|----|----|----|----|\n| $x_1$| $P_{XY}(x_1,y_1)$ | $P_{XY}(x_1,y_2)$ | $\\ldots$ | $P_{XY}(x_1,y_N)$|\n| $x_2$| $P_{XY}(x_2,y_1)$ | $P_{XY}(x_2,y_2)$ | $\\ldots$ | $P_{XY}(x_2,y_N)$|\n| $\\vdots$ |$\\vdots$ |$\\vdots$ |$\\vdots$ |$\\vdots$ |\n| $x_M$| $P_{XY}(x_M,y_1)$ | $P_{XY}(x_M,y_2)$ | $\\ldots$ | $P_{XY}(x_M,y_N)$|\n</div>\n\n\n\n\nLa **función de probabilidad conjunta** es:\n<div class=\"center\">\n<!--| $S/P$| 1 | 2| 3 | 4 | 5| 6 | 8| 9| 10 | 12 | 15 | 16 | 18 | 20 | 24 | 25 | 30 | 36 \n|--|--|--|--|--|--|--|--|--|--|--|--|--|--|--|--|--|--|--\n| 2   |$\\frac{1}{36}$|0 |0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0\n| 3   | 0 | $\\frac{2}{36}$ |0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0\n| 4   | 0 | 0 | $\\frac{2}{36}$ | $\\frac{1}{36}$ | 0|0|0|0|0|0|0|0|0|0|0|0|0|0\n| 5   | 0 | 0 | 0 | $\\frac{2}{36}$ | 0 | $\\frac{2}{36}$ | 0|0|0|0|0|0|0|0|0|0|0|0\n| 6   | 0 | 0 | 0 | 0 | $\\frac{2}{36}$ | 0 | $\\frac{2}{36}$ |$\\frac{1}{36}$|0|0|0|0|0|0|0|0|0|0\n| 7 | 0 |0|0|0|0|$\\frac{2}{36}$|0|0|$\\frac{2}{36}$|$\\frac{2}{36}$|0|0|0|0|0|0|0|0\n|8  |0|0|0|0|0|0|0|0|0|$\\frac{2}{36}$|$\\frac{2}{36}$|$\\frac{1}{36}$|0|0|0|0|0|0\n|9 |0|0|0|0|0|0|0|0|0|0|0|0|$\\frac{2}{36}$|$\\frac{2}{36}$|0|0|0|0\n| 10 | 0|0|0|0|0|0|0|0|0|0|0|0|0|0|$\\frac{2}{36}$|$\\frac{1}{36}$|0|0\n| 11 | 0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|$\\frac{2}{36}$|0\n| 12 |  0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|$\\frac{1}{36}$-->\n\n|$P/S$|2|3|4|5|6|7|8|9|10|11|12|\n|--|--|--|--|--|--|--|--|--|--|--|--|\n|1|$\\frac{1}{36}$|0|0|0|0|0|0|0|0|0|0|\n|2|0| $\\frac{2}{36}$ |0|0|0|0|0|0|0|0|0|\n|3|0|0| $\\frac{2}{36}$ |0|0|0|0|0|0|0|0|\n|4|0|0| $\\frac{1}{36}$ | $\\frac{2}{36}$ |0|0|0|0|0|0|0|\n|5|0|0|0|0| $\\frac{2}{36}$ |0|0|0|0|0|0|\n|6|0|0|0| $\\frac{2}{36}$ |0|$\\frac{2}{36}$|0|0|0|0|0|\n|8|0|0|0|0| $\\frac{2}{36}$ |0|0|0|0|0|0|\n|9|0|0|0|0|$\\frac{1}{36}$|0|0|0|0|0|0|\n|10|0|0|0|0|0|$\\frac{2}{36}$|0|0|0|0|0|\n|12|0|0|0|0|0|$\\frac{2}{36}$|$\\frac{2}{36}$|0|0|0|0|\n|15|0|0|0|0|0|0|$\\frac{2}{36}$|0|0|0|0|\n|16|0|0|0|0|0|0|$\\frac{1}{36}$|0|0|0|0|\n|18|0|0|0|0|0|0|0|$\\frac{2}{36}$|0|0|0|\n|20|0|0|0|0|0|0|0|$\\frac{2}{36}$|0|0|0|\n|24|0|0|0|0|0|0|0|0|$\\frac{2}{36}$|0|0|\n|25|0|0|0|0|0|0|0|0|$\\frac{1}{36}$|0|0|\n|30|0|0|0|0|0|0|0|0|0|$\\frac{2}{36}$|0|\n|36|0|0|0|0|0|0|0|0|0|0|$\\frac{1}{36}$|\n\n\n</div>\n\nVamos a definir unas funciones en `R` para calcular la **función de probabilidad conjunta**.\n\nLa función `pdado` devuelve la probabilidad de que salga la cara `x` en un dado de `n` caras donde por defecto $n=6$:\n```{r}\npdado =function(x,n=6)  sapply(x,FUN=function(x) \n  if( x %in% c(1:n))  {return(1/n)} else {return(0)})\n```\nVamos a probarla. La probabilidad de que salga la cara 4 en un dado de 6 caras vale:\n```{r}\npdado(4,6)\n```\n\n\nLa función `pdado2` devuelve la probabilidad de que salgan las caras `x` e  `y` cuando lanzamos un dado de `n` caras dos veces:\n```{r}\npdado2 =function(x,y,n=6) {pdado(x,n)*pdado(y,n)}\n```\nPor ejemplo la probabilidad de que salgan las caras 3 y 4 en un dado de 6 caras es:\n```{r}\npdado2(3,4,6)\n```\n\nLa función `psum_prod` nos da la **función de probabilidad conjunta** de la suma y el producto cuando lanzamos dos dados de `n` caras:\n```{r}\npsum_prod=function(x,y,n=6){\n  Dxy=data.frame(d1=rep(1:n,each=n),d2=rep(1:n,times=n))\n  Dxy$suma=Dxy$d1+Dxy$d2\n  Dxy$producto=Dxy$d1*Dxy$d2\n  aux=Dxy[Dxy$suma==x& Dxy$producto==y,]\n  sum(apply(aux[,1:2],FUN=function(x) {pdado2(x[1],x[2],n=n)},1 ))\n}\n```\n\nPor ejemplo, sabemos que $P_{SP}(6,8)=\\frac{2}{36}=`r round(2/36,4)`$:\n\n\n```{r}\npsum_prod(6,8)\n```\n\nla tabla de la **función de probabilidad conjunta** para la variable $(S,P)$ hacemos lo siguiente:\n\n```{r}\nn=6\nDxy=data.frame(d1=rep(1:n,each=n),d2=rep(1:n,times=n))\nDxy$suma=Dxy$d1+Dxy$d2\nDxy$producto=Dxy$d1*Dxy$d2\ntabla.func.prob.conjunta=prop.table(table(Dxy$suma,Dxy$producto))\nknitr::kable(round(tabla.func.prob.conjunta[,1:9],3))\nknitr::kable(round(tabla.func.prob.conjunta[,10:18],3))\n```\n\nlo cual finaliza nuestros cálculos en `R`. \n\n</div>\n</div>\n\n#### Propiedades de la función de probabilidad conjunta\n\nSea $(X,Y)$ una **variable aleatoria bidimensional discreta** con conjunto de valores $(X,Y)(\\Omega)=\\{(x_i,y_j)\\, i=1,2,\\ldots,\\ j=1,2,\\ldots\\}$. Entonces su **función de probabilidad conjunta** verifica las propiedades siguientes:\n\nLa suma de todos los valores de la **función de probabilidad conjunta** sobre el conjunto de valores siempre vale 1: $$\\sum_{i}\\sum_j P_{XY}(x_i,y_j)=1.$$\n\n\nSea $B$ una región del plano. El valor de la probabilidad $P((X,Y)\\in B)$ se puede calcular de la forma siguiente:\n\n$$\nP((X,Y)\\in B) =\\sum_{(x_i,y_j)\\in B} P_{XY}(x_i,y_j).\n$$\n\nEs decir, la probabilidad de que la variable bidimensional tome valores en $B$ es igual a la suma de todos aquellos valores de la función de probabilidad conjunta que están en $B$.\n\nEn particular, tenemos la sigueinte propiedad que relaciona la **función de distribución conjunta** con la **función de probabilidad conjunta**:\n\n$$\nF_{XY}(x,y)=\\sum_{x_i\\leq x, y_j\\leq y} P_{XY}(x_i,y_j).\n$$\nDicha expresión se deduce de la expresión anterior considerando $B=(-\\infty,x]\\times (-\\infty,y]$.\n\n\n<div class=\"exercise\">\n**Ejercicio**\n\nEn el  ejemplo del lanzamiento de  los dos dados. Comprobad usando la tabla de la función de probabilidad conjunta que la suma de todos sus valores suma 1.\n</div>\n\n<div class=\"example\">\n**Ejemplo: lanzamientos de dados (continuación)**\n\nApliquemos la fórmula que relaciona la función de distribución conjunta con la función de probabilidad conjunta para $(x,y)=(5,4)$.\n\n<div class=\"example-sol\">\n\nRecordemos la tabla de la función de probabilidad conjunta hasta $S=5$ y $P=4$:\n\n<div class=\"center\">\n| $S/P$| 1 | 2| 3 | 4 | \n|--|--|--|---|---|--\n| 2    |$\\frac{1}{36}$|0 |0|0|$\\ldots$\n| 3    | 0 | $\\frac{2}{36}$ |0|0|$\\ldots$\n| 4   | 0 | 0 | $\\frac{2}{36}$ | $\\frac{1}{36}$ | $\\ldots$\n| 5   | 0 | 0 | 0 | $\\frac{2}{36}$ | $\\ldots$\n| $\\vdots$ | $\\vdots$ | $\\vdots$ | $\\vdots$ | $\\vdots$ | $\\vdots$\n</div>\n\nObservamos que los únicos valores $(x_i,y_j)\\in (X,Y)(\\Omega)$ que verifican $x_i\\leq 5$ y $y_j\\leq 4$ son $(2,1)$, $(3,2)$, $(4,3)$, $(4,4)$ y $(5,4)$. Por tanto,\n\n$$\n\\begin{array}{rl}\nF_{SP}(5,4) &= P_{SP}(2,1)+P_{SP}(3,2)+P_{SP}(4,3)+P_{SP}(4,4)+P_{SP}(5,4) \\\\ & = \\frac{1}{36}+\\frac{2}{36}+\\frac{2}{36}+\\frac{1}{36}+\\frac{2}{36} = \\frac{8}{36}=\\frac{2}{9}.\n\\end{array}\n$$\nEs decir, \"a largo plazo\", de cada 9 ocasiones que lanzamos un dado dos veces; en 2 ocasiones obtenemos un resultado cuya suma es menor o igual que 5 y cuyo producto es menor o igual que 4.\n\n\nPara definir la **función de distribución conjunta** definimos la función siguiente en `R`:\n\n```{r}\nfunc.dist.conj = function(x,y,n=6){\n  sum(tabla.func.prob.conjunta[as.integer(rownames(tabla.func.prob.conjunta))<=x,\n                            as.integer(colnames(tabla.func.prob.conjunta)) <=y])\n}\n```\n\nComprobemos que $F_{SP}(5,4)=\\frac{2}{9}=`r round(2/9,4)`$:\n\n```{r}\nfunc.dist.conj(5,4)\n```\n\n</div>\n</div>\n\n\n### Distribuciones  marginales\n\nConsideremos una variable aleatoria **bidimensional discreta $(X,Y)$** con **función de probabilidad conjunta** $P_{XY}(x_i,y_j)$, con $(x_i,y_j)\\in (X,Y)(\\Omega)$, $i=1,2,\\ldots$, $j=1,2,\\ldots$.\n\nLa tabla de la **función de probabilidad conjunta** contiene suficiente información para obtener las **funciones de probabilidad** de las variables $X$ e $Y$. \n\nDichas variables $X$ e $Y$ se denominan **distribuciones marginales** y sus correspondientes **funciones de probabilidad**, **funciones de probabilidad marginales** $P_X$ de la variable $X$ y $P_Y$ de la variable $Y$.\n\nVeamos cómo obtener $P_X$ y $P_Y$ a partir de la tabla $P_{XY}$.\n\n<l class=\"prop\">Proposición. Expresión de las funciones de probabilidad marginales. </l>\nSea $(X,Y)$ una variable aleatoria **bidimensional discreta** con **función de probabilidad conjunta** $P_{XY}(x_i,y_j)$, con $(x_i,y_j)\\in (X,Y)(\\Omega)$, $i=1,2,\\ldots$, $j=1,2,\\ldots$.\n\nLas **funciones de probabilidad marginales** $P_X(x_i)$ y $P_Y(y_j)$ se calculan usando las expresiones siguientes:\n\n$$\n\\begin{array}{rl}\nP_X(x_i)  & = \\sum_{j=1} P_{XY}(x_i,y_j),\\  i=1,2,\\ldots,\\\\ P_Y(y_j) &  = \\sum_{i=1} P_{XY}(x_i,y_j),\\ \\ j=1,2,\\ldots\n\\end{array}\n$$\n\nSi consideramos la funciòn  $P_{XY}$ como una tabla bidimensional en la  que en la primera fila están los valores de la variable $Y$ ($y_1,y_2,\\ldots$) y en la primera columna están los valores de la variable $X$ ($x_1,x_2,\\ldots$). Para obtener la **función de probabilidad marginal** de la variable $X$ en el valor $x_i$, $P_X(x_i)$, hay que sumar todos los valores de $P_{XY}(x_i,y_j)$ correspondientes a la fila $i$-ésima y para obtener la **función de probabilidad marginal** de la variable $Y$ en el valor $y_j$, $P_Y(y_j)$, hay que sumar todos los valores de $P_{XY}(x_i,y_j)$ correspondientes a la columna $j$-ésima.\n\n#### Ejemplo\n\n<div class=\"example\">\n**Ejemplo de la suma y el producto de los resultados de dos lanzamientos de un dado**\n\nHallemos la función de probabilidad marginal para la suma de los resultados $S$:\n\n<div class=\"example-sol\">\n\nUsando la expresión  de la probabilidad marginal tenemos que \n\n$$\n\\begin{array}{rl}\nP_S(2) & = P_{SP}(2,1)=\\frac{1}{36},\\\\\nP_S(3) & = P_{SP}(3,2)=\\frac{2}{36},\\\\\nP_S(4) & = P_{SP}(4,3)+P_{SP}(4,4)=\\frac{2}{36}+\\frac{1}{36}=\\frac{3}{36}=\\frac{1}{12},\\\\\nP_S(5) & = P_{SP}(5,4)+P_{SP}(5,6)=\\frac{2}{36}+\\frac{2}{36}=\\frac{4}{36}=\\frac{1}{9},\\\\\nP_S(6) & = P_{SP}(6,5)+P_{SP}(6,8)+P_{SP}(6,9)=\\frac{2}{36}+\\frac{2}{36}+\\frac{1}{36}=\\frac{5}{36},\\\\\nP_S(7) & = P_{SP}(7,6)+P_{SP}(7,10)+P_{SP}(7,12)=\\frac{2}{36}+\\frac{2}{36}+\\frac{2}{36}=\\frac{6}{36}=\\frac{1}{6},\\\\\nP_S(8) & = P_{SP}(8,12)+P_{SP}(8,15)+P_{SP}(8,16)=\\frac{2}{36}+\\frac{2}{36}+\\frac{1}{36}=\\frac{5}{36},\\\\\nP_S(9) & = P_{SP}(9,18)+P_{SP}(9,20)=\\frac{2}{36}+\\frac{2}{36}=\\frac{4}{36}=\\frac{1}{9},\\\\\nP_S(10) & = P_{SP}(10,24)+P_{SP}(10,25)=\\frac{2}{36}+\\frac{1}{36}=\\frac{3}{36}=\\frac{1}{12},\\\\\nP_S(11) & = P_{SP}(11,30)=\\frac{2}{36},\\\\\nP_S(12) & = P_{SP}(12,36)=\\frac{1}{36}.\n\\end{array}\n$$\n\nLa **función de probabilidad marginal** de la suma $S$ queda resumida en la tabla siguiente:\n\n<div class=\"center\">\n| $S$| 2 | 3| 4 | 5 | 6| 7| 8 | 9 | 10 | 11 | 12 \n|--|--|--|--|--|--|--|--|--|--|--|--\n| $P_S$|$\\frac{1}{36}$|$\\frac{2}{36}$|$\\frac{3}{36}$|$\\frac{4}{36}$|$\\frac{5}{36}$|$\\frac{6}{36}$|$\\frac{5}{36}$|$\\frac{4}{36}$|$\\frac{3}{36}$|$\\frac{2}{36}$|$\\frac{1}{36}$\n</div>\n\nPara hallar la **función de probabilidad marginal** de la suma basta sumar las filas de la tabla que nos daba la **función de probabilidad conjunta**:\n```{r}\nmarginal.suma = apply(tabla.func.prob.conjunta,1,sum)\nmarginal.suma\n```\n\nDe la misma manera, para hallar la **función de probabilidad marginal** del producto basta sumar las columnas de la tabla anterior:\n```{r}\nmarginal.producto = apply(tabla.func.prob.conjunta,2,sum)\nmarginal.producto\n```\n</div>\n</div>\n\n## Variables aleatorias bidimensionales continuas\n\n### Definición\n\nRecordemos la definición de **variable continua unidimensional**: $X$ es continua si existe una función $f_X:\\mathbb{R}\\longrightarrow \\mathbb{R}$, llamada **función de densidad** no negativa $f_X(x)\\geq 0$, para todo $x\\in\\mathbb{R}$ tal que para cualquier intervalo $(a,b)$, la probabilidad de que $X$ esté en $(a,b)$ se calcula de la forma siguiente:\n\n$$\nP(X\\in B)=P(a< X < b)=\\int_B f_{X}(x)\\,du=\\int_a^b f_{X}(x)\\,dx.\n$$\n\nLa generalización natural es, entonces:\n\n<l class=\"definition\">Definición de variable aleatoria bidimensional continua. </l>\nSea $(X,Y)$ una variable aleatoria bidimensional. Diremos que $(X,Y)$ es continua si existe una función \n$f_{XY}:\\mathbb{R}^2\\longrightarrow \\mathbb{R}$ llamada **función de densidad** no negativa $f_{XY}(x,y)\\geq 0$ para todo $(x,y)\\in\\mathbb{R}^2$ tal que dado cualquier región $B$ del plano, la probabilidad de que $(X,Y)$ esté en $B$ se calcula de la forma siguiente:\n$$\nP((X,Y)\\in B)=\\int\\int_B f_{XY}(x,y)\\,dx\\,dy.\n$$\n\n#### Ejemplos\n<div class=\"example\">\n**Ejemplo: cálculo probabilidad distribución bidimensional**\n\nConsideremos la siguiente **función de densidad**:\n$$\nf_{XY}(x,y)=\\begin{cases}\n1, & \\mbox{ si }0\\leq x\\leq 1,\\ 0\\leq y\\leq 1, \\\\\n0, & \\mbox{en caso contrario.}\n\\end{cases}\n$$\nEn este caso, si consideramos $B=\\left[-1,\\frac{1}{2}\\right]\\times \\left[-1,\\frac{1}{2}\\right]$, la probabilidad de que $(X,Y)$ esté en $B$ se calcularía de la forma siguiente:\n\n<div class=\"example-sol\">\n\n$$\n\\begin{array}{rl}\nP((X,Y)\\in B)&=\\int\\int_{B} f_{XY}(x,y)\\, dx\\, dy=\\int_{-1}^{\\frac{1}{2}}\\int_{-1}^{\\frac{1}{2}} f_{XY}(x,y)\\, dx\\, dy \\\\\n&=\\int_0^{\\frac{1}{2}}\\int_0^{\\frac{1}{2}} 1\\, dx\\,dy=\\int_0^{\\frac{1}{2}} 1\\, dx\\int_0^{\\frac{1}{2}} 1\\, dy=\\frac{1}{2}\\cdot\\frac{1}{2}=\\frac{1}{4}.\n\\end{array}\n$$\n\nEn la figura siguiente hemos dibujado en morado la región donde $f_{XY}$ no es cero, es decir $[0,1]\\times [0,1]$, la región $B$ en verde y la región intersección de las dos anteriores que es donde tenemos que integrar la **función de densidad** dada.\n\n<div class=\"center\">\n\n```{r, echo=FALSE, label=bid8,fig.cap=\"\"}\nknitr::include_graphics(\"Images/VaUniformeBidi.png\",dpi=1200)\n```\n</div>\n</div>\n</div>\n\n### Propiedades de la función de densidad\nSea $(X,Y)$ una **variable aleatoria bidimensional continua** con **función de densidad** $f_{XY}$. Esta función verifica las propiedades siguientes:\n\n* La integral de dicha función sobre todo el plano vale 1: \n\n$$\n\\int\\int_{\\mathbb{R}^2} f_{XY}(x,y)\\,dx\\,dy =1.\n$$\n\nPara ver dicha propiedad, basta considerar $B=\\mathbb{R}^2$, tener en cuenta que el suceso $(X,Y)\\in \\mathbb{R}^2$ es el total $\\Omega$ y aplicar la definición de $f_{XY}$:\n\n$$\nP((X,Y)\\in \\mathbb{R}^2)=1= \\int\\int_{\\mathbb{R}^2} f_{XY}(x,y)\\,dx\\,dy.\n$$\n\n* La relación que hay entre la **función de distribución** $F_{XY}$ y la **función de densidad** $f_{XY}$ es la siguiente:\n\n$$\nF_{XY}(x,y)=\\int_{-\\infty}^x\\int_{-\\infty}^y f_{XY}(u,v)\\,du\\,dv.\n$$\n\nPara ver dicha propiedad, basta considerar $B=(-\\infty,x]\\times (-\\infty,y]$ y aplicar la definición de **función de distribución**:\n\n$$\nF_{XY}(x,y)=P((X,Y)\\in (-\\infty,x]\\times (-\\infty,y])=\\int_{-\\infty}^x\\int_{-\\infty}^y f_{XY}(u,v)\\,du\\,dv.\n$$\n\n* La relación que hay entre la **función de densidad** $f_{XY}$ y la **función de distribución** $F_{XY}$ es la siguiente:\n\n$$\nf_{XY}(x,y)=\\frac{\\partial^2 F_{XY}(x,y)}{\\partial x\\partial y}.\n$$\n\nDicha propiedad se deduce de la anterior, derivando primero respecto a $x$ y después respecto a $y$ para eliminar las dos integrales.\n\n* Las **funciones de densidad marginales** de las variables $X$ e $Y$, $f_X(x)$ y $f_Y(y)$ respectivamente, se calculan de la forma siguiente:\n\n$$\nf_X(x)=\\int_{-\\infty}^\\infty f_{XY}(x,y)\\, dy,\\ f_Y(y)=\\int_{-\\infty}^\\infty f_{XY}(x,y)\\, dx\n$$\n\n\n#### Ejemplos\n<div class=\"example\">\n\n**Ejemplo: continuación  ejemplo  anterior**\n\nComprobemos las propiedades usando la **función de densidad** del ejemplo anterior: \n$f_{XY}(x,y)=\\begin{cases}\n1, & \\mbox{ si }0\\leq x\\leq 1,\\ 0\\leq y\\leq 1, \\\\\n0, & \\mbox{en caso contrario.}\n\\end{cases}$\n\n<div class=\"example-sol\">\n* La integral de $f_{XY}$ sobre todo el plano vale 1:\n\n$$\n\\begin{array}{rcl}\n\\int\\int_{\\mathbb{R}^2} f_{XY}(x,y)\\,dx\\, dy &=&\\int_0^1\\int_0^1 1\\, dx\\, dv\\\\\n&=&\\int_0^1 1\\, dx\\int_0^1 1\\, dy=1\\cdot 1=1.\n\\end{array}\n$$\n\n* Vamos a calcular la función de distribución $F_{XY}$. Para ello dividimos el plano en 5 zonas tal como muestra la figura siguiente:\n\n<div class=\"center\">\n\n```{r, echo=FALSE, label=bid9,fig.cap=\"\"}\nknitr::include_graphics(\"Images/VaUniformeBidi2.png\",dpi=1200)\n```\n</div>\n\nSea $(x,y)$ un punto cualquiera de $\\mathbb{R}^2$. De cara a calcular $F_{XY}(x,y)$ tenemos que averiguar el conjunto intersección siguiente: $([0,1]\\times [0,1])\\cap ((-\\infty,x]\\times (-\\infty,y])$ ya que el dominio donde $f_{XY}$ es no nula es $[0,1]\\times [0,1]$ y la función de distribución $F_{XY}(x,y)$ valdrá:\n\n$$\n\\begin{array}{rl}\nF_{XY}(x,y)&=\\int_{-\\infty}^x\\int_{-\\infty}^y f_{XY}(u,v)\\,du\\,dv\\\\ &=\n\\int\\int_{([0,1]\\times [0,1])\\cap ((-\\infty,x]\\times (-\\infty,y])} f_{XY}(u,v)\\,du\\,dv.\n\\end{array}\n$$\n\n* Caso $(x,y)\\in \\mbox{Zona A}$ o $x<0$ o $y<0$ En este caso: $([0,1]\\times [0,1])\\cap ((-\\infty,x]\\times (-\\infty,y])=\\emptyset.$ Ver figura siguiente donde la zona morada $([0,1]\\times [0,1]$) no se interseca con la zona verde ($(-\\infty,x]\\times (-\\infty,y]$).\n\nPor tanto en este caso, $F_{XY}(x,y)=0$.\n\n<div class=\"center\">\n\n```{r, echo=FALSE, label=bid10,fig.cap=\"\"}\nknitr::include_graphics(\"Images/VaUniformeBidi3.png\",dpi=1200)\n```\n</div>\n\n\n* Caso $(x,y)\\in \\mbox{Zona B}$, o $(x,y)\\in [0,1]\\times [0,1]$. En este caso: $([0,1]\\times [0,1])\\cap ((-\\infty,x]\\times (-\\infty,y])=[0,x]\\times [0,y].$ Ver figura siguiente. \n\nPor tanto en este caso, \n 19\n \n $$\nF_{XY}(x,y)=\\int_0^x \\int_0^y 1\\,du\\,dv =\\int_0^x 1\\, du\\int_0^y 1\\, dy =x\\cdot y.\n$$\n\n<div class=\"center\">\n\n```{r, echo=FALSE, label=bid11,fig.cap=\"\"}\nknitr::include_graphics(\"Images/VaUniformeBidi4.png\",dpi=1200)\n```\n</div>\n\nDejamos como ejercicio los otros casos. En resumen:\n$$\nF_{XY}(x,y)=\\begin{cases}\n0, & \\mbox{ si }x<0, \\mbox{ o }y<0,\\\\\nx y, & \\mbox{ si }(x,y)\\in [0,1]\\times [0,1],\\\\\nx, & \\mbox{ si }0\\leq x\\leq 1,\\ y>1,\\\\\ny, & \\mbox{ si }x>1,\\ 0\\leq y\\leq 1,\\\\\n1, & \\mbox{ si } x>1,\\ y>1.\n\\end{cases}\n$$\n¿Os suena? \n\nVer el primer ejemplo que pusimos del tema. Es la misma variable aleatoria bidimensional. \nAhora sabemos que se trata de una **variable aleatoria bidimensional continua**.\n\n\nComprobemos ahora que si derivamos dos veces la expresión de $F_{XY}$, primero respecto $x$ y después respecto $y$, obtendremos la **función de densidad** $f_{XY}$.\n\nSi derivamos respecto $x$ obtenemos:\n\n$$\n\\frac{\\partial F_{XY}(x,y)}{\\partial x}=\\begin{cases}\n0, & \\mbox{ si }x<0, \\mbox{ o }y<0,\\\\\ny, & \\mbox{ si }(x,y)\\in [0,1]\\times [0,1],\\\\\n1, & \\mbox{ si }0\\leq x\\leq 1,\\ y>1,\\\\\n0, & \\mbox{ si }x>1,\\ 0\\leq y\\leq 1,\\\\\n0, & \\mbox{ si } x>1,\\ y>1.\n\\end{cases}\n$$\nSi ahora derivamos respecto $y$ obtenemos:\n\n$$\n\\frac{\\partial^2 F_{XY}(x,y)}{\\partial y\\partial x}=\\begin{cases}\n0, & \\mbox{ si }x<0, \\mbox{ o }y<0,\\\\\n1, & \\mbox{ si }(x,y)\\in [0,1]\\times [0,1],\\\\\n0, & \\mbox{ si }0\\leq x\\leq 1,\\ y>1,\\\\\n0, & \\mbox{ si }x>1,\\ 0\\leq y\\leq 1,\\\\\n0, & \\mbox{ si } x>1,\\ y>1,\n\\end{cases}\n$$\nexpresión que coincide con la **función de densidad** $f_{XY}(x,y)$.\n\n\nHallemos para finalizar las **funciones de densidad marginales**. Empecemos con $f_X(x)$:\n$$\nf_X(x)=\\int_{-\\infty}^\\infty  f_{XY}(x,y)\\, dy.\n$$\nRecordemos que la región donde no se anulaba la **función de densidad conjunta** $f_{XY}$ era el cuadrado $[0,1]\\times [0,1]$. Por tanto, fijado $x$, el valor de $f_X(x)$ es no nulo si la recta vertical $X=x$ interseca dicho cuadrado. Y esto ocurre siempre que $x\\in (0,1)$. Por tanto,\n$$\nf_X(x)=\\begin{cases}\n\\int_{0}^1  f_{XY}(x,y)\\, dy=\\int_{0}^1  1\\, dy=1, & \\mbox{ si }x\\in (0,1),\\\\\n0, & \\mbox{en caso contrario.}\n\\end{cases}\n$$\nPor tanto la variable $X$ sigue la distribución uniforme en el intervalo $[0,1]$.\n\nDejamos como ejercicio comprobar que la variable $Y$ también sigue la distribución uniforme en el mismo intervalo.\n</div>\n\n</div>\n\n<div class=\"example\">\n**Ejemplo: otra función de densidad bidimensional**\n\nConsideremos la variable aleatoria bidimensional $(X,Y)$ con **función de densidad**:\n$$\nf_{XY}(x,y)=\\begin{cases}\nc\\cdot \\mathrm{e}^{-x}\\cdot\\mathrm{e}^{-y}, & 0\\leq y\\leq x < \\infty,\\\\\n0, & \\mbox{ en caso contrario,}\n\\end{cases}\n$$\ndonde $c$ es un valor que se tiene que hallar para que $f_{XY}$ sea función de densidad. Calcular $c$ y comprobar todas las propiedades de la función de densidad.\n\n<div class=\"example-sol\">\n\nPara hallar $c$, hemos de imponer que la integral de la función anterior debe ser 1 sobre todo el plano $\\mathbb{R}^2$.\n\nPrimero fijémonos en como es la región de integración (zona morada de la figura). Fijado un valor $x\\geq 0$, el valor $y$ va desde $y=0$ hasta $y=x$. Por tanto, para calcular el valor de $c$, hay que hacer lo siguiente:\n\n<div class=\"center\">\n\n```{r, echo=FALSE, label=bid12,fig.cap=\"\"}\nknitr::include_graphics(\"Images/Ejemplo2Bidi.png\",dpi=1200)\n```\n</div>\n\n$$\n\\begin{array}{rl}\n1 &=  \\int\\int_{\\mathbb{R}^2}f_{XY}(x,y)\\, dx\\, dy=\\int_{x=0}^{x=\\infty}\\int_{y=0}^{y=x} c \\cdot\\mathrm{e}^{-x}\\cdot\\mathrm{e}^{-y} \\, dy\\, dx \\\\\n  &=  c\\cdot \\int_{x=0}^{x=\\infty}\\mathrm{e}^{-x}\\cdot\\int_{y=0}^{y=x}\\mathrm{e}^{-y}\\, dy\\, dx = c \\cdot  \\int_{x=0}^{x=\\infty}\\mathrm{e}^{-x}\\cdot\\left[-\\mathrm{e}^{-y}\\right]_{y=0}^{y=x}\\, dx \\\\\n  &=  c \\cdot\\int_{x=0}^{x=\\infty}\\mathrm{e}^{-x}\\cdot\\left(1-\\mathrm{e}^{-x}\\right)\\, dx =c \\cdot\\int_{x=0}^{x=\\infty}\\left(\\mathrm{e}^{-x}-\\mathrm{e}^{-2x}\\right)\\, dx \n  \\\\ & =  c \\cdot\\left[-\\mathrm{e}^{-x}+\\frac{1}{2}\\mathrm{e}^{-2x}\\right]_{x=0}^{x=\\infty} = c\\left(1-\\frac{1}{2}\\right)=\\frac{c}{2}.\n\\end{array}\n$$\n\nEl valor de $c$ es $c=2$.\n\nVamos a calcular seguidamente su función de distribución.\n\nFijémonos que, en este caso, si $x<0$ o $y<0$, $F_{XY}(x,y)=0$, ya que el dominio $B=(-\\infty,x]\\times (-\\infty,y]$ no interseca la zona morada del gráfico anterior.\n\nSuponemos entonces que $x\\geq 0$ e $y\\geq 0$. \n\nVamos a considerar dos casos:\n\n* $x\\leq y$. Ver zona verde del gráfico siguiente.\n\n* $x\\geq y$. Ver zona morada del gráfico siguiente.\n\n\n<div class=\"center\">\n\n```{r, echo=FALSE, label=bid13,fig.cap=\"\"}\nknitr::include_graphics(\"Images/Ejemplo2Bidi2.png\",dpi=1200)\n```\n</div>\n\n\n* Caso $x\\leq y$ (zona verde de la figura adjunta). En este caso, si hacemos la intersección de la región $B=(-\\infty,x]\\times (-\\infty,y]$ (zona azul) con la zona morada o región donde $f_{XY}(x,y)\\neq 0$ obtenemos el triángulo $T_{x,y}=\\{(u,v)\\in\\mathbb{R}^2,\\ 0\\leq u\\leq x,\\ 0\\leq v\\leq u\\}.$ Ver figura adjunta.\n\nPor tanto,\n$$\n\\begin{array}{lcr}\nF_{XY}(x,y) & = & \\int_{u=0}^{u=x}\\int_{v=0}^{v=u} f_{XY}(u,v)\\,dv\\,du= 2 \\cdot\\int_{u=0}^{u=x} \\mathrm{e}^{-u}\\int_{v=0}^{v=u}  \\mathrm{e}^{-v}\\,dv\\,du\\\\ &  = & \n2 \\cdot\\int_{u=0}^{u=x} \\mathrm{e}^{-u}\\cdot\\left[-\\mathrm{e}^{-v}\\right]_{v=0}^{v=u}\\, du =  2 \\cdot\\int_{u=0}^{u=x} \\mathrm{e}^{-u} \\cdot (1-\\mathrm{e}^{-u})\\, du \n\\\\ & = & 2 \\int_{u=0}^{u=x} \\left(\\mathrm{e}^{-u}-\\mathrm{e}^{-2u}\\right)\\, du=2\\cdot \\left[-\\mathrm{e}^{-u}+\\frac{1}{2}\\cdot\\mathrm{e}^{-2u}\\right]_{u=0}^{u=x}  \\\\ & = &\n2\\cdot\\left(-\\mathrm{e}^{-x}+\\frac{1}{2}\\cdot\\mathrm{e}^{-2x}+1-\\frac{1}{2}\\right) =1-2\\cdot\\mathrm{e}^{-x}+\\mathrm{e}^{-2x}.\n\\end{array}\n$$\n\n<div class=\"center\">\n\n```{r, echo=FALSE, label=bid14,fig.cap=\"\"}\nknitr::include_graphics(\"Images/Ejemplo2Bidi3.png\",dpi=1200)\n```\n</div>\n\n\n* Caso $x\\geq y$ (zona morada de la figura adjunta). En este caso, si hacemos la intersección de la región $B=(-\\infty,x]\\times (-\\infty,y]$ (zona azul) con la zona morada o región donde $f_{XY}(x,y)\\neq 0$ obtenemos el trapecio $T_{x,y}=\\{(u,v)\\in\\mathbb{R}^2,\\ 0\\leq v\\leq y,\\ v\\leq u\\leq x\\}.$ Ver figura adjunta.\n\nPor tanto,\n\n$$\n\\begin{array}{rl}\nF_{XY}(x,y) &=  \\int_{v=0}^{v=y}\\int_{u=v}^{u=x} f_{XY}(u,v)\\,dv\\,du= 2\\cdot\\int_{v=0}^{v=y} \\mathrm{e}^{-v}\\int_{u=v}^{u=x} \\mathrm{e}^{-u}\\,du\\,dv \\\\\n&=  2 \\cdot\\int_{v=0}^{v=y} \\mathrm{e}^{-v}\\cdot\\left[-\\mathrm{e}^{-u}\\right]_{u=v}^{u=x}\\, dv  = 2 \\cdot\\int_{v=0}^{v=y} \\mathrm{e}^{-v}\\cdot (\\mathrm{e}^{-v}-\\mathrm{e}^{-x})\\, du \\\\ \n&=  2 \\cdot\\int_{v=0}^{v=y} \\left(\\mathrm{e}^{-2v}-\\mathrm{e}^{-v-x}\\right)\\, du=2 \\cdot\\left[-\\frac{1}{2}\\mathrm{e}^{-2v}+\\mathrm{e}^{-v-x}\\right]_{v=0}^{v=y}  \n\\\\ &=  2\\cdot\\left(-\\frac{1}{2}\\cdot\\mathrm{e}^{-2y}+\\mathrm{e}^{-x-y}+\\frac{1}{2}-\\mathrm{e}^{-x}\\right) \\\\&=  1-2\\cdot\\mathrm{e}^{-x}-\\mathrm{e}^{-2y}+2\\cdot\\mathrm{e}^{-x-y}.\n\\end{array}\n$$\n\n\n<div class=\"center\">\n\n```{r, echo=FALSE, label=bid15,fig.cap=\"\"}\nknitr::include_graphics(\"Images/Ejemplo2Bidi4.png\",dpi=1200)\n```\n</div>\n\nEn resumen:\n$$\nF_{XY}(x,y)=\\begin{cases}\n1-2\\cdot\\mathrm{e}^{-x}+\\mathrm{e}^{-2x}, & \\mbox{si }x\\geq 0,\\ y\\geq 0,\\ x\\leq y,\\\\\n1-2\\cdot\\mathrm{e}^{-x}-\\mathrm{e}^{-2y}+2\\cdot\\mathrm{e}^{-x-y}, & \\mbox{si }x\\geq 0,\\ y\\geq 0,\\ x\\geq y,\\\\\n0, & \\mbox{en caso contrario.}\n\\end{cases}\n$$\n\nComprobemos a continuación que si derivamos dos veces la expresión de $F_{XY}$, primero respecto $x$ y después respecto $y$, obtendremos la **función de densidad** $f_{XY}$.\n\nSi derivamos respecto $x$ obtenemos:\n$$\n\\frac{\\partial F_{XY}(x,y)}{\\partial x}=\\begin{cases}\n2\\cdot\\mathrm{e}^{-x}-2\\cdot\\mathrm{e}^{-2x}, & \\mbox{si }x\\geq 0,\\ y\\geq 0,\\ x\\leq y,\\\\\n2\\cdot\\mathrm{e}^{-x}-2\\cdot\\mathrm{e}^{-x-y}, & \\mbox{si }x\\geq 0,\\ y\\geq 0,\\ x\\geq y,\\\\\n0, & \\mbox{en caso contrario.}\n\\end{cases}\n$$\nSi ahora derivamos respecto $y$ obtenemos:\n$$\n\\frac{\\partial^2 F_{XY}(x,y)}{\\partial y\\partial x}=\\begin{cases}\n0, & \\mbox{si }x\\geq 0,\\ y\\geq 0,\\ x\\leq y,\\\\\n2\\cdot\\mathrm{e}^{-x-y}, & \\mbox{si }x\\geq 0,\\ y\\geq 0,\\ x\\geq y,\\\\\n0, & \\mbox{en caso contrario.}\n\\end{cases}\n$$\nexpresión que coincide con la **función de densidad** $f_{XY}(x,y)$.\n\nHallemos las **funciones de densidad marginales**. Fijémonos que basta tener en cuenta los casos en que $x\\geq 0$ e $y\\geq 0$ ya que en caso contrario tanto $f_X(x)$ como $f_Y(y)$ son nulas.\n\n$$\n\\begin{array}{rl}\nf_X(x) &=   \\int_{-\\infty}^{\\infty} f_{XY}(x,y)\\, dy =\\int_{y=0}^{y=x}2\\cdot\\mathrm{e}^{-x-y}\\, dy = 2\\cdot\\left[-\\mathrm{e}^{-x-y}\\right]_{y=0}^{y=x} \\\\ &=   2\\left(\\mathrm{e}^{-x}-\\mathrm{e}^{-2x}\\right),\\mbox{ si }x\\geq 0,\n\\end{array}\n$$\n\n$$\n\\begin{array}{rl}\nf_Y(y) & =  \\int_{-\\infty}^{\\infty} f_{XY}(x,y)\\, dx =\\int_{x=y}^{x=\\infty}2\\cdot\\mathrm{e}^{-x-y}\\, dx = 2\\cdot\\left[-\\mathrm{e}^{-x-y}\\right]_{x=y}^{x=\\infty}\\\\ &= 2\\cdot\\mathrm{e}^{-2y}, \\mbox{ si }y\\geq 0.\n\\end{array}\n$$\n\nVemos que la variable $Y$ corresponde a una distribución exponencial de parámetro $\\lambda =2$.\n\nDibujemos la **función de densidad conjunta** y la **función de distribución conjunta** con `R`. Primero las definimos:\n\n```{r}\nfun.den.con = function(x,y){ifelse(x>=0 & y>=0 & x>=y,\n                                   2*exp(-x-y),0)}\nfun.dist.con = function(x,y){ifelse(x>=0 & y>=0 & x<=y,\n                    1-2*exp(-x)+exp(-2*x),ifelse(x>=0 & y>=0 & x>=y,\n                    1-2*exp(-x)-exp(-2*y)+2*exp(-x-y),0))}\n```\nA continuación las dibujamos para $x$ e $y$ entre $-1$ y $4$:\n\n```{r,fig.align='center',fig.height=6.5}\nx=seq(from=-1,to=4,by=0.1)\ny=seq(from=-1,to=4,by=0.1)\nz.fun.den.con=outer(x,y,fun.den.con)\nz.fun.dist.con = outer(x,y,fun.dist.con)\npersp(x,y,z.fun.den.con,theta=50,phi=40,col=\"green\",shade=0.25,ticktype=\"detailed\")\n```\n</div>\n</div>\n\n### La distribución gaussiana bidimensional\nVamos a generalizar la distribución normal a dos dimensiones.\n\n<l class=\"definition\">Definición de distribución gaussiana bidimensional. </l>\nDiremos que la distribución de la variable aleatoria bidimensional $(X,Y)$ es **gaussiana bidimensional** dependiendo del parámetro $\\rho$ si su **función de densidad conjunta** es:\n$$\nf_{XY}(x,y)=\\frac{1}{2\\cdot\\pi\\cdot\\sqrt{1-\\rho^2}}\\mathrm{e}^{-\\frac{(x^2-2\\cdot\\rho\\cdot x\\cdot y+y^2)}{2\\cdot(1-\\rho^2)}},\\ -\\infty <x,y<\\infty.\n$$\n\n\nPropiedades de la **función de densidad de la variable gaussiana bidimensional**:\n\n* Para cualquier punto $(x,y)\\in\\mathbb{R}^2$, la **función de densidad** es no nula: $f_{XY}(x,y)>0$.\n\n* La **función de densidad** tiene un único máximo absoluto en el punto $(0,0)$ que vale $f_{XY}(0,0)=\\frac{1}{2\\pi\\sqrt{1-\\rho^2}}.$ Por tanto, para $\\rho=0$, dicho máximo alcanza el mínimo valor posible y si $\\rho\\to \\pm 1$, dicho máximo tiende a $\\infty$.\n\n* Las densidades marginales $f_X(x)$ y $f_Y(y)$ son normales $N(0,1)$. \n\n<div class=\"dem\">\nVeámoslo con $f_X(x)$. Por simetría, quedaría deducido para $f_Y(y)$:\n$$\n\\begin{array}{rl}\nf_X(x) & =\\frac{1}{2\\cdot\\pi\\cdot\\sqrt{1-\\rho^2}}\\int_{-\\infty}^\\infty \\mathrm{e}^{-\\frac{(x^2-2\\cdot\\rho \\cdot x \\cdot y+y^2)}{2\\cdot(1-\\rho^2)}}\\, dy =\n\\frac{1}{2\\cdot\\pi\\cdot\\sqrt{1-\\rho^2}}\\mathrm{e}^{-\\frac{x^2}{2\\cdot(1-\\rho^2)}}\\int_{-\\infty}^\\infty \\mathrm{e}^{-\\frac{(-2\\cdot\\rho x\\cdot y+y^2)}{2\\cdot(1-\\rho^2)}}\\, dy \\\\ & = \\frac{1}{2\\cdot\\pi\\cdot\\sqrt{1-\\rho^2}}\\mathrm{e}^{-\\frac{x^2}{2\\cdot(1-\\rho^2)}} \\int_{-\\infty}^\\infty \\mathrm{e}^{-\\frac{(y-\\rho x)^2}{2(1-\\rho^2)}} \\mathrm{e}^{\\frac{\\rho^2 \\cdot x^2}{2\\cdot(1-\\rho^2)}}\\, dy \\\\ & =\\frac{1}{2\\cdot\\pi\\cdot\\sqrt{1-\\rho^2}}\\mathrm{e}^{-\\frac{x^2}{2}} \\int_{-\\infty}^\\infty \\mathrm{e}^{-\\frac{(y-\\rho x)^2}{2(1-\\rho^2)}}\\, dy,  \\mbox{ hacemos el cambio $z=\\frac{y-\\rho x}{\\sqrt{1-\\rho^2}}$}\\\\ & = \\frac{1}{2\\cdot\\pi\\cdot\\sqrt{1-\\rho^2}}\\mathrm{e}^{-\\frac{x^2}{2}} \\int_{-\\infty}^\\infty \\mathrm{e}^{-\\frac{z^2}{2}}\\sqrt{1-\\rho^2}\\, dy =\\frac{1}{\\sqrt{2\\pi}}\\mathrm{e}^{-\\frac{x^2}{2}},\n\\end{array}\n$$\nfunción que coincide con la **función de densidad** de la variable $N(0,1)$. \n\nEn el último paso hemos usado que \n$$\n\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^\\infty \\mathrm{e}^{-\\frac{z^2}{2}}\\, dz=1,\n$$\nya que correspondería al área de una **función de densidad** de una distribución $N(0,1)$.\n</div>\n\n\n#### La distribución gaussiana bidimensional en `R`\n<div class=\"example\">\nmnormt\nLa función que nos la densidad de la **distribución normal bidimensional** es `nbvpdf` y tiene 5 parámetros: la **media** de $X$ ($\\mu_X$), la **media** de $Y$ ($\\mu_Y$), la **desviación típica** de $X$ ($\\sigma_X$), la **desviación típica** de $Y$ ($\\sigma_Y$) y un concepto que veremos más adelante, la **correlación** entre $X$ e $Y$ ($\\rho_{XY}$).\n\n<div class=\"example-sol\">\nEn el ejemplo que estamos tratando, los valores de los parámetros anteriores son: $\\mu_X=\\mu_Y=0$, $\\sigma_X=\\sigma_Y=1$ y $\\rho_{XY}=\\rho.$\n\nVamos a hacer un gráfico de la **distribución normal bidimensional** para $\\rho=\\frac{1}{2}.$\n\n```{r,fig.align='center',fig.height=6}\nlibrary(mnormt)\nx     <- seq(-3, 3, 0.1) \ny     <- seq(-3, 3, 0.1)\nmu    <- c(0, 0)\nsigma <- matrix(c(1, 0.5, 0.5, 1), nrow=2)\nf     <- function(x, y) dmnorm(cbind(x, y), mu, sigma)\nz     <- outer(x, y, f)\n\n# Ahora dibujo  los puntos de la densidad\npersp(x, y, z, theta=-30, phi=25, expand=0.6,\n      ticktype='detailed',col=\"blue\",\n      cex.axis=0.5,cex.lab=0.8)\n```\n</div>\n</div>\n\n## Independencia de variables aleatorias\n\n### Independencia de variables aleatorias discretas###\n\nRecordemos que dos sucesos $A$ y $B$ son independientes si $P(A\\cap B)=P(A)\\cdot P(B)$.\n\n¿Cómo trasladar dicho concepto al caso de variables aleatorias?\n\nEn el caso de **variables aleatorias discretas bidimensionales** vimos que, dada una variable aleatoria bidimensional discreta $(X,Y)$ con $(X,Y)(\\Omega)=\\{(x_i,y_j),\\ i=1,2,\\ldots,j=1,2,\\ldots\\}$, los sucesos de la forma $\\{X=x_i,\\  Y=y_j\\}$ determinaban cómo se distribuían los valores de la variable $(X,Y)$. De ahí la definición siguiente:\n\n<l class=\"definition\">Definición de independencia para variables aleatorias bidimensionales discretas. </l>\nSean $(X,Y)$ una **variable aleatoria bidimensional discreta** con $(X,Y)(\\Omega)=\\{(x_i,y_j),\\ i=1,2,\\ldots,j=1,2,\\ldots\\}$ y **función de probabilidad** $P_{XY}$ y **funciones de probabilidad marginales** $P_X$ y $P_Y$. Entonces $X$ e $Y$ son independientes si:\n$$\nP_{XY}(x_i,y_j)=P_X(x_i)\\cdot P_Y(y_k),\\ i=1,2,\\ldots,j=1,2,\\ldots\n$$\no dicho de otra forma:\n$$\nP(X=x_i,\\ Y=y_k)=P(X=x_i)\\cdot P(Y=y_k),\\ i=1,2,\\ldots,j=1,2,\\ldots\n$$\n\n\n<div class=\"example\">\n**Ejemplo: suma y el producto de los resultados de dos lanzamientos de un dado**\n\nConsideramos la variable aleatoria $(S,P)$ donde $S$ representa la suma de los valores obtenidos al lanzar dos veces un dado y $P$, su producto. \n\n<div class=\"example-sol\">\nEn este caso $S$ y $P$ no son independientes ya que recordemos que por ejemplo $P_{SP}(3,2)=\\frac{2}{36}$, $P_S(3)=\\frac{2}{36}$ y $P_P(2)=\\frac{2}{36}$, ya que en este último caso, sólo hay dos posibles resultados en los que el producto dé 2: el $(1,2)$ y el $(2,1)$.\n\nEntonces no se cumple que $P_{SP}(3,2)=P_S(3)\\cdot P_P(2)$, ya que $\\frac{2}{36}\\neq \\frac{2}{36}\\cdot \\frac{2}{36}$.\n\nDe ahí que no sean independientes ya que la condición anterior se debería cumplir para todos los valores $x_i$ e $y_k$ y hemos encontrado un contraejemplo en donde no se cumple.\n</div>\n</div>\n\n<l class=\"observ\">Observación. </l>\nSi la tabla de la **función de probabilidad conjunta** de $(X,Y)$ contiene algún $0$, $X$ e $Y$ no pueden ser independientes. ¿Podéis decir por qué?\n\n<div class=\"example\">\n**Ejemplo: un caso de imdependencia**\n\nVeamos un caso de independencia. Consideramos el experimento aleatorio de lanzar un dado dos veces. Sea $X$ el resultado del primer lanzamiento e $Y$, el resultado del segundo lanzamiento.\n\nVeamos que, en este caso, $X$ e $Y$ son independientes.\n\n<div class=\"example-sol\">\nEl valor de $(X,Y)(\\Omega)=\\{(1,1),(1,2),\\ldots,(6,6)\\}$, en total 36 resultados.\n\nLa **función de probabilidad conjunta** en un valor cualquiera $(i,j)$ con $i,j\\in\\{1,2,3,4,5,6\\}$ es:\n$P_{XY}(i,j)=\\frac{1}{36}$ ya que la probabilidad que salga $i$ en el primer lanzamiento es $\\frac{1}{6}$ y la probabilidad de que salga $j$ en el segundo lanzamiento, también. Por tanto, la probabilidad de que salga $i$ en el primer lanzamiento y $j$ en el segundo es: $\\frac{1}{6}\\cdot \\frac{1}{6}=\\frac{1}{36}.$\n\n\nLas **funciones de densidad marginales** de $X$ e $Y$ son:\n<div class=\"center\">\n| $X$ o $Y$| 1 | 2| 3 | 4 | 5| 6 \n|--|--|--|--|--|--|--\n| $P_X$ o $P_Y$|$\\frac{1}{6}$|$\\frac{1}{6}$|$\\frac{1}{6}$|$\\frac{1}{6}$|$\\frac{1}{6}$|$\\frac{1}{6}$\n</div>\n\nPor tanto, para todo $(i,j)$ con $i,j\\in\\{1,2,3,4,5,6\\}$ se cumplirá:\n$$\nP_{XY}(i,j)=\\frac{1}{36}=\\frac{1}{6}\\cdot \\frac{1}{6}=P_X(i)\\cdot P_Y(j).\n$$\nDeducimos que son independientes.\n\nPara comprobar si dos variables aleatorias $X$ e $Y$ son independientes o no en `R` en general, una vez calculada la tabla de la **función de probabilidad**, podemos calcular la tabla de **independencia teórica** $P_T(x_i,y_j)$ y compararlas. Ésta segunda tabla se define de la forma siguiente: \n$$\nP_T(x_i,y_j)=P_X(x_i)\\cdot P_Y(y_j),\n$$\ndonde $P_X$ y $P_Y$ son las distribuciones marginales.\n\nLa tabla de **independencia teórica ** en el caso de la suma y el producto se calcularían de la forma siguiente:\n```{r}\ntabla.ind.teor =  marginal.suma%*%t(marginal.producto)\ntabla.ind.teor = as.data.frame(tabla.ind.teor)\nrownames(tabla.ind.teor)=rownames(tabla.func.prob.conjunta)\ncolnames(tabla.ind.teor)=colnames(tabla.func.prob.conjunta)\n```\nSi comparamos los resultados de la tabla de **independencia teórica** mostrada a continuación con los resultados de la tabla de la **función de probabilidad conjunta**, veréis que no son iguales. Por tanto, $S$ y $P$ no son **independientes**.\n\n```{r,echo=FALSE}\nknitr::kable(round(tabla.ind.teor[,1:9],3))\nknitr::kable(round(tabla.ind.teor[,10:18],3))\n```\n\nlo cual finaliza nuestros cálculos en `R`.\n</div>\n</div>\n\n### Independencia de variables aleatorias continuas\n\nLa definición dada para **variables aleatorias discretas** se traslada de forma natural a las **variables aleatorias continuas**:\n\n<l class=\"definition\">Definición de independencia para variables aleatorias bidimensionales continuas. </l>\nSean $(X,Y)$ una **variable aleatoria bidimensional continua** con **función de densidad conjunta** $f_{XY}$ y **funciones de densidad marginales** $f_X$ y $f_Y$. Entonces $X$ e $Y$ son independientes si:\n$$\nf_{XY}(x,y)=f_X(x)\\cdot f_Y(y),\\ \\mbox{para todo $x,y\\in\\mathbb{R}$.}\n$$\n\n#### Ejemplos\n<div class=\"example\">\n**Ejemplo: densidad uniforme en el cuadrado unidad**\n\nRecordemos el ejemplo siguiente visto donde teníamos una **variable aleatoria bidimensional continua** $(X,Y)$ con\n**función de densidad conjunta**:\n$$\nf_{XY}(x,y)=\\begin{cases}\n1, & \\mbox{ si }0\\leq x\\leq 1,\\ 0\\leq y\\leq 1, \\\\\n0, & \\mbox{en caso contrario.}\n\\end{cases}\n$$\ny con densidad marginales:\n$$\nf_{X}(x)=\\begin{cases}\n1, & \\mbox{ si }0\\leq x\\leq 1,\\\\\n0, & \\mbox{en caso contrario.}\n\\end{cases}\\quad f_{Y}(y)=\\begin{cases}\n1, & \\mbox{ si }0\\leq y\\leq 1,\\\\\n0, & \\mbox{en caso contrario.}\n\\end{cases}\n$$\n\nVeamos que son independientes.\n\n<div class=\"example-sol\">\n\nConsideremos dos casos:\n\n* $(x,y)\\in [0,1]\\times [0,1]$. En este caso:\n$$\nf_{XY}(x,y) =1 =1\\cdot 1=f_X(x)\\cdot f_Y(y).\n$$\n\n* $(x,y)\\not\\in [0,1]\\times [0,1]$. En este caso:\n$$\nf_{XY}(x,y) =0 = f_X(x)\\cdot f_Y(y),\n$$\nya que si $(x,y)\\not\\in [0,1]\\times [0,1]$, o $x\\not\\in [0,1]$ o $y\\not\\in [0,1]$. Por tanto $f_X(x)=0$ o $f_Y(y)=0$. En cualquier caso, $f_X(x)\\cdot f_Y(y)=0$.\n</div>\n</div>\n\n<div class=\"example\">\n**Ejemplo: otra función de densidad (continuación)**\n\nRecordemos el ejemplo siguiente visto donde teníamos una **variable aleatoria bidimensional continua** $(X,Y)$ con **función de densidad conjunta**:\n$$\nf_{XY}(x,y)=\\begin{cases}\n2 \\cdot \\mathrm{e}^{-x}\\cdot\\mathrm{e}^{-y}, & 0\\leq y\\leq x < \\infty,\\\\\n0, & \\mbox{ en caso contrario,}\n\\end{cases}\n$$\ny con densidad marginales:\n$$\nf_X(x)  = 2\\left(\\mathrm{e}^{-x}-\\mathrm{e}^{-2x}\\right),\\mbox{ si }x\\geq 0, \\quad\nf_Y(y)  =  2\\mathrm{e}^{-2y}, \\mbox{ si }y\\geq 0.\n$$\n\n\nEn este caso no son independientes ya que claramente $f_{XY}(x,y)\\neq f_X(x)\\cdot f_Y(y)$.\n\n<div class=\"example-sol\">\nEn este caso, recordemos que la **función de densidad conjunta** de $(X,Y)$ es:\n$$\nf_{XY}(x,y)=\\frac{1}{2\\cdot \\pi\\cdot \\sqrt{1-\\rho^2}}\\mathrm{e}^{-\\frac{(x^2-2\\cdot \\rho \\cdot x\\cdot y+y^2)}{2\\cdot (1-\\rho^2)}},\\ -\\infty <x,y<\\infty.\n$$\nLas **funciones de densidad marginales** de $X$ e $Y$ correspondían a $N(0,1)$:\n$$\n\\begin{array}{rl}\nf_X(x) & =\\frac{1}{\\sqrt{2\\pi}}\\cdot \\mathrm{e}^{-\\frac{x^2}{2}},\\ -\\infty <x<\\infty,\\\\ f_Y(y) & =\\frac{1}{\\sqrt{2\\pi}}\\mathrm{e}^{-\\frac{y^2}{2}},\\ -\\infty <y<\\infty.\n\\end{array}\n$$\n\n¿Para qué valor(es) de $\\rho$ las variables normales estándar $X$ e $Y$ son independientes?\n\no, ¿para qué valor(es) de $\\rho$ se cumple?\n\n$$\nf_X(x)\\cdot f_Y(y)=\\frac{1}{2\\cdot\\pi}\\mathrm{e}^{-\\frac{x^2+y^2}{2}} = \\frac{1}{2\\cdot\\pi\\cdot\\sqrt{1-\\rho^2}}\\mathrm{e}^{-\\frac{(x^2-2\\cdot\\rho x \\cdot y+y^2)}{2\\cdot (1-\\rho^2)}}.\n$$\nLa respuesta es claramente para $\\rho=0$.\n\nPor tanto, $\\rho$ se puede interpretar como un parámetro de independencia, cuánto más cercano a cero esté, más cerca de la independencia estarán las variables $X$ e $Y$.\n</div>\n</div>\n\n\n### Relación de la independencia y la función de distribución\nEl siguiente resultado nos da la relación entre la **independencia de variables aleatorias** y su **función de distribución conjunta**:\n\n<l class=\"prop\">Teorema. </l>\nSea $(X,Y)$ una variable aleatoria bidimensional. Entonces \n$X$ e $Y$ son independientes si, y sólo si, la **función de distribución conjunta** es el producto de las **funciones de distribución marginales** en todo valor $(x,y)\\in\\mathbb{R}^2$:\n$$\nF_{XY}(x,y)=F_X(x)\\cdot F_Y(y),\\ (x,y)\\in\\mathbb{R}^2.\n$$\n\n<div class=\"example\">\n**Ejemplo**\n\n\nConsideramos el experimento aleatorio de lanzar un dado dos veces. Sea $X$ el resultado del primer lanzamiento e $Y$, el resultado del segundo lanzamiento.\n\nRecordemos que, en este caso, $X$ e $Y$ son independientes.\n\n<div class=\"example-sol\">\n\nEn primer lugar notemos que si $x<1$ o $y<1$, $F_{XY}(x,y)=0$ ya que el suceso $\\{X\\leq x,\\ Y\\leq y\\}$ es vacío.\n\nDe la misma forma como $x<1$ o $y<1$, o el suceso $\\{X\\leq x\\}$ o el suceso $\\{Y\\leq y\\}$ son vacíos. Por tanto, o $F_X(x)=0$ o $F_Y(y)=0$.\n\nEn cualquier caso, se cumple $F_{XY}(x,y)=0=F_X(x)\\cdot F_Y(y)$.\n\nPodemos suponer, por tanto, que $x\\geq 1$ e $y\\geq 1$.\n\nSea $(x,y)\\in \\mathbb{R}^2$ con $x\\geq 1$ e $y\\geq 1$. Podemos suponer tal que existen dos valores $i$ y $j$ en $\\{1,2,\\ldots\\}$ con $i\\leq x < i+1$ y $j\\leq y <j+1$.\n\n\nEl valor de la **función de distribución conjunta** en $(x,y)$ es: \n$$\nF_{XY}(x,y)=\\begin{cases}\n\\frac{i\\cdot j}{36}, & \\mbox{si }i\\leq 6, \\ j\\leq 6, \\\\\n\\frac{6 \\cdot i}{36}, & \\mbox{si }i\\leq 6,\\ j\\geq 6,\\\\\n\\frac{6\\cdot j}{36}, & \\mbox{si }i\\geq 6,\\ j\\leq 6,\\\\\n1, & \\mbox{ si }i\\geq 6,\\ j\\geq 6,\n\\end{cases}\n$$\n\nya que:\n\n$$\n\\begin{array}{rl}\nF_{XY}(x,y) & =P(X\\leq i,\\ Y\\leq j)=P(\\{(k,l)\\in \\{1,2,3,4,5,6\\}^2,\\ |\\ k\\leq i,\\ l\\leq j\\})\\\\ & =P(\\{(1,1),\\ldots,(1,j),\\ldots,(i,1),\\ldots,(i,j)\\})\n\\\\\n& =\\begin{cases}\n\\frac{i\\cdot j}{36}, & \\mbox{si }i\\leq 6, \\ j\\leq 6, \\\\\n\\frac{6\\cdot i}{36}, & \\mbox{si }i\\leq 6,\\ j\\geq 6,\\\\\n\\frac{6\\cdot j}{36}, & \\mbox{si }i\\geq 6,\\ j\\leq 6,\\\\\n1, & \\mbox{ si }i\\geq 6,\\ j\\geq 6,\n\\end{cases},\n\\end{array}\n$$\nya que claramente el cardinal del conjunto \n$$\\{(1,1),\\ldots,(1,j),\\ldots,(i,1),\\ldots,(i,j)\\}$$\nes \n\n$$\n\\begin{cases}\ni\\cdot j, & \\mbox{si }i\\leq 6, \\ j\\leq 6, \\\\\n\\cdot  i, & \\mbox{si }i\\leq 6,\\ j\\geq 6,\\\\\n6\\cdot j, & \\mbox{si }i\\geq 6,\\ j\\leq 6,\\\\\n36, & \\mbox{ si }i\\geq 6,\\ j\\geq 6.\n\\end{cases}\n$$\n\nHallemos ahora la función de distribución de $X$ e $Y$ que consiste en el resultado del lanzamiento de un dado.\n\nDado $x\\in\\mathbb{R}$ con $x\\geq 1$, existe un $i$ con $i\\in\\{1,2,\\ldots,\\}$ con $i\\leq x <i+1$. En este caso, el valor de $F_X(x)$ es:\n\n$$\nF_X(x)=\\begin{cases}\n\\frac{i}{6}, &\\mbox{si }i\\leq 6,\\\\\n1, & \\mbox{si }i\\geq 6,\n\\end{cases}\n$$\nya que:\n\n$$\n\\begin{array}{rl}\nF_X(x) = & F_X(i)=P(X\\leq i)=P(\\{k\\in\\{1,2,3,4,5,6\\},\\ |\\ k\\leq i\\})\n\\\\ = & \n\\begin{cases}\n\\frac{i}{6}, &\\mbox{si }i\\leq 6,\\\\\n1, & \\mbox{si }i\\geq 6,\n\\end{cases}\n\\end{array}\n$$\n\\begin\nya que el cardinal del conjunto $\\{k\\in\\{1,2,3,4,5,6\\},\\ |\\ k\\leq i\\}$ es $\\begin{cases}\ni, &\\mbox{si }i\\leq 6,\\\\\n6, & \\mbox{si }i\\geq 6.\n\\end{cases}$\n\nLa función de distribución de $Y$ es de la misma forma.\n\nPor último, comprobemos que se verifica que $F_{XY}(x,y)=F_X(x)\\cdot F_Y(y)$, si $x\\geq 1$ e $y\\geq 1$.\n\nSea $(x,y)\\in\\mathbb{R}^2$ y sean los enteros $i$ y $j$ tales que $i\\leq x<i+1$ y $j\\leq y<j+1$. Consideremos 4 casos:\n\n* $i\\leq 6, \\ j\\leq 6$. En este caso:\n$$\nF_{XY}(x,y)=\\frac{i\\cdot j}{36}=\\frac{i}{6}\\cdot \\frac{j}{6}=F_X(x)\\cdot F_Y(y).\n$$\n\n* $i\\leq 6,\\ j\\geq 6$. En este caso:\n$$\nF_{XY}(x,y)=\\frac{6i}{36}=\\frac{i}{6}\\cdot 1=F_X(x)\\cdot F_Y(y).\n$$\n\n* $i\\geq 6,\\ j\\leq 6$. En este caso:\n$$\nF_{XY}(x,y)=\\frac{6j}{36}=1\\cdot \\frac{j}{6}=F_X(x)\\cdot F_Y(y).\n$$\n\n* $i\\geq 6,\\ j\\geq 6$. En este caso:\n$$\nF_{XY}(x,y)=1=1\\cdot 1=F_X(x)\\cdot F_Y(y).\n$$\n\nEn resumen, para todo $(x,y)\\in \\mathbb{R}^2$ se verifica que $F_{XY}(x,y)=F_X(x)\\cdot F_Y(y)$, tal como queríamos ver.\n</div>\n</div>\n\n<div class=\"example\">\n**Ejemplo**\n\nRecordemos la variable aleatoria bidimensional continua con **función de densidad conjunta**:\n$$\nf_{XY}(x,y)=\\begin{cases}\n1, & \\mbox{ si }0\\leq x\\leq 1,\\ 0\\leq y\\leq 1, \\\\\n0, & \\mbox{en caso contrario.}\n\\end{cases}\n$$\nSu **función de distribución conjunta** es: \n\n<div class=\"example-sol\">\n\n$$\nF_{XY}(x,y)=\\begin{cases}\n0, & \\mbox{si }x<0,\\mbox{ o }y<0,\\\\\nx\\cdot y, & \\mbox{si }0\\leq x\\leq 1,\\ 0\\leq y\\leq 1, \\\\\nx, & \\mbox{si }0\\leq x\\leq 1,\\ y> 1, \\\\\ny, & \\mbox{si }0\\leq y\\leq 1,\\ x> 1, \\\\\n1, & x\\geq 1,\\ y\\geq 1.\n\\end{cases}\n$$\n\nRecordemos también que las **distribuciones marginales** de $X$ e $Y$ eran uniformes en el intervalo $[0,1]$. Por tanto, las **funciones de distribución marginales** son:\n$$\nF_X(x)=\\begin{cases}\n0, & \\mbox{si }x\\leq 0, \\\\\nx, & \\mbox{si }0\\leq x\\leq 1, \\\\\n1, & \\mbox{si }x\\geq 1. \\\\\n\\end{cases},\\quad \nF_Y(y)=\\begin{cases}\n0, & \\mbox{si }y\\leq 0, \\\\\ny, & \\mbox{si }0\\leq y\\leq 1, \\\\\n1, & \\mbox{si }y\\geq 1. \\\\\n\\end{cases}\n$$\nRecordemos que $X$ e $Y$ son independientes. Verifiquemos que $F_{XY}(x,y)=F_X(x)\\cdot F_Y(y)$. \n\nDistinguiremos cinco casos:\n\n* $x<0$ o $y<0$. En este caso, $F_{XY}(x,y)=0$ y, o $F_X(x)=0$, si $x<0$, o $F_Y(y)=0$, si $y<0$. En cualquier caso, se cumple que $F_{XY}(x,y)=F_X(x)\\cdot F_Y(y)$.\n\n* $0\\leq x\\leq 1,\\ 0\\leq y\\leq 1$. En este caso, $F_{XY}(x,y)=xy$, $F_X(x)=x$ y $F_Y(y)=y$. Claramente, se cumple que $F_{XY}(x,y)=F_X(x)\\cdot F_Y(y)$.\n\n* $0\\leq x\\leq 1,\\ y> 1$. En este caso, $F_{XY}(x,y)=x$, $F_X(x)=x$ y $F_Y(y)=1$. Claramente, se cumple que $F_{XY}(x,y)=F_X(x)\\cdot F_Y(y)$.\n\n* $x >1,\\ 0\\leq y\\leq  1$. En este caso, $F_{XY}(x,y)=y$, $F_X(x)=1$ y $F_Y(y)=y$. Claramente, se cumple que $F_{XY}(x,y)=F_X(x)\\cdot F_Y(y)$.\n\n* $x\\geq 1,\\ y\\geq 1$. En este caso, $F_{XY}(x,y)=1$, $F_X(x)=1$ y $F_Y(y)=1$. Claramente, se cumple que $F_{XY}(x,y)=F_X(x)\\cdot F_Y(y)$.\n\n</div>\n</div>\n\n## Momentos conjuntos y valores esperados conjuntos\n\nEl **valor esperado** de una variable aleatoria $X$ se identifica con el *centro de masa de la distribución de $X$*. \n\nLa **varianza** proporciona una medida de la *extensión de la distribución*. \n   \nEn el caso de dos variables aleatorias, estamos interesados en cómo $X$ e $Y$ varían juntas. \n\nEn particular, nos interesa saber si la variación de $X$ e $Y$ está correlacionada. Por ejemplo, si $X$ aumenta, ¿Y tiende a aumentar o disminuir? \n\nLos momentos conjuntos de $X$ e $Y$, que se definen como valores esperados de las funciones de $X$ e $Y$, proporcionan esta información.\n\n### Valor esperado de una función de dos variables aleatorias\nSea $(X,Y)$ una variable aleatoria bidimensional. \n\nSea $P_{XY}$ su **función de probabilidad conjunta** en el caso en que $(X,Y)$ sea **discreta** y $f_{XY}$ su **función de densidad conjunta** en el caso en que $(X,Y)$ sea **continua**.\n\nSea $Z=g(X,Y)$ una **variable aleatoria unidimensional** función de las variables $X$ e $Y$. Por ejemplo: \n\n* Suma de las dos variables $g(x,y)=x+y$: $Z=X+Y$.\n* Producto de las dos variables $g(x,y)=x\\cdot y$: $Z=X\\cdot Y$.\n* Suma de los cuadrados de las variables $g(x,y)=x^2+y^2$: $Z=X^2+Y^2$.\n\nHay que tener en cuenta que $Z$, como **variable aleatoria unidimensional** tiene una **función de probabilidad** $P_Z$ en el caso en que $(X,Y)$ sea discreta y una **función de densidad** $f_Z$ en el caso en que $(X,Y)$ sea continua.\n\nEl siguiente resultado nos dice cómo calcular el **valor esperado** de $Z$ sin tener que calcular $P_Z$ o $f_Z$, sólo usando la información de la **variable aleatoria conjunta** $(X,Y)$:\n\n<l class=\"prop\">Proposición. </l>\nEl valor esperado de $Z$ se puede hallar usando la expresión siguiente:\n\n* en el caso en que $(X,Y)$ sea discreta con $(X,Y)(\\Omega)=\\{(x_i,y_j),\\ i=1,2,\\ldots, j=1,2,\\ldots\\}$,\n$$\nE(Z)  = E(g(X,Y))  =\\sum_{x_i}\\sum_{y_j}g(x_i,y_j)\\cdot P(x_i,y_j),\n$$\n\n* en el caso en que $(X,Y)$ sea continua:\n$$\nE(Z)=E(g(X,Y))=\\int_{-\\infty}^\\infty \\int_{-\\infty}^\\infty g(x,y)\\cdot f_{XY}(x,y)\\, dx\\, dy.\n$$\n\n#### Ejemplos\n<div class=\"example\">\n**Ejemplo: suma y producto de dos dados (continuación)**\n\nConsideremos el ejemplo de la variable $(S,P)$ que nos daba la suma y el producto de los resultados cuando lanzábamos dos veces un dado. \n\nVamos a calcular $E(S+P)$.\n\n<div class=\"example-sol\">\n\nRecordemos que ya hemos calculado $P_{SP}$. La expresión de $E(S+P)$ es:\n\n$$\n\\begin{array}{rl}\nE(S+P) & = (2+1)\\cdot P_{SP}(2,1)+(3+2)\\cdot P_{SP}(3,2)+(4+3)\\cdot P_{SP}(4,3)  \\\\ &\n\\quad +(4+4)\\cdot P_{SP}(4,4) + (5+4)\\cdot P_{SP}(5,4)+(5+6)\\cdot P_{SP}(5,6)\\\\ & \n\\quad +(6+5)\\cdot P_{SP}(6,5)+(6+8)\\cdot P_{SP}(6,8)+ (6+9)\\cdot P_{SP}(6,9) \\\\ &\n\\quad + (7+6)\\cdot P_{SP}(7,6)+(7+10)\\cdot P_{SP}(7,10)+(7+12)\\cdot P_{SP}(7,12)\\\\ & \n\\quad + (8+12)\\cdot P_{SP}(8,12)+(8+15)\\cdot P_{SP}(8,15)+(8+16)\\cdot P_{SP}(8,16)\\\\ & \n\\quad +(9+18)\\cdot P_{SP}(9,18)+ (9+20)\\cdot P_{SP}(9,20)\\\\ & \n\\quad +(10+24)\\cdot P_{SP}(10,24) +(10+25)\\cdot P_{SP}(10,25)\\\\ &\n\\quad +(11+30)\\cdot P_{SP}(11,30)  + (12+36)\\cdot P_{SP}(12,36) \\\\ & \n=  3\\cdot \\frac{1}{36}+5\\cdot\\frac{2}{36}+7\\cdot \\frac{2}{36}+8\\cdot \\frac{1}{36}+9\\cdot \\frac{2}{36}+11\\cdot\\frac{2}{36}+11\\cdot \\frac{2}{36}+14\\cdot\\frac{2}{36}\n\\\\ &  \\quad  +15\\cdot\\frac{1}{36} + 13\\cdot\\frac{2}{36}+17\\cdot\\frac{2}{36}+19\\cdot\\frac{2}{36}+20\\cdot\\frac{2}{36}+23\\cdot\\frac{2}{36}+24\\cdot\\frac{1}{36}\n\\\\ & \\quad+27\\cdot\\frac{2}{36}+29\\cdot\\frac{2}{36} + 34\\cdot\\frac{2}{36}+35\\cdot\\frac{1}{36}+41\\cdot\\frac{2}{36}+48\\cdot\\frac{1}{36}\\\\\n& =\\frac{`r 3+10+14+8+18+22+22+14*2+15+13*2+17*2+19*2+20*2+23*2+24+27*2+29*2+34*2+35+41*2+48`}{36}= `r (3+10+14+8+18+22+22+14*2+15+13*2+17*2+19*2+20*2+23*2+24+27*2+29*2+34*2+35+41*2+48)/36`.\n\\end{array}\n$$\n\nHallar el valor esperado de la suma $E(S+P)$ una vez hallada la tabla de la **función de probabilidad conjunta**, en `R` es bastante sencillo usando la función `outer`: \n\n```{r}\nvalores.suma = as.integer(rownames(tabla.func.prob.conjunta))\nvalores.producto = as.integer(colnames(tabla.func.prob.conjunta))\nsuma.valores = outer(valores.suma,valores.producto,\"+\")\n(valor.esperado.suma = sum(suma.valores*tabla.func.prob.conjunta))\n```\n</div>\n</div>\n\n<l class=\"observ\">Observación:</l>\nEn `R` para hallar el valor esperado de una función $g(X,Y)$, $E(g(X,Y))$ de las variables aleatorias $X$ e $Y$, basta sustituir el valor `+` en el script anterior por `FUN=g`, definiendo previamente la función `g`.\n\n<div class=\"example\">\n**Ejemplo: otra densidad (continuación)**\n\nRecordemos el ejemplo donde $(X,Y)$ era una variable aleatoria bidimensional continua con **función de densidad conjunta**:\n$$\nf_{XY}(x,y)=\\begin{cases}\n2\\cdot \\mathrm{e}^{-x}\\cdot\\mathrm{e}^{-y}, & 0\\leq y\\leq x < \\infty,\\\\\n0, & \\mbox{ en caso contrario.}\n\\end{cases}\n$$\nCalculemos $E(X\\cdot Y)$:\n<div class=\"example-sol\">\n$$\n\\begin{array}{rl}\nE(X\\cdot Y) & =\\displaystyle \\int_{x=0}^{x=\\infty} \\int_{y=0}^{y=x} 2\\cdot x\\cdot y \\mathrm{e}^{-x}\\cdot\\mathrm{e}^{-y}\\, dy\\, dx\\\\\n& =\\displaystyle  2\\cdot\\int_{x=0}^{x=\\infty} x \\cdot\\mathrm{e}^{-x} \\cdot\\int_{y=0}^{y=x}  y \\cdot\\mathrm{e}^{-y}\\, dy\\, dx\\\\\n& =\\displaystyle  2\\int_{x=0}^{x=\\infty}x\\cdot \\mathrm{e}^{-x} \\left[-\\mathrm{e}^{-y}\\cdot (y+1)\\right]_{y=0}^{y=x}\\, dx\\\\\n& =\\displaystyle  2\\cdot\\int_{x=0}^{x=\\infty}x \\cdot\\mathrm{e}^{-x} \\cdot\\left(1-\\mathrm{e}^{-x}(x+1)\\right)\\, dx \\\\ \n&= \\displaystyle 2\\cdot\\int_{x=0}^{x=\\infty}x\\cdot\\left( \\mathrm{e}^{-x}-\\mathrm{e}^{-2x}\\right)-x^2\\cdot\\mathrm{e}^{-2x}\\, dx \\\\ \n& =\\displaystyle  2\\cdot\\left[-\\mathrm{e}^{-x}(x+1)+\\frac{1}{4}\\cdot\\mathrm{e}^{-2 x}(1+2x)+\\frac{1}{4} \\cdot\\mathrm{e}^{-2 x} \\left(2 x^2+2\n   x+1\\right)\\right]_{x=0}^{x=\\infty} \\\\\n   &= \\displaystyle 2\\cdot \\left(1-\\frac{1}{4}-\\frac{1}{4}\\right)=1.\n\\end{array}\n$$\nEn el último cálculo hemos usado integración por partes para integrar $\\int x\\mathrm{e}^{-x}\\,dx$, $\\int x\\mathrm{e}^{-2x}\\,dx$ y $\\int x^2\\mathrm{e}^{-2x}\\, dx$.\n</div>\n</div>\n\n<div class=\"exercise\">\n**Ejercicio**\n\nHallar $E(X+Y)$ para el ejemplo anterior.\n\n</div>\n\n### Valor esperado de una función de dos variables aleatorias independientes\n\nEl siguiente resultado nos simplifica el cálculo del **valor esperado de una función de dos variables aleatorias** en el caso en que sean **independientes**:\n\n<l class=\"prop\">Proposición: cálculo del valor esperado de una función de dos variables aleatorias en el caso de independencia. </l>\nSea $(X,Y)$ una variable aleatoria bidimensionaltal que $X$ e $Y$ son independientes. \nSea $Z=g(X,Y)$ una variable aleatoria unidimensional función de $X$ e $Y$  en la  que podemos \"separar\" las variables $x$ e $y$ en la función $g$. Es decir, existen dos funciones $g_x$ y $g_y$ tal que $g(x,y)=g_x(x)\\cdot g_y(y)$ para todo valor $x,y\\in\\mathbb{R}$. En este caso, el valor esperado de $Z$ se puede calcular como:\n$$\nE(Z)=E(g(X,Y))=E_X(g_x(X))\\cdot E_Y(g_y(Y)).\n$$\n\n\nEs decir, el cálculo de $E(g(X,Y))$ que es una suma doble en el caso de que $(X,Y)$ sea **discreta** o una integral doble en el caso en que $(X,Y)$ sea continua se transforma en el producto de dos sumas simples (caso **discreto**) o el producto de dos integrales simples (caso **continuo**):\n\n$$\n\\begin{array}{rl}\nE(Z) & =E(g(X,Y))=\\displaystyle\\left(\\sum_{x_i} g_x(x_i)\\cdot P_X(x_i)\\right)\\cdot \\left(\\sum_{y_j} g_y(y_j)\\cdot P_Y(y_j)\\right),\\\\ &\\ \\quad \\mbox{caso discreto},\\\\\nE(Z) & =E(g(X,Y))=\\displaystyle\\left(\\int_{-\\infty}^\\infty g_x(x)\\cdot f_X(x)\\, dx\\right)\\cdot \\left(\\int_{-\\infty}^\\infty g_y(y)\\cdot f_Y(y)\\right), \\\\  &\\ \\quad \\mbox{caso continuo}.\n\\end{array}\n$$\n\nUn caso particular de aplicación de la proposición anterior  es el calculo de  $E(X\\cdot Y)$ cuando $X$ e $Y$ son independientes. En este caso $g(x,y)=x\\cdot y$, $g_x(x)=x$, y $g_y(y)=y$. \n\nPodemos escribir, por tanto:\n$$\nE(X\\cdot Y)=E_X(X)\\cdot E_Y(Y).\n$$\n\n#### Ejemplos\n<div class=\"example\">\n**Ejemplo: lanzar dos veces un dado (continuación)**\n\nRecordemos el experimento aleatorio que consiste en lanzar un dado dos veces. Sea $X$ el resultado del primer lanzamiento e $Y$, el resultado del segundo lanzamiento.\n\nHemos visto que $X$ e $Y$ son independientes. \n\nLas marginales de $X$ e $Y$ recordemos que son las siguientes:\n<div class=\"center\">\n| $X$ o $Y$| 1   |2 | 3 | 4 | 5 |  6\n|----|---|---|---|---|---|---\n| $P_X(i)$ o $P_Y(i)$ | $\\frac{1}{6}$ | $\\frac{1}{6}$ | $\\frac{1}{6}$ | $\\frac{1}{6}$ | $\\frac{1}{6}$ | $\\frac{1}{6}$\n</div>\nCalculemos $E(X\\cdot Y)$ usando la proposición anterior:\n<div class=\"example-sol\">\n$$\nE(X\\cdot Y)=\\displaystyle E_X(X)\\cdot E_Y(Y)=\\left(\\sum_{i=1}^6 i\\cdot \\frac{1}{6}\\right)\\cdot \\left(\\sum_{i=1}^6 i\\cdot \\frac{1}{6}\\right)=\\left(\\frac{21}{6}\\right)^2 = `r (21/6)^2`.\n$$\nDejamos como ejercicio el cálculo de $E(X\\cdot Y)$ usando la **función de probabilidad conjunta**  $P_{XY}$ y comprobar que da el mismo resultado.\n</div>\n</div>\n\n<div class=\"example\">\n**Ejemplo: densidad uniforme cuadrado unidad**\n\nRecordemos la variable aleatoria bidimensional continua con **función de densidad conjunta**:\n$$\nf_{XY}(x,y)=\\displaystyle \\begin{cases}\n1, & \\mbox{ si }0\\leq x\\leq 1,\\ 0\\leq y\\leq 1, \\\\\n0, & \\mbox{en caso contrario.}\n\\end{cases}\n$$\ndonde vimos que $X$ e $Y$ eran independientes y de distribución uniforme en el intervalo $[0,1]$.\n\nCalculemos $E(X\\cdot Y)$ usando la proposición:\n\n<div class=\"example-sol\">\n\n$$\n\\begin{array}{rl}\nE(X\\cdot Y)= & \\displaystyle E_X(X)\\cdot E_Y(Y)=\\int_0^1 x\\cdot 1\\, dx\\cdot \\int_0^1 y\\cdot 1\\, dy\\\\\n& =\\left[\\frac{x^2}{2}\\right]_{x=0}^{x=1}\\cdot \\left[\\frac{y^2}{2}\\right]_{y=0}^{y=1}=\\frac{1}{2}\\cdot \\frac{1}{2}=\\frac{1}{4}.\n\\end{array}\n$$\nDejamos como ejercicio el cálculo de $E(X\\cdot Y)$ usando la **función de densidad conjunta**  $f_{XY}$ y comprobar que da el mismo resultado.\n</div>\n</div>\n\n### Momentos conjuntos\n\nA continuación vamos a definir el momento de orden $(k,l)$ para una variable aleatoria bidimensional $(X,Y)$ para intentar obtener información de su comportamiento conjunto:\n\n<l class=\"definition\">Definición de momento conjunto. </l>\nSean $(X,Y)$ una variable aleatoria bidimensional con **función de probabilidad conjunta** $P_{XY}$ en el caso discreto y **función de densidad conjunta** $f_{XY}$ en el caso continuo. Dados $k$ y $l$ números enteros positivos, definimos el **momento conjunto de orden $(k,l)$** para la variable $(X,Y)$ como:\n$$\nE\\left(X^k Y^l\\right)=\\begin{cases}\n\\displaystyle \\sum_{x_i}\\sum_{y_j} x_i^k y_j^l P_{XY}(x_i,y_j), & \\mbox{ caso discreto,} \\\\\\displaystyle\n\\int_{-\\infty}^\\infty\\int_{-\\infty}^\\infty x^k y^l f_{XY}(x,y)\\, dx\\, dy. & \\mbox{ caso continuo.}\n\\end{cases}\n$$\n\n<l class=\"observ\">Observación.</l>\nSi consideramos $l=0$, los momentos conjuntos de orden $(k,0)$ coinciden con los momentos de orden $k$ de la variable aleatoria $X$. \n\nDe la misma forma, considerando $k=0$, los momentos conjuntos de orden $(0,l)$ coinciden con los momentos de orden $l$ de la variable aleatoria $Y$.\n\nPara $l=1$ y $k=1$ obtenemos el momento de orden $(1,1)$ ya visto anteriormente: $E(X\\cdot Y)$, denominado **correlación entre las variables $X$ e $Y$**. Si dicha correlación es cero, $E(X\\cdot Y)=0$, se dice que las variables $X$ e $Y$ son **ortogonales**.\n\n### Momentos conjuntos centrados en las medias\nA continuación definamos los **momentos conjuntos centrados en las medias**:\n\n<l class=\"definition\">Definición de momento conjunto. </l>\nSean $(X,Y)$ una variable aleatoria bidimensional con **función de probabilidad conjunta** $P_{XY}$ en el caso discreto y **función de densidad conjunta** $f_{XY}$ en el caso continuo. Sean $\\mu_X=E(X)$ y $\\mu_Y=E(Y)$ los **valores esperados** de las variables $X$ e $Y$, respectivamente. Dados $k$ y $l$ números enteros positivos, definimos el **momento conjunto de orden $(k,l)$ centrado en las medias** para la variable $(X,Y)$ como:\n$$\nE\\left((X-\\mu_X)^k\\cdot  (Y-\\mu_Y)^l\\right)=\\begin{cases}\n\\sum_{x_i}\\sum_{y_j} (x_i-\\mu_X)^k \\cdot (y_j-\\mu_Y)^l\\cdot  P_{XY}(x_i,y_j), & \\\\\\ \\qquad \\mbox{ caso discreto,}& \\\\\n\\int_{-\\infty}^\\infty\\int_{-\\infty}^\\infty (x-\\mu_X)^k\\cdot  (y-\\mu_Y)^l\\cdot  f_{XY}(x,y)\\, dx\\, dy. & \\\\ \\ \\qquad\\mbox{ caso continuo.} &\n\\end{cases}\n$$\n\n### Covariancia entre las variables\nEl **momento conjunto centrado en las medias para $k=1$ y $l=1$** se denomina **covariancia** entre las variables $X$ e $Y$:\n$$\n\\mathrm{Cov}(X,Y)=E((X-\\mu_X)\\cdot (Y-\\mu_Y)).\n$$\nLa covariancia puede calcularse a partir de la **correlación** entre las variables:\n$$\n\\mathrm{Cov}(X,Y)=E((X-\\mu_X) \\cdot (Y-\\mu_Y))=E(X\\cdot Y)-\\mu_X\\cdot \\mu_Y,\n$$\n\nya que, usando las propiedades de la esperanza, tenemos:\n$$\n\\begin{array}{rl}\nE((X-\\mu_X)\\cdot (Y-\\mu_Y)) & =E(X\\cdot Y-\\mu_Y \\cdot X-\\mu_X \\cdot Y+\\mu_X\\cdot \\mu_Y)\\\\ & =E(X\\cdot Y)-\\mu_Y\\cdot E(X)-\\mu_X \\cdot E(Y)+\\mu_X\\cdot \\mu_Y \\\\ &  = E(X\\cdot Y)-\\mu_Y\\cdot \\mu_X-\\mu_X \\cdot \\mu_Y+\\mu_X\\cdot \\mu_Y \\\\ & = E(X\\cdot Y)-\\mu_X\\cdot \\mu_Y.\n\\end{array}\n$$\n\n<l class=\"observ\">Observación. </l>\nSi las variables $X$ e $Y$ son **independientes**, su **covarianza** es nula ya que vimos que $E(X\\cdot Y)=\\mu_X\\cdot \\mu_y$.\n\nLa **covarianza** es una medida de lo relacionadas están las variables $X$ e $Y$:\n\n* Si cuando $X\\geq \\mu_X$, también ocurre que $Y\\geq \\mu_Y$ o viceversa, cuando $X\\leq \\mu_X$, también ocurre que $Y\\leq \\mu_Y$, el valor $(X-\\mu_X)(Y-\\mu_Y)$ será positivo y la **covarianza** será positiva.\n\n* Si por el contrario, cuando $X\\geq \\mu_X$, también ocurre que $Y\\leq \\mu_Y$ o viceversa, cuando $X\\leq \\mu_X$, también ocurre que $Y\\geq \\mu_Y$, el valor $(X-\\mu_X)(Y-\\mu_Y)$ será negativo y la **covarianza** será negativa.\n\n* En cambio, si a veces ocurre una cosa y a veces ocurre otra, la **covarianza** va cambiando de signo y puede tener un valor cercano a 0.\n\n#### Propiedades de la covarianza\n* Sea $(X,Y)$ una variable aleatoria bidimensional. Entonces la **varianza de la suma/resta** se calcula usando la expresión siguiente:\n$$\n\\mathrm{Var}(X\\pm Y)=\\mathrm{Var}(X)+\\mathrm{Var}(Y)\\pm 2 \\mathrm{Cov}(X,Y).\n$$\n\n<div class=\"dem\">\n**Demostración**\n\nLa varianza de la suma/resta de las variables es, usando la propiedad de la **varianza**:\n$$\n\\mathrm{Var}(X\\pm Y)=E\\left((X\\pm Y)^2\\right)-\\left(E(X\\pm Y)\\right)^2.\n$$\nDesarrollando las expresiones anteriores, obtenemos:\n$$\n\\begin{array}{rl}\n\\mathrm{Var}(X\\pm Y) & =E\\left(X^2+Y^2\\pm 2XY\\right)-\\left(E(X)\\pm E(Y)\\right)^2 \\\\ & =\nE(X^2)+E(Y^2)\\pm 2\\cdot E(X\\cdot Y) \\\\ &\\qquad\\qquad - \\left(E(X)^2+E(Y)^2\\pm 2\\cdot E(X)\\cdot E(Y)\\right)\n\\\\ & = E(X^2)-E(X)^2+E(Y^2)-E(Y)^2\\\\ &\\qquad\\qquad \\pm 2\\cdot (E(X\\cdot Y)-E(X)\\cdot E(Y)) \\\\ & = \\mathrm{Var}(X)+\\mathrm{Var}(Y)\\pm 2\\cdot \\mathrm{Cov}(X,Y),\n\\end{array}\n$$\ntal como queríamos ver.\n\n</div>\n\nUna consecuencia de la propiedad anterior es el resultado siguiente:\n\n<l class=\"prop\">Proposición: si las variables son independientes, la varianza de la suma es la suma de varianzas. </l>\nSea $(X,Y)$ una variable aleatoria bidimensional donde las variables $X$ e $Y$ son **independientes**. \nEntonces:\n$$\n\\mathrm{Var}(X+Y)=\\mathrm{Var}(X)+\\mathrm{Var}(Y).\n$$\n\n\n<div class=\"dem\">\n**Demostración**\n\nLa demostración es muy sencilla: basta aplicar la fórmula  de la varianza  de la suma y tener en cuenta que, como $X$ e $Y$ son independientes, su covarianza es cero: $\\mathrm{Cov}(X,Y)=0$.\n</div>\n\n### Coeficiente de correlación entre las variables\n\nLa **covarianza** depende de las unidades en las que están las variables $X$ e $Y$ ya que si $a>0$ y $b>0$, entonces:\n$$\n\\mathrm{Cov}(a\\cdot X,b\\cdot Y)=a\\cdot b\\cdot \\mathrm{Cov}(X,Y).\n$$\nPor tanto, si queremos \"medir\" la relación que existe entre las variables $X$ e $Y$ tendremos que \"normalizar\" la **covarianza** definiendo el **coeficiente de correlación** entre las variables $X$ e $Y$:\n\n<l class=\"definition\">Definición del coeficiente de correlación. </l>\nSea $(X,Y)$ una variable aleatoria bidimensional. Se define el **coeficiente de correlación** entre las variables $X$ e $Y$ como: \n$$\n\\rho_{XY}=\\frac{\\mathrm{Cov}(X,Y)}{\\sqrt{\\mathrm{Var}(X)}\\cdot\\sqrt{\\mathrm{Var}(Y)}}=\\frac{E(X\\cdot Y)-\\mu_X\\cdot \\mu_Y}{\\sqrt{E\\left(X^2\\right)-\\mu_X^2}\\cdot \\sqrt{E\\left(Y^2\\right)-\\mu_Y^2}}.\n$$\n\n<l class=\"observ\">Observación. </l>\nSi las variables $X$ e $Y$ son **independientes**, su **coeficiente de correlación** $\\rho_{XY}=0$ es nulo ya que su **covarianza** lo es.\n\nNotemos también que la **correlación** no tiene unidades y es invariante a cambios de escala.\n\nAdemás, la **covarianza** de las **variables tipificadas** $\\frac{X-\\mu_X}{\\sigma_X}$ y $\\frac{Y-\\mu_Y}{\\sigma_Y}$ coincide con la **correlación** de $X$ e $Y$.\n\nEl **coeficiente de correlación** es un valor normalizado ya que siempre está entre -1 y 1: $-1\\leq\\rho_{XY}\\leq 1$.\n\n<div class=\"dem\">\nPara  demostrar de este hecho, sean $\\mu_X=E(X)$, $\\mu_Y=E(Y)$, $\\sigma_X=\\sqrt{\\mathrm{Var}(X)}$ y $\\sigma_Y=\\sqrt{\\mathrm{Var}(Y)}$. \n\nConsideremos la variable $Z=\\left(\\frac{X-\\mu_X}{\\sigma_X}\\pm \\frac{Y-\\mu_Y}{\\sigma_Y}\\right)^2$. Como $Z\\geq 0$, tenemos que $E(Z)\\geq 0$. Desarrollemos el valor de $E(Z)$:\n\n$$\n\\begin{array}{rl}\nE(Z) & = E\\left(\\frac{X-\\mu_X}{\\sigma_X}\\pm \\frac{Y-\\mu_Y}{\\sigma_Y}\\right)^2 = E\\left(\\left(\\frac{X-\\mu_X}{\\sigma_X}\\right)^2+\\left(\\frac{Y-\\mu_Y}{\\sigma_Y}\\right)^2\\pm 2\\left(\\frac{X-\\mu_X}{\\sigma_X}\\right)\\cdot  \\left(\\frac{Y-\\mu_Y}{\\sigma_Y}\\right)\\right) \\\\ & =\nE\\left(\\left(\\frac{X-\\mu_X}{\\sigma_X}\\right)^2\\right)+E\\left(\\left(\\frac{Y-\\mu_Y}{\\sigma_Y}\\right)^2\\right)\\pm 2\\cdot  E\\left(\\left(\\frac{X-\\mu_X}{\\sigma_X}\\right) \\cdot \\left(\\frac{Y-\\mu_Y}{\\sigma_Y}\\right)\\right) \\\\ & =\n\\frac{1}{\\sigma_X^2}E\\left(\\left(X-\\mu_X\\right)^2\\right)+\\frac{1}{\\sigma_Y^2}E\\left(\\left(Y-\\mu_Y\\right)^2\\right)\\pm \\frac{2}{\\sigma_X\\cdot \\sigma_Y}E\\left(\\left(X-\\mu_X\\right) \\left(Y-\\mu_Y\\right)\\right) \\\\ & = \\frac{1}{\\sigma_X^2}\\sigma_X^2+\n\\frac{1}{\\sigma_Y^2}\\sigma_Y^2 \\pm\\frac{2}{\\sigma_X\\cdot \\sigma_Y} \\mathrm{Cov}(X,Y) = 1+1\\pm 2\\cdot \\frac{\\mathrm{Cov}(X,Y)}{\\sigma_X\\sigma_Y}=2\\cdot (1\\pm\\rho_{XY})\n\\end{array}\n$$\n\nAhora, como $E(Z)\\geq 0$, tenemos que $1\\pm \\rho_{XY}\\geq 0$, lo que significa que, por un lado $1+\\rho_{XY}\\geq 0$ y, por otro, $1-\\rho_{XY}\\geq 0$. De la primera inecuación, deducimos que $\\rho_{XY}\\geq -1$ y de la segunda, $\\rho_{XY}\\leq 1$. \n\nEn resumen, $-1\\leq\\rho_{XY}\\leq 1$, tal como queríamos ver.\n</div>\n\n\n#### Ejemplos\n<div class=\"example\">\n**Ejemplo: otra densidad (continuación)**\n\nHallemos el **coeficiente de correlación** para el ejemplo de la variable aleatoria bidimensional continua con **función de densidad conjunta**:\n\n$$\nf_{XY}(x,y)=\\begin{cases}\n2\\cdot  \\mathrm{e}^{-x}\\cdot \\mathrm{e}^{-y}, & 0\\leq y\\leq x < \\infty,\\\\\n0, & \\mbox{ en caso contrario,}\n\\end{cases}\n$$\nRecordemos los cálculos realizados anteriormente:\n\n\n<div class=\"example-sol\">\n\n* $E(X\\cdot Y)=1.$\n\n* $f_X(x)=2\\cdot \\left(\\mathrm{e}^{-x}-\\mathrm{e}^{-2x}\\right)$, si $x\\geq 0$. Su esperanza es:\n\n$$\n\\begin{array}{rl}\nE(X)&=\\int_0^\\infty x\\cdot 2\\left(\\mathrm{e}^{-x}-\\mathrm{e}^{-2x}\\right)\\, dx=2 \\left[\\frac{1}{4} \\mathrm{e}^{-2 x} (2 x+1)-\\mathrm{e}^{-x}(x+1)\\right]_0^\\infty \\\\\n& = 2\\left(1-\\frac{1}{4}\\right)=\\frac{3}{2}.\n\\end{array}\n$$\n\nCalculemos a continuación su varianza: $\\mathrm{Var}(X)=E\\left(X^2\\right)-\\mu_X^2$. El valor de $E\\left(X^2\\right)$ es:\n$$\n\\begin{array}{rl}\nE\\left(X^2\\right) & =\\displaystyle \\int_0^\\infty x^2 \\cdot 2\\cdot \\left(\\mathrm{e}^{-x}-\\mathrm{e}^{-2\\cdot x}\\right)\\, dx\\\\\n&=\\displaystyle 2 \\cdot  \\left[\\frac{1}{4} \\cdot \\mathrm{e}^{-2 \\cdot x} \\cdot  (2\\cdot x^2+2\\cdot x+1)- \\mathrm{e}^{-x} \\cdot (x^2+2\\cdot x+2)\\right]_0^\\infty \\\\ & = 2\\cdot \\left(2-\\frac{1}{4}\\right)=\\frac{7}{2}.\n\\end{array}\n$$\n\nEl valor de la varianza de $X$ es: $\\mathrm{Var}(X)=\\frac{7}{2}-\\left(\\frac{3}{2}\\right)^2 = \\frac{5}{4}.$\n\n* La variable $Y$ era exponencial de parámetro $\\lambda =2$. Por tanto, $E(Y)=\\frac{1}{2}$, $\\mathrm{Var}(Y)=\\frac{1}{4}$.\n\nEl **coeficiente de correlación** entre las variables $X$ e $Y$ es:\n$$\n\\rho_{XY}=\\frac{E(X\\cdot Y)-\\mu_X\\cdot \\mu_Y}{\\sqrt{\\mathrm{Var}(X)}\\cdot\\sqrt{\\mathrm{Var}(Y)}}=\\frac{1-\\frac{3}{2}\\cdot \\frac{1}{2}}{\\sqrt{\\frac{5}{4}}\\cdot\\sqrt{\\frac{1}{4}}}=\\frac{\\sqrt{5}}{5}\\approx `r round(sqrt(5)/5,3)`.\n$$\nVemos que la **correlación** entre las variables $X$ e $Y$ es positiva pero no demasiado ya que su valor no está cercano a 1.\n</div>\n</div>\n\n<div class=\"example\">\n**Ejemplo: normal bidimensional**\n\nRecordemos que la **función de densidad** de la variable aleatoria **normal bidimensional** es:\n$f_{XY}(x,y)=\\frac{1}{2\\cdot \\pi\\cdot \\sqrt{1-\\rho^2}}\\mathrm{e}^{-\\frac{(x^2-2\\cdot \\rho \\cdot x\\cdot y+y^2)}{2\\cdot (1-\\rho^2)}},\\ -\\infty <x,y<\\infty.$\n\nLas **variables aleatorias marginales** son normales estándar o $N(0,1)$.\n\nHallemos el **coeficiente de correlación $\\rho_{XY}$** en este caso.\n\n\n\n<div class=\"example-sol\">\nCalculemos $E(X\\cdot Y)$:\n\n\n\n$$\n\\begin{array}{rl}\nE(X\\cdot Y) & = \\displaystyle\\int_{-\\infty}^\\infty x y \\frac{1}{2\\cdot\\cdot\\pi\\cdot\\sqrt{1-\\rho^2}}\\mathrm{e}^{-\\frac{(x^2-2\\rho\\cdot x\\cdot y+y^2)}{2(1-\\rho^2)}}\\, dy\\, dx \\\\\n& = \\displaystyle\\frac{1}{2\\pi\\sqrt{1-\\rho^2}}\\int_{x=-\\infty}^{x=\\infty}x\\cdot  \\mathrm{e}^{-\\frac{x^2}{2\\cdot(1-\\rho^2)}}\\int_{y=-\\infty}^{y=\\infty}y \\mathrm{e}^{-\\frac{(-2\\cdot\\rho\\cdot  x\\cdot y+y^2)}{2\\cdot(1-\\rho^2)}}\\, dy\\, dx \\\\ & = \\displaystyle\\frac{1}{2\\cdot\\pi\\cdot\\sqrt{1-\\rho^2}}\\int_{x=-\\infty}^{x=\\infty}x  \\mathrm{e}^{-\\frac{x^2}\\cdot{2(1-\\rho^2)}}  \\mathrm{e}^{\\frac{\\rho^2\\cdot x^2}{2\\cdot(1-\\rho^2)}} \\int_{y=-\\infty}^{y=\\infty}y \\mathrm{e}^{-\\frac{(y-\\rho y)^2}{2\\cdot(1-\\rho^2)}}\\, dy\\, dx,\\\\ &\\ \\qquad\\mbox{ cambio de variable en la segunda integral } z=\\frac{y-\\rho x}{\\sqrt{1-\\rho^2}},\\\\\n& = \\displaystyle\\frac{1}{2\\pi\\sqrt{1-\\rho^2}}\\int_{x=-\\infty}^{x=\\infty}x  \\mathrm{e}^{-\\frac{x^2}{2}}  \\int_{z=-\\infty}^{z=\\infty} \\left(z \\sqrt{1-\\rho^2}+\\rho x\\right) \\sqrt{1-\\rho^2}\\mathrm{e}^{-\\frac{z^2}{2}}\\, dz\\, \\\\\n& = \\displaystyle\\frac{1}{2\\pi} \\int_{x=-\\infty}^{x=\\infty}x  \\mathrm{e}^{-\\frac{x^2}{2}}\\left(\\sqrt{1-\\rho^2}\\int_{z=-\\infty}^{z=\\infty} z\\cdot \\mathrm{e}^{-\\frac{z^2}{2}}\\, dz +\\rho\\cdot x \\int_{z=-\\infty}^{z=\\infty}\\mathrm{e}^{-\\frac{z^2}{2}}\\, dz \\right)\\, dx\n\\end{array}\n$$\n\nAhora, usando que el valor esperado de una variable $N(0,1)$ es cero tenemos que:\n$\\int_{z=-\\infty}^{z=\\infty} z \\mathrm{e}^{-\\frac{z^2}{2}}\\, dz =0,$ y usando que la integral de la **función de densidad** de la $N(0,1)$ ($\\frac{1}{\\sqrt{2\\pi}}\\mathrm{e}^{-\\frac{z^2}{2}}$) sobre todo $\\mathbb{R}$ es 1, tenemos que:\n$\\int_{z=-\\infty}^{z=\\infty} \\mathrm{e}^{-\\frac{z^2}{2}}\\, dz =\\sqrt{2\\pi}.$\n\nPor tanto,\n$$\nE(X\\cdot Y)=\\frac{\\rho}{2\\pi} \\int_{x=-\\infty}^{x=\\infty} x^2  \\mathrm{e}^{-\\frac{x^2}{2}}\\sqrt{2\\pi}\\, dx=\\frac{\\rho}{\\sqrt{2\\pi}}\\int_{x=-\\infty}^{x=\\infty} x^2  \\mathrm{e}^{-\\frac{x^2}{2}}\\, dx.\n$$\nPor último, usando que la varianza de la distribución $Z=N(0,1)$ es 1, tenemos que $\\mathrm{Var}(Z)=E\\left(Z^2\\right)-E(Z)^2$. Como $E(Z)=0$, deducimos que $E\\left(Z^2\\right)=1$:\n$$\n\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^\\infty x^2\\mathrm{e}^{-\\frac{x^2}{2}}\\, dx=1,\\ \\Rightarrow \\int_{-\\infty}^\\infty x^2\\mathrm{e}^{-\\frac{x^2}{2}}\\, dx=\\sqrt{2\\pi}.\n$$\nEl valor de $E(X\\cdot Y)$ es:\n$$\nE(X\\cdot Y)=\\frac{\\rho}{\\sqrt{2\\pi}}\\sqrt{2\\pi}=\\rho.\n$$\n\nLa correlación entre las variables $X$ e $Y$ es precisamente $\\rho$.\n\nAhora, usando que $\\mu_X=\\mu_Y=0$ y $\\sigma_X=\\sigma_Y=1$ ya que recordemos que las marginales son $N(0,1)$, el **coeficiente de correlación** entre las variables $X$ e $Y$ es:\n$$\n\\rho_{XY}=\\frac{E(X\\cdot Y)-\\mu_X\\cdot \\mu_Y}{\\sqrt{\\mathrm{Var}(X)}\\cdot\\sqrt{\\mathrm{Var}(Y)}}=\\frac{\\rho-0\\cdot 0}{1\\cdot 1}=\\rho.\n$$\nPor tanto, $\\rho$ es el **coeficiente de correlación** entre las variables $X$ e $Y$ y mide lo correlacionadas que están dichas variables.\n</div>\n</div>\n\n### Incorrelación e independencia\n\nHemos visto que si dos variables $X$ e $Y$ son **independientes**, entonces son **incorreladas**, es decir,  la **covarianza** es 0 ($E(X\\cdot Y)=E(X)\\cdot E(Y)$).\n\nEl recíproco, sin embargo, es falso. Veamos un ejemplo de variables **incorreladas** que no son independientes.\n\n<div class=\"example\">\n**Ejemplo de variables aleatorias incorreladas pero no independientes**\n\nConsideremos la variable aleatoria bidimensional continua con **función de densidad**:\n$$\nf_{XY}(x,y)=\\begin{cases}\n\\frac{3}{8}(x^2+y^2), & \\mbox{si }(x,y)\\in [-1,1]\\times [-1,1],\\\\\n0, & \\mbox{en caso contrario.}\n\\end{cases}\n$$\n\nDejamos como ejercicio comprobar que es una **función de densidad**. Es decir, que es positiva y que la integral sobre todo el plano vale 1.\n\n<div class=\"example-sol\">\n\nCalculemos las **densidades marginales**:\n\n$$\n\\begin{array}{rl}\nf_X(x) & = \\int_{-1}^{1} \\frac{3}{8}\\cdot (x^2+y^2)\\, dy = \\frac{3}{8}\\cdot\\left[x^2\\cdot y+\\frac{y^3}{3}\\right]_{-1}^1 =\\frac{3}{8}\\cdot\\left(2 \\cdot x^2+\\frac{2}{3}\\right)=\\frac{3}{4}\n\\cdot x^2+\\frac{1}{4}, \\\\\nf_Y(y) & = \\int_{-1}^{1} \\frac{3}{8}\\cdot(x^2+y^2)\\, dx = \\frac{3}{8}\\cdot\\left[\\frac{x^3}{3}+y^2 x\\right]_{-1}^1 =\\frac{3}{8}\\cdot\\left(\\frac{2}{3}+2 y^2+\\right)=\\frac{3}{4}\\cdot y^2+\\frac{1}{4}.\n\\end{array}\n$$\n\nLos valores esperados de cada variable $X$ e $Y$ son:\n\n$$\n\\begin{array}{rl}\nE(X) & =\\int_{-1}^1 x \\cdot\\left(\\frac{3}{4} \\cdot x^2+\\frac{1}{4}\\right)\\, dx =0, \\mbox{al integrar una función impar,}\\\\\nE(Y) & =\\int_{-1}^1 x \\left(\\frac{3}{4}\\cdot y^2+\\frac{1}{4}\\right)\\, dx =0, \\mbox{al integrar una función impar.}\n\\end{array}\n$$\n\nEl valor de la **correlación** entre $X$ e $Y$ es:\n\n$$\n\\begin{array}{rl}\nE(X\\cdot Y) & =\\int_{-1}^1\\int_{-1}^1 x \\cdot y \\cdot \\frac{3}{8}\\cdot (x^2+y^2)\\, dy\\, dx\\\\ & =\\frac{3}{8}\\cdot\\left(\\int_{-1}^1\\int_{-1}^1 x^3 \\cdot y\\, dy \\, dx+\\int_{-1}^1\\int_{-1}^1 x\\cdot y^3\\, dy \\, dx\\right) \\\\ & = \\frac{3}{8} \\left(\\int_{x=-1}^{x=1}x^3 \\left[\\frac{y^2}{2}\\right]_{y=-1}^{y=1}\\, dx + \\int_{y=-1}^{y=1}y^3 \\left[\\frac{x^2}{2}\\right]_{x=-1}^{x=1}\\right)=0.\n\\end{array}\n$$\n\nEl **coeficiente de correlación** entre $X$ e $Y$ es: $\\rho_{XY}=E(X\\cdot Y)-E(X)\\cdot E(Y)=0-0\\cdot 0=0$. Por tanto, son **incorreladas**.\n\nEn cambio no son **independientes** ya que claramente si $(x,y)\\in [-1,1]\\times [-1,1]$,\n\n$$\nf_{XY}(x,y)=\\frac{3}{8}(x^2+y^2) \\neq f_X(x)\\cdot f_Y(y)=\\left(\\frac{3}{4} x^2+\\frac{1}{4}\\right)\\cdot \\left(\\frac{3}{4} y^2+\\frac{1}{4}\\right).\n$$\n\n</div>\n</div>\n\n## Variables aleatorias condicionales y valor esperado condicional\n\nMuchas **variables aleatorias bidimensionales** de interés práctico no son independientes.\n\nPor ejemplo, la salida $Y$ de un canal de comunicación debe depender de la entrada $X$ para transmitir información. \n\nEn esta sección vamos a introducir variables aleatorias $Y$ cuya distribución depende de otras $X$. Dichas variables se denominan **variables aleatorias condicionales**.\n\nTambién nos interesa el valor esperado de la **variable condicional** $Y$ suponiendo que conocemos $X=x$.\n\n### Variables aleatorias condicionales discretas\n\nSea $(X,Y)$ una variable aleatoria bidimensional. Sea $B$ un subconjunto de los números reales $\\mathbb{R}$. Recordemos que la **probabilidad condicional** del suceso $\\{Y\\in B\\}$ suponiendo que $X=x$ se definía de la forma siguiente:\n$$\nP(Y\\in B|X=x)=\\frac{P(Y\\in B,\\ X=x)}{P(X=x)}, \\mbox{ siempre que }P(X=x)>0.\n$$\n\nLa definición anterior motiva la definición siguiente de **variable aleatoria condicional discreta**:\n\n<l class=\"definition\">Definición de variable aleatoria condicional discreta. </l>\nSea $(X,Y)$ una variable aleatoria bidimensional discreta con conjunto de valores $(X,Y)(\\Omega)=\\{(x_i,y_j)\\ i=1,2,\\ldots, j=1,2,\\ldots\\}$ y **función de probabilidad conjunta** $P_{XY}$. Sean $x_i$ un valor de $X(\\Omega)$ con $P(X=x_i)>0$. Entonces definimos la **función de probabilidad** de la **variable aleatoria condicional discreta** $Y|X=x_i$ como:\n$$\nP_{Y|X=x_i}(y_j)=P(Y=y_j|X=x_i)=\\frac{P(X=x_i,\\ Y=y_j)}{P(X=x_i)}=\\frac{P_{XY}(x_i,y_j)}{P_X(x_i)}.\n$$\n\n\n¡<l class=\"observ\">Observación. </l>\nLa **función de probabilidad** de la **variable aleatoria condicional $Y|X=x_i$** depende únicamente de la **función de probabilidad conjunta** de la variable aleatoria bidimensional $(X,Y)$.\n\n<l class=\"observ\">Observación. </l>\nAl ser $Y|X=x_i$ una variable aleatoria unidimensional, su **función de probabilidad** tiene que verificar que la suma de todos sus valores tiene que dar 1. Es decir:\n$$\n\\sum_{y_j} P(Y=y_j|X=x_i)=1.\n$$\nVeámoslo:\n<div class=\"dem\">\n\n$$\n\\begin{array}{rl}\n\\sum_{y_j} P(Y=y_j|X=x_i) &=\\displaystyle \\sum_{y_j} \\frac{P_{XY}(x_i,y_j)}{P_X(x_i)}=\\frac{1}{P_X(x_i)}\\sum_{y_j} P_{XY}(x_i,y_j) \\\\ \n&=\\frac{1}{P_X(x_i)}\\cdot P_X(x_i)=1.\n\\end{array}\n$$\n</div>\n\n\n<l class=\"observ\">Observación. </l>\nSi $X$ e $Y$ son independientes, la distribución de $Y|X=x_iY$ es la misma que la de $Y$, es decir, la **variable aleatoria condicional $Y|X=x_i$** coincide con $Y$. Es decir, condicionar con $X=x_i$ no tiene ningún efecto sobre $Y$.\n\n<div class=\"dem\">\nEfectivamente, veamos que $P_{Y|X=x_i}(y_j)=P_Y(y_j)$ para todo valor $y_j$ de $Y(\\Omega)$:\n\n$$\n\\begin{array}{rl}\nP_{Y|X=x_i}(y_j) &=\\frac{P_{XY}(x_i,y_j)}{P_X(x_i)} \\stackrel{\\mbox{Por ser independientes}}{=}\\frac{P_Y(y_j)\\cdot P_X(x_i)}{P_X(x_i)}\\\\\n& =P_Y(y_j).\n\\end{array}\n$$\n</div>\n\n<l class=\"observ\">Observación. </l>\nLa definición de la **función de probabilidad** de la  **variable aleatoria condicional $X|Y=y_j$** se definiría de forma similar:\n\n$$\nP_{X|Y=y_j}(x_i)=P(X=x_i|Y=y_j)=\\frac{P(X=x_i,\\ Y=y_j)}{P(Y=y_j)}=\\frac{P_{XY}(x_i,y_j)}{P_Y(y_j)}, \n$$\npara todo $x_i\\in X(\\Omega)$.\n\n<l class=\"observ\">Observación.</l>\nSi tenemos la tabla de la **función de probabilidad conjunta** $P_{XY}$, para hallar la **función de distribución de la variable $Y|X=x_i$** es equivalente a considerar la fila del valor $x_i$ a la tabla y dividir todos los valores de la fila por la suma de los valores en dicha fila:\n\n<div class=\"center\">\n| $Y|X=x_i$| $y_1$    | $y_2$  | $\\ldots$ | $y_N$ |\n|----|----|----|----|----|\n| $P_{Y|X=x_i}$| $\\frac{P_{XY}(x_i,y_1)}{P_X(x_i)}$ | $\\frac{P_{XY}(x_i,y_2)}{P_X(x_i)}$ | $\\ldots$ | $\\frac{P_{XY}(x_i,y_N)}{P_X(x_i)}$|\n</div>\n\n<l class=\"observ\">Observación.</l>\nDe la misma manera, si tenemos la tabla de la **función de probabilidad conjunta** $P_{XY}$, para hallar la **función de distribución de la variable $X|Y=y_j$** es equivalente a considerar la columna del valor $y=y_j$ a la tabla y dividir todos los valores de la columna por la suma de los valores en dicha columna:\n\n<div class=\"center\">\n| $X|Y=y_j$|  $P_{X|Y=y_j}$\n|----|----|\n| $x_1$| $\\frac{P_{XY}(x_1,y_j)}{P_Y(y_j)}$ |\n| $\\vdots$| $\\vdots$ |\n| $x_M$| $\\frac{P_{XY}(x_M,y_j)}{P_Y(y_j)}$ |\n</div>\n\n#### Ejemplos\n\n<div class=\"example\">\n**Ejemplo de la suma y el producto de los resultados de dos lanzamientos de un dado**\n\nVamos a hallar la **variable aleatoria condicional $S|P=12$**.\n\nTenemos calculada la tabla de la **función de probabilidad conjunta $P_{SP}$**.\n\nSi $P=12$, los únicos valores $x_i$ de $S(\\Omega)$ para los que se verifica $P_{SP}(x_i,12)\\neq 0$ son 7 y 8.\n\nAdemás si calculamos $P_P(12)$, obtenemos $P(P=12)=\\frac{4}{36}$ ya que hay 4 casos en que el producto da 12: $(3,4), (4,3), (2,6)$ y $(6,2)$.\n\nPor tanto, la tabla de la **función de probabilidad condicional** de la variable $S|P=12$ es:\n\n<div class=\"center\">\n| $S|P=12$| $P_{S|P=12}$ \n|----|----|----|\n|$7$| $\\frac{\\frac{2}{36}}{\\frac{4}{36}}=\\frac{1}{2}$ | \n|$8$| $\\frac{\\frac{2}{36}}{\\frac{4}{36}}=\\frac{1}{2}$ |\n</div>\n</div>\n\n\n<div class=\"example\">\n**Ejemplo de la suma y el producto de los resultados de dos lanzamientos de un dado**\n\nVamos a hallar la **variable aleatoria condicional $P|S=8$**.\n\n<div class=\"example-sol\">\nSi $S=8$, los únicos valores $y_j$ de $P(\\Omega)$ para los que se verifica $P_{SP}(8,y_j)\\neq 0$ son 12 y 15 y 16.\n\nEl valor de $P_S(8)$ recordemos que valía: $P_S(8)=\\frac{5}{36}$.\n\nPor tanto, la tabla de la **función de probabilidad condicional** de la variable $P|S=8$ es:\n\n<div class=\"center\">\n| $P|S=8$| $12$ | $15$ | $16$ \n|----|----|----|----|\n|$P_{P|S=8}$| $\\frac{\\frac{2}{36}}{\\frac{5}{36}}=\\frac{2}{5}$ | $\\frac{\\frac{2}{36}}{\\frac{5}{36}}=\\frac{2}{5}$ | $\\frac{\\frac{1}{36}}{\\frac{5}{36}}=\\frac{1}{5}$\n</div>\n\nPara hallar la **variable aleatoria condicional** $S|P=12$ hemos de condicionar por la columna $P=12$ en la tabla de la **función de probabilidad conjunta**:\n\n```{r}\nprob.cond.p12=tabla.func.prob.conjunta[,valores.producto==12]/\n  sum(tabla.func.prob.conjunta[,valores.producto==12])\nprob.cond.p12\n```\n\nEl problema es que aparecen valores con **función de probabilidad marginal** nulos. Para eliminarlos hacemos lo siguiente:\n\n```{r}\nprob.cond.p12.buena = prob.cond.p12[prob.cond.p12!=0]\nprob.cond.p12.buena\n```\nPara hallar la **función de probabilidad marginal** $P|S=8$, haríamos lo siguiente:\n\n```{r}\nprob.cond.s8=tabla.func.prob.conjunta[valores.suma==8,]/\n  sum(tabla.func.prob.conjunta[valores.suma==8,])\n(prob.cond.s8.buena = prob.cond.s8[prob.cond.s8!=0])\n```\n\n</div>\n</div>\n\n### Variables aleatorias condicionales continuas\nLa definición en el caso continua se hace cambiando la **función de probabilidad** por la **función de densidad**:\n\n<l class=\"definition\">Definición de variable aleatoria condicional discreta. </l>\nSea $(X,Y)$ una variable aleatoria bidimensional continua con **función de densidad conjunta** $f_{XY}$. Sean $x\\in\\mathbb{R}$ con $f_X(x)>0$. Entonces definimos la **función de densidad** de la **variable aleatoria condicional continua** $Y|X=x$ como:\n$$\nf_{Y|X=x}(y)=\\frac{f_{XY}(x,y)}{f_X(x)}.\n$$\n\n<l class=\"observ\">Observación. </l>\nLa **función de densidad** de la **variable aleatoria condicional continua$Y|X$** depende únicamente de la **función de densidad conjunta** de la variable aleatoria bidimensional $(X,Y)$.\n\n<l class=\"observ\">Observación. </l>\nAl ser $Y|X=x$ una variable aleatoria unidimensional, su **función de densidad** tiene que verificar que la integral de dicha función sobre todo $\\mathbb{R}$ tiene que ser 1. Es decir:\n\n$$\n\\int_{-\\infty}^\\infty f_{Y|X=x}(y)\\, dy=1.\n$$\n\nVeámoslo:\n<div class=\"dem\">\n\n$$\n\\begin{array}{rl}\n\\int_{-\\infty}^\\infty f_{Y|X=x}(y)\\, dy & =\\int_{-\\infty}^\\infty \\frac{f_{XY}(x,y)}{f_X(x)}\\, dy=\\frac{1}{f_X(x)}\\int_{-\\infty}^\\infty f_{XY}(x,y)\\, dy\\\\\n& = \\frac{1}{f_X(x)}\\cdot f_X(x) =1.\n\\end{array}\n$$\n\n</div>\n\n<l class=\"observ\">Observación. </l>\nSi $X$ e $Y$ son independientes, $Y|X=x =Y$, es decir, la **variable aleatoria condicional $Y|X=x$** coincide con $Y$. Es decir, condicionar con $X=x$ no tiene ningún efecto sobre $Y$.\n\n<div class=\"dem\">\nEfectivamente, veamos que $f_{Y|X=x}(y)=f_Y(y)$ para todo valor $y\\in\\mathbb{R}.$\n\n$$\nf_{Y|X=x}(y) =\\displaystyle \\frac{f_{XY}(x,y)}{f_X(x)} \\stackrel{\\mbox{Al ser independientes}}{=}\\frac{f_Y(y)\\cdot f_X(x)}{f_X(x)}=f_Y(y).\n$$\n</div>\n\n<l class=\"observ\">Observación. </l>\nLa definición de la **función de densidad** de la  **variable aleatoria condicional $X|Y=y$** se definiría de forma similar:\n$$\nf_{X|Y=y}(x)=\\frac{f_{XY}(x,y)}{f_Y(y)},\n$$\npara todo $x\\in\\mathbb{R}$.\n\n#### Ejemplos\n\n<div class=\"example\">\n**Ejemplo: otra función de densidad (continuación)**\n\nRecordemos el ejemplo de la variable aleatoria bidimensional continua con **función de densidad**:\n$$\nf_{XY}(x,y)=\\begin{cases}\n2 \\cdot \\mathrm{e}^{-x}\\cdot \\mathrm{e}^{-y}, & 0\\leq y\\leq x < \\infty,\\\\\n0, & \\mbox{ en caso contrario,}\n\\end{cases}\n$$\nDado un valor $x_0\\geq 0$ cualquiera, vamos a hallar la **función de densidad** de la **variable aleatoria condicional** $Y|X=x_0$.\n\n\n<div class=\"example-sol\">\n\nFijémonos que, fijado un valor $x_0$, los valores $y$ para los cuales $f_{XY}(x_0,y)\\neq 0$ cumplen $0\\leq y\\leq x_0$.\nPor tanto,\n\n$$\nf_{Y|X=x_0}(y)=\\frac{f_{XY}(x_0,y)}{f_X(x_0)}=\\frac{2\\cdot \\mathrm{e}^{-x_0}\\cdot \\mathrm{e}^{-y}}{f_X(x_0)},\n$$\nsi $0\\leq y\\leq x_0$, y $f_{Y|X=x_0}(y)=0$, en caso contrario.\n\nRecordemos que la **densidad marginal** de la variable $X$ era: $f_X(x_0)=2\\left(\\mathrm{e}^{-x_0}-\\mathrm{e}^{-2x_0}\\right)$. \n\nLa **función de densidad marginal** de la variable $Y|X=x_0$ es:\n\n$$\nf_{Y|X=x_0}(y)=\\frac{2\\cdot \\mathrm{e}^{-x_0}\\cdot \\mathrm{e}^{-y}}{2\\cdot \\left(\\mathrm{e}^{-x_0}-\\mathrm{e}^{-2\\cdot x_0}\\right)}=\\frac{e^{-y}}{1-\\mathrm{e}^{-x_0}},\n$$\n\nsi $0\\leq y\\leq x_0$, y $f_{Y|X=x_0}(y)=0$, en caso contrario.\n\nSea ahora $y_0>0$. Calculemos ahora la **densidad marginal** de la variable $X|Y=y_0$.\n\nFijémonos que, fijado un valor $y_0$, los valores $x$ para los cuales $f_{XY}(x,y_0)\\neq 0$ cumplen $y_0\\leq x\\leq \\infty$. Por tanto,\n\n$$\nf_{X|Y=y_0}(x)=\\frac{f_{XY}(x,y_0)}{f_Y(y_0)}=\\frac{2\\cdot \\mathrm{e}^{-x}\\cdot \\mathrm{e}^{-y_0}}{f_Y(y_0)},\n$$\nsi $y_0\\leq x\\leq \\infty$, y $f_{X|Y=y_0}(x)=0$, en caso contrario.\n\nRecordemos que la variable $Y$ era exponencial de parámetro $\\lambda=2$. Por tanto, $f_Y(y_0)=2\\mathrm{e}^{-2y_0}$.\n\nLa **función de densidad marginal** de la variable $X|Y=y_0$ es:\n\n$$\nf_{X|Y=y_0}(x)=\\frac{2\\cdot \\mathrm{e}^{-x}\\cdot \\mathrm{e}^{-y_0}}{2\\cdot \\mathrm{e}^{-2\\cdot y_0}}=\\frac{\\mathrm{e}^{-x}}{\\mathrm{e}^{-y_0}},\n$$\nsi $y_0\\leq x\\leq \\infty$, y $f_{X|Y=y_0}(x)=0$, en caso contrario.\n</div>\n</div>\n\n<div class=\"example\">\n**Ejemplo: normal bidimensional**\n\nSea $(X,Y)$ una variable aleatoria bidimensional normal bidimensional con **densidad conjunta**:\n\n$$\nf_{XY}(x,y)=\\frac{1}{2\\pi\\sqrt{1-\\rho^2}}\\mathrm{e}^{-\\frac{(x^2-2\\rho xy+y^2)}{2(1-\\rho^2)}},\\ -\\infty <x,y<\\infty.\n$$\nSea $x\\in\\mathbb{R}$. Hallemos la **función de densidad** de la **variable aleatoria condicionada** $Y|X=x$.\n\n<div class=\"example-sol\">\n\nRecordemos que las **marginales** eran $N(0,1)$. Por tanto, $f_X(x)=\\frac{1}{\\sqrt{2\\pi}}\\mathrm{e}^{-\\frac{x^2}{2}}.$\n\nLa **función de densidad** de la variable condicional $Y|X=x$ es:\n\n$$\n\\begin{array}{rl}\nf_{Y|X=x}(y) & =  \\frac{f_{XY}(x,y)}{f_X(x)}=\\frac{\\frac{1}{2\\cdot \\pi\\cdot \\sqrt{1-\\rho^2}}\\mathrm{e}^{-\\frac{(x^2-2\\cdot \\rho \\cdot x\\cdot y+y^2)}{2(1-\\rho^2)}}}{\\frac{1}{\\sqrt{2\\pi}}\\mathrm{e}^{-\\frac{x^2}{2}}}\\\\\n& =\\frac{1}{\\sqrt{2\\cdot \\pi\\cdot  (1-\\rho^2)}}\\mathrm{e}^{-\\frac{(y-\\rho\\cdot  x)^2}{2\\cdot (1-\\rho^2)}},\\ y\\in\\mathbb{R}.\n\\end{array}\n$$\nConcluimos que la **variable aleatoria condicional $Y|X=x$** es una normal de parámetros $\\mu_{Y|X=x}=\\rho x$ y $\\sigma_{Y|X=x}^2 =1-\\rho^2$.\n\n<l class=\"obrv\"> Observaciones</l>\n\nTenemos dos observaciones con respecto al resultado obtenido:\n\n* La varianza de la **variable aleatoria condicional** no depende de la $x$ que se ha fijado. Sólo depende del parámetro $\\rho$. La $x$ sólo influye en la media de dicha variable.\n\n* En el caso en que $\\rho=0$, que significa que $X$ e $Y$ son independientes, la distribución condicional de $Y|X=x$ es una $N(0,1)$, distribución que coincide con la distribución de la **variable aleatoria marginal** $Y$.\n\n</div>\n</div>\n\n### Valores esperados condicionales\n\n<l class=\"definition\">Definición de valor esperado condicional.</l>\nDada una variable aleatoria bidimensional $(X,Y)$, definimos el **valor esperado de la variable $Y$ dado que $X=x$** como $E(Y|x)$, es decir, el valor esperado de la **variable aleatoria condicional $Y|X=x$**:\n$$\nE(Y|x)=\\begin{cases}\n\\displaystyle\\sum_{y_j} y_j \\cdot P_{Y|X=x}(y_j), & \\mbox{ caso discreto,}\\\\\n\\displaystyle\\int_{-\\infty}^\\infty y \\cdot f_{Y|X=x}(y)\\,dy, & \\mbox{ caso continuo.}\n\\end{cases}\n$$\n\nTenemos el siguiente resultado relacionado con los valores esperados: el valor esperado respecto $x$ del valor esperado de la **variable condicional $Y|X=x$** coincide con el valor esperado de la variable $Y$:\n\n<l class=\"prop\">Proposición. </l>\nSea $(X,Y)$ una variable aleatoria bidimensional. Sean $E(Y|x)$ el **valor esperado condicional de $Y$** respecto $x$. Entonces el valor esperado de la *variable aleatoria* $E(Y|X)$ como función de la variable $X$ es el valor esperado de la variable $Y$:\n$$\nE_X(E(Y|X))=E(Y).\n$$\n\n<div class=\"dem\">\n**Demostración**\n\nHaremos la demostración en el caso continuo. Dejamos como ejercicio la demostración para el caso discreto.\n\nSea $f_{XY}$ la **función de densidad conjunta** y $f_X$ y $f_Y$ las **funciones de densidad marginales**.\n\nEl valor de $E_X(E(Y|X))$ es:\n$$\n\\begin{array}{rl}\nE_X(E(Y|X)) & =\\int_{x=-\\infty}^{x=\\infty} E(Y|x)f_X(x)\\, dx=\\int_{x=-\\infty}^{x=\\infty}\\int_{y=-\\infty}^{y=\\infty} y f_{Y|X=x}(y)\\, dy f_X(x)\\, dx \\\\ & = \\int_{x=-\\infty}^{x=\\infty}\\int_{y=-\\infty}^{y=\\infty} y \\frac{f_{XY}(x,y)}{f_X(x)}f_X(x)\\, dy\\, dx = \\int_{y=-\\infty}^{y=\\infty} y \\int_{x=-\\infty}^{x=\\infty}f_{XY}(x,y)\\, dx\\, dy \\\\ &  = \\int_{y=-\\infty}^{y=\\infty} y f_Y(y)\\, dy = E(Y),\n\\end{array}\n$$\ntal como queríamos ver.\n</div>\n\n### Relación con el problema de la regresión general\n\nEl problema de la **regresión general** es el siguiente:\n\nSea $(X,Y)$ una variable aleatoria bidimensional. Queremos hallar una función $g$ tal que la variable $\\hat{Y}=g(X)$ explique mejor la variable $Y$. \n\nDicho de forma más explícita, queremos hallar una función $g$ tal que minimice el error cometido al aproximar $Y$ por $\\hat{Y}=g(X)$. Dicho error se definede forma natural como el valor esperado de la variable $(Y-g(X))^2$:\n\n$$\n\\min_g E\\left((Y-g(X))^2\\right).\n$$\n\nEl siguiente resultado nos dice cuál es la función $g$:\n\n<l class=\"prop\">Proposición: </l>\nLa función $g$ solución del problema de **regresión general** es la siguiente: $g(x)=E(Y|X=x)$. \n\nEs decir, la función $g$ asigna a cada valor $x$ de la variable aleatoria $X$, el valor esperado de la **variable condicional** $Y|X=x$.\n\nEn resumen, la función $g(x)=E(Y|X=x)$ es la función que minimiza el error. A la curva $y=g(x)$ se la denomina **curva general de regresión de $Y$ sobre $X$**.\n\n### Valores esperados condicionales. Caso general\n\nPodemos generalizar los valores esperados condicionales en el sentido que en lugar de hallar $E(Y|X=x)$, hallar $E(g(Y)|X=x)$, donde $g$ es una función de la variable aleatoria $Y$:\n\n<l class=\"definition\">Definición de valor esperado condicional.</l>\n\nDada una variable aleatoria bidimensional $(X,Y)$ y una función $g$, definimos el **valor esperado de la variable $g(Y)$ dado que $X=x$** como $E(g(Y)|x)$, es decir, el valor esperado de la **variable aleatoria condicional $g(Y)|X=x$**:\n\n$$\nE(g(Y)|x)=\\begin{cases}\n\\sum_{y_j} g(y_j) P_{Y|X=x}(y_j), & \\mbox{ caso discreto,}\\\\\n\\int_{-\\infty}^\\infty g(y) f_{Y|X=x}(y)\\,dy, & \\mbox{ caso continuo.}\n\\end{cases}\n$$\n\n<l class=\"observ\">Observación:</l> cuando $g(y)=y^k$, tenemos definidos los **momentos condicionados de orden $k$** de la variable $Y|X=x$.\n\n#### Ejemplos\n\n<div class=\"example\">\n\n**Ejemplo de la suma y el producto de los resultados de dos lanzamientos de un dado**\n\nVamos a hallar el valor esperado de la **variable aleatoria condicional $P|S=8$**.\n\n<div class=\"example-sol\">\n\nRecordemos su **función de probabilidad**:\n\n<div class=\"center\">\n| $P|S=8$| $12$ | $15$ | $16$ \n|----|----|----|----|\n|$P_{P|S=8}$| $\\frac{\\frac{2}{36}}{\\frac{5}{36}}=\\frac{2}{5}$ | $\\frac{\\frac{2}{36}}{\\frac{5}{36}}=\\frac{2}{5}$ | $\\frac{\\frac{1}{36}}{\\frac{5}{36}}=\\frac{1}{5}$\n</div>\n\n\nSu valor esperado es:\n$$\nE(P|S=8)=12\\cdot \\frac{2}{5}+15\\cdot \\frac{2}{5}+16\\cdot \\frac{1}{5}=\\frac{`r 12*2+15*2+16`}{5}=`r (12*2+15*2+16)/5`.\n$$\nEl valor medio del producto de los resultados al lanzar un dado dos veces cuando la suma de dichos resultados es 8 vale `r (12*2+15*2+16)/5`.\n\nEl valor esperado de la variable $E(P|S=8)$ es:\n\n```{r}\nvalores.cond.s8=as.integer(names(prob.cond.s8.buena))\nsum(valores.cond.s8*prob.cond.s8.buena)\n```\n</div>\n</div>\n\n<div class=\"example\">\n**Ejemplo: otra densidad (continuación)**\n\nRecordemos el ejemplo de la variable aleatoria bidimensional continua con **función de densidad**:\n\n$$\nf_{XY}(x,y)=\\begin{cases}\n2 \\mathrm{e}^{-x}\\mathrm{e}^{-y}, & 0\\leq y\\leq x < \\infty,\\\\\n0, & \\mbox{ en caso contrario,}\n\\end{cases}\n$$\n\nYa sebemos que si fijamos $x_0>0$, la **función de densidad** de la **variable aleatoria condicionada** $Y|X=x_0$ es:\n\n$$\nf_{Y|X=x_0}(y)=\\begin{cases}\n\\frac{e^{-y}}{1-\\mathrm{e}^{-x_0}}, & \\mbox{ si }0\\leq y\\leq x_0, \\\\\n0, & \\mbox{en caso contrario.}\n\\end{cases}\n$$\n\nHallemos su valor esperado:\n\n<div class=\"example-sol\">\n\n\n$$\nE(Y|X=x_0)=\\int_0^{x_0} y \\frac{e^{-y}}{(1-\\mathrm{e}^{-x_0})}\\, dy=\\frac{1}{(1-\\mathrm{e}^{-x_0})}\\left[-\\mathrm{e}^{-y} (y+1)\\right]_0^{x_0} = \\frac{1-\\mathrm{e}^{-x_0}(1+x_0)}{1-\\mathrm{e}^{-x_0}}.\n$$\n\nVerifiquemos la propiedad vista anteriormente $E_X(E(Y|x))=E(Y)$. Recordemos que la **función de densidad marginal** de la variable $X$ era: $f_X(x)=2\\left(\\mathrm{e}^{-x}-\\mathrm{e}^{-2x}\\right)$, para $x>0$:\n\n$$\n\\begin{array}{rl}\nE_X(E(Y|x)) & =\\int_0^\\infty E(Y|x)\\cdot f_X(x)\\, dx = \\int_0^\\infty \\frac{1-\\mathrm{e}^{-x}(1+x)}{1-\\mathrm{e}^{-x}}\\cdot 2\\left(\\mathrm{e}^{-x}-\\mathrm{e}^{-2x}\\right)\\, dx \n\\\\ & =  2\\int_0^\\infty \\frac{1-\\mathrm{e}^{-x}(1+x)}{1-\\mathrm{e}^{-x}} \\mathrm{e}^{-x}\\left(1-\\mathrm{e}^{-x}\\right)\\, dx = 2 \\int_0^\\infty \\left(\\mathrm{e}^{-x}-\\mathrm{e}^{-2x}(1+x)\\right)\\, dx \\\\ & = 2\\left[-\\mathrm{e}^{-x}+\\mathrm{e}^{-2 x}\n   \\left(\\frac{x}{2}+\\frac{3}{4}\\right)\\right]_0^\\infty = 2 \\left(1-\\frac{3}{4}\\right)=\\frac{1}{2}.\n\\end{array}\n$$\n\nRecordemos que la variable $Y$ era exponencial de parámetro $\\lambda=2$. Por tanto $E(Y)=\\frac{1}{\\lambda}=\\frac{1}{2}$, valor que coincide con el hallado, tal como queríamos ver.\n</div>\n</div>\n\n## Variables aleatorias definidas como función de dos variables aleatorias conjuntas\n\n\nDado un experimento aleatorio, a veces estaremos interesados en una o más funciones de las variables asociadas con el experimento.\n\nPor ejemplo, si consideramos el experimento aleatorio de lanzar un dado dos veces y definimos la **variable aleatoria bidimensional** $(X_1,X_2)$ como la variable que nos da el resultado de cada lanzamiento, podemos expresar la suma y el producto  como $S=X_1+X_2$, $P=X_1\\cdot X_2$.\n\nOtros ejemplos podrían ser considerar el experimento aleatoria de realizar mediciones repetidas de la misma cantidad aleatoria. Entonces, podríamos estar interesados en el valor máximo y mínimo en el conjunto, así como la media muestral y la varianza muestral. \n\nEn esta sección presentamos métodos para determinar las probabilidades de eventos que involucran **funciones de dos variables aleatorias**.\n\nDaremos métodos de cómo hallar la **función de distribución** y la **función de probabilidad** (caso discreto) o la **función de densidad** (caso continuo) de la variable aleatoria definida como función de la **variable aleatoria bidimensional**.\n\n### Variable aleatoria función de la variable aleatoria bidimensional\n\n<l class=\"prop\">Proposición.</l>\n\nSea $(X,Y)$ una variable aleatoria bidimensional con **función de probabilidad** $P_{XY}$ (caso discreto) o **función de densidad** (caso continuo). Sea $g$ una función y definimos la **variable aleatoria unidimensional** $Z$ como $Z=g(X,Y)$. Entonces la función de distribución de $Z$ es:\n$$\n\\begin{array}{rl}\nF_Z(z) & = \\displaystyle P(Z\\leq z)=\\sum\\sum_{(x_i,y_j),\\ |\\ g(x_i,y_j)\\leq z} P_{XY}(x_i,y_j),\\ z\\in\\mathbb{R},\\\\ &\\ \\qquad\\mbox{ (caso discreto),}\\\\\nF_Z(z) & = \\displaystyle  P(Z\\leq z)=\\int\\int_{(x,y)\\in\\mathbb{R}^2,\\ |\\ g(x,y)\\leq z} f_{XY}(x,y)\\,dy\\, dx, \\ z\\in\\mathbb{R},\\\\ &\\ \\qquad\\mbox{ (caso continuo).}\n\\end{array}\n$$\n\n</div>\n\n<l class=\"observ\">Observación.</l>\n\nEn el caso discreto, la variable aleatoria será discreta con valores $Z(\\Omega)=\\{z_{ij}=g(x_i,y_j),\\ |\\ (x_i,y_j)\\in (X,Y)(\\Omega)\\}$. \nHay que tener en cuenta que en dicho conjunto puede haber repeticiones, es decir, pueden existir dos parejas $(i,j)$ y $(i',j')$ tal que $z_{ij}=z_{i'j'}$.\n\nLa expresión de la **función de probabilidad** en el caso discreto se complica mucho debido a dichas repeticiones y es mejor hallarla en cada caso concreto.\n\nLa última observación se puede aplicar también en el caso continuo: la expresión de la **función de densidad** se halla en cada caso concreto.\n\n\n<div class=\"example\">\n**Ejemplo del lanzamiento de un dado dos veces.**\n\nConsideremos el experimento aleatorio de lanzar dos veces un dado. \n\nSea $(X,Y)$ la **variable aleatoria** bidimensional discreta ya estudiada anteriormente donde $X$ nos da el resultado del primer lanzamiento e $Y$, el resultado del segundo lanzamiento.\n\nVimos que $(X,Y)(\\Omega)=\\{(i,j),\\ i=1,2,3,4,5,6,\\ j=1,2,3,4,5,6\\}$ con **función de probabilidad conjunta** $P_{XY}(i,j)=\\frac{1}{36}$, $i=1,2,3,4,5,6,\\ j=1,2,3,4,5,6.$\n\nAnteriormente hemos estudiado la suma $S$ de los resultados. En este caso podemos interpretar $S=g(X,Y)$ donde $g(x,y)=x+y$.\n\nComo la función $S$ ya ha sido estudiada y el producto se ha dejado como ejercicio, estudiaremos la siguiente variable aleatoria función de $X$ e $Y$: $Z=X^2+Y^2$. \n\nRealizaremos los cálculos con ayuda de `R` ya que hacerlos a mano es bastante tedioso.\n\nLos valores de $Z(\\Omega)$ son: $Z(\\Omega)=\\{z_{ij}=i^2+j^2,\\ i=1,2,3,4,5,6,\\ j=1,2,3,4,5,6\\}$. Observad que hay parejas $(i,j)$ que dan lugar a los mismos valores, por ejemplo $1^2+2^2 = 2^2+1^2$, y, en general, si $i\\neq j$, $z_{ij}=i^2+j^2=z_{ji}=j^2+i^2$.\n\n\n<div class=\"example-sol\">\n\nPara hallar el conjunto $Z(\\Omega)$ usamos la función `outer` de `R`:\n\n```{r}\ng=function(x,y){x^2+y^2}  ## definimos la función g\nsort(unique(as.vector(outer(1:6,1:6,g))))\n```\n\nVemos que hay `r length(sort(unique(as.vector(outer(1:6,1:6,g)))))` valores distintos de la variable $Z$.\n\nPara hallar la **función de probabilidad** de $Z$ hemos de calcular para cada valor $z_k$, las parejas $(i,j)$ tal que $i^2+j^2=z_k$:\n\n```{r}\nvalores.variable.Z = sort(unique(as.vector(outer(1:6,1:6,g))))  \nmatriz.valores = outer(1:6,1:6,g) ## aplicamos la función g a \n##  todas las parejas (i,j), i,j=1,2,3,4,5,6\nfrecuencias = c()  ## vector donde guardaremos las frecuencias de los valores de Z\nfor (i in 1:length(valores.variable.Z)){\n  z=valores.variable.Z[i]\n  frecuencias=c(frecuencias,length(matriz.valores[matriz.valores==z]))\n}\nfrecuencias\n```\n\nLa **función de probabilidad** de $Z$ es:\n```{r}\nfunción.probabilidad.Z=data.frame(rbind(valores.variable.Z,round(frecuencias/36,3)))\nrownames(función.probabilidad.Z)=c(\"Z\",\"P_Z\")\nfunción.probabilidad.Z\n```\n\n</div>\n</div>\n\n<div class=\"example\">\n**Ejemplo variables aleatorias continuas**\n\nRecordemos la variable aleatoria bidimensional $(X,Y)$ con **función de densidad**:\n$$\nf_{XY}(x,y)=\\begin{cases}\n2\\cdot  \\mathrm{e}^{-x}\\cdot \\mathrm{e}^{-y}, & 0\\leq y\\leq x < \\infty,\\\\\n0, & \\mbox{ en caso contrario,}\n\\end{cases}\n$$\nConsideremos la variable aleatoria $Z=X+Y$. Vamos a calcular la **función de densidad** de $Z$.\n\nEn primer lugar, los valores de $Z$ para los que $f_Z(z)\\neq 0$ cumplen $z\\geq 0$ ya que $X\\geq 0$ e $Y\\geq 0$.\n\nCalculemos la **función de distribución** de la variable $Z$. Sean $z\\in\\mathbb{R}$ con $z\\geq 0$:\n\n<div class=\"example-sol\">\n\n\n\n$$\n\\begin{array}{rl}\nF_Z(z) & =  P(Z\\leq z)=P(X+Y\\leq z)\\\\\n& =\\displaystyle \\int\\int_{\\{(x,y)\\mathbb{R}^2,\\ |\\ x+y\\leq z\\}\\cap \\{(x,y)\\in \\mathbb{R}^2,\\ |\\ 0\\leq y\\leq x<\\infty\\}} 2\\cdot  \\mathrm{e}^{-x}\\cdot \\mathrm{e}^{-y}\\, dy\\, dx\n\\end{array}\n$$\n\nEl gráfico siguiente muestra en color violeta la región de integración para hallar $F_Z(z)$ dado un $z\\geq 0$.\n<div class=\"center\">\n\n```{r, echo=FALSE, label=bid19,fig.cap=\"\"}\nknitr::include_graphics(\"Images/EjSumaXY.png\",dpi=1200)\n```\n</div>\n\nEl valor de $F_Z(z)$ es: (fijémonos que primero fijamos la $y$ y para cada $y$ la $x$ va desde la recta $x=y$ hasta la recta $x=z-y$)\n\n$$\n\\begin{array}{rl}\nF_Z(z) & =\\displaystyle\\int_{y=0}^{y=\\frac{z}{2}}\\int_{x=y}^{x=z-y}2 \\cdot\\mathrm{e}^{-x}\\cdot\\mathrm{e}^{-y}\\, dx\\, dy = 2 \\cdot\\int_{y=0}^{y=\\frac{z}{2}} \\mathrm{e}^{-y} \\cdot\\left[-\\mathrm{e}^{-x}\\right]_{x=y}^{x=z-y}\\, dy \\\\ & \n\\displaystyle = 2 \\cdot\\cdot\\int_{y=0}^{y=\\frac{z}{2}} \\mathrm{e}^{-y}\\cdot \\left(\\mathrm{e}^{-y}-\\mathrm{e}^{y-z}\\right)\\, dy = 2 \\cdot\\int_{y=0}^{y=\\frac{z}{2}} \\left(\\mathrm{e}^{-2y}-\\mathrm{e}^{-z} \\right)\\, dy  \\\\\n& = 2\\cdot\\left[-\\frac{1}{2}\\mathrm{e}^{-2y}-\\mathrm{e}^{-z} \\cdot y\\right]_{y=0}^{y=\\frac{z}{2}}  = \\displaystyle 2\\cdot\\left(\\frac{1}{2}-\\frac{1}{2}\\mathrm{e}^{-z}-\\frac{z}{2}\\mathrm{e}^{-z}\\right) \\\\ \n& = 1-\\mathrm{e}^{-z}\\cdot(1+z),\\ z\\geq 0.\n\\end{array}\n$$\nLa **función de densidad** de $Z$ es:\n\n\n$$\nf_Z(z)=F'_Z(z)=z\\cdot\\mathrm{e}^{-z},\\ z\\geq 0,\n$$\ny $f_Z(z)=0$ en caso contrario.\n</div>\n</div>\n\n<div class=\"example\">\n**Ejemplo de la suma de dos normales**\n\nConsideremos el caso en que la variable aleatoria $(X,Y)$ tenga distribución **normal bidimensional**. \n\nRecordemos que su **función de densidad conjunta** era:\n$$\nf_{XY}(x,y)=\\frac{1}{2\\cdot \\pi\\cdot \\sqrt{1-\\rho^2}}\\mathrm{e}^{-\\frac{(x^2-2\\cdot \\rho\\cdot  x\\cdot y+y^2)}{2\\cdot (1-\\rho^2)}},\\ -\\infty <x,y<\\infty.\n$$\nConsideremos $S=X+Y$. Estudiemos qué distribución tiene $S$.\n\nDado un valor $z\\in\\mathbb{R}$, la **función de distribución** de $S$ en $s$ es:\n\n<div class=\"example-sol\">\n\n$$\n\\begin{array}{rl}\nF_S(s) & =P(S\\leq s)=\\displaystyle \\int\\int_{\\{(x,y)\\in\\mathbb{R}^2,\\ |\\ x+y\\leq s\\}}\\frac{1}{2\\pi\\sqrt{1-\\rho^2}}\\mathrm{e}^{-\\frac{(x^2-2\\rho xy+y^2)}{2(1-\\rho^2)}}\\, dy\\, dx \\\\\n& =\\displaystyle  \\frac{1}{2\\pi\\sqrt{1-\\rho^2}} \\int_{x=-\\infty}^{x=\\infty}\\int_{y=-\\infty}^{y=s-x}\\mathrm{e}^{-\\frac{(x^2-2\\rho xy+y^2)}{2(1-\\rho^2)}}\\, dy\\, dx \\\\\n& = \\displaystyle  \\frac{1}{2\\pi\\sqrt{1-\\rho^2}} \\int_{x=-\\infty}^{x=\\infty} \\mathrm{e}^{-\\frac{x^2}{2(1-\\rho^2)}} \\int_{y=-\\infty}^{y=s-x}\\mathrm{e}^{-\\frac{(-2\\rho xy+y^2)}{2(1-\\rho^2)}}\\, dy\\, dx  \\\\ &\\ \\qquad\\mbox{hacemos el siguiente cambio  en la segunda integral $t=y+x$}\\\\\n& = \\displaystyle  \\frac{1}{2\\pi\\sqrt{1-\\rho^2}} \\int_{x=-\\infty}^{x=\\infty} \\mathrm{e}^{-\\frac{x^2}{2(1-\\rho^2)}} \\int_{t=-\\infty}^{t=s}\\mathrm{e}^{-\\frac{(-2\\rho x(t-x)+(t-x)^2)}{2(1-\\rho^2)}}\\, dt\\, dx \n\\end{array}\n$$\n\n\n$$\n\\begin{array}{rl}\nF_S(s) \n& =  \\displaystyle \\frac{1}{2\\pi\\sqrt{1-\\rho^2}} \\int_{x=-\\infty}^{x=\\infty} \\mathrm{e}^{-\\frac{(1+\\rho)x^2}{1-\\rho^2}}\\int_{t=-\\infty}^{t=s} \\mathrm{e}^{-\\frac{(t^2-2(1+\\rho) t x)}{2(1-\\rho^2)}}\\, dt\\, dx \\\\\n& = \\frac{1}{2\\pi\\sqrt{1-\\rho^2}} \\int_{x=-\\infty}^{x=\\infty} \\mathrm{e}^{-\\frac{(1+\\rho)x^2}{1-\\rho^2}}\\int_{t=-\\infty}^{t=s} \\mathrm{e}^{-\\frac{(t-(1+\\rho)x)^2}{2(1-\\rho^2)}} \\mathrm{e}^{\\frac{(\\rho+1)^2 x^2}{2(1-\\rho^2)}}\\, dt\\, dx  \\\\\n& = \\frac{1}{2\\pi\\sqrt{1-\\rho^2}} \\int_{x=-\\infty}^{x=\\infty} \\mathrm{e}^{-\\frac{x^2}{2}}\\cdot \\sqrt{2\\pi (1-\\rho^2)} F_X(s)\\, dx, \\\\ \n& \\mbox{ donde $F_X(s)$ es la función de distribución}\\\\\n& \\mbox{de una variable $X$ normal de parámetros} \\\\ \n& \\mbox{ $\\mu =(1+\\rho)x$ y $\\sigma^2=1-\\rho^2$.} \\\\ \n& = \\frac{1}{\\sqrt{2\\pi}}\\int_{x=-\\infty}^{x=\\infty} \\mathrm{e}^{-\\frac{x^2}{2}}\\cdot F_X(s)\\, dx.\n\\end{array}\n$$ \n\nPara calcular la **función de densidad** $f_S(s)$ aplicamos la expresión $f_S(s)=F'_S(s)$ y la derivación bajo el signo integral:\n\n$$\n\\begin{array}{rl}\nf_S(s) & = \\frac{1}{\\sqrt{2\\pi}}\\int_{x=-\\infty}^{x=\\infty} \\mathrm{e}^{-\\frac{x^2}{2}}\\cdot f_X(s)\\, dx = \\frac{1}{\\sqrt{2\\pi}}\\int_{x=-\\infty}^{x=\\infty} \\mathrm{e}^{-\\frac{x^2}{2}}\\cdot \\frac{1}{\\sqrt{2\\pi (1-\\rho^2)}}\\mathrm{e}^{-\\frac{(s-(1+\\rho)x)^2}{2(1-\\rho^2)}}\\, dx \\\\ & = \\frac{\\mathrm{e}^{-\\frac{s^2}{2(1-\\rho^2)}}}{2\\pi\\sqrt{1-\\rho^2}} \\int_{x=-\\infty}^{x=\\infty} \\mathrm{e}^{-\\frac{(2(1+\\rho) x^2-2(1+\\rho)xs)}{2(1-\\rho^2)}}\\, dx= \\frac{\\mathrm{e}^{-\\frac{s^2}{2(1-\\rho^2)}}}{2\\pi\\sqrt{1-\\rho^2}} \\int_{x=-\\infty}^{x=\\infty} \\mathrm{e}^{-\\frac{\\left(x-\\frac{s}{2}\\right)^2}{1-\\rho}}\\mathrm{e}^{\\frac{s^2}{4(1-\\rho)}}\\, dx\n\\end{array}\n$$\n\nEn la última integral hacemos el cambio $u=x-\\frac{z}{2}$:\n$$\nf_S(s)  =\\frac{\\mathrm{e}^{-\\frac{s^2}{4(1+\\rho)}}}{2\\pi\\sqrt{1-\\rho^2}} \\int_{u=-\\infty}^{u=\\infty} \\mathrm{e}^{-\\frac{u^2}{1-\\rho}}\\, du.\n$$\nA continuación usando que $f_Z(z)=\\frac{1}{\\sqrt{2\\pi}}\\mathrm{e}^{-\\frac{x^2}{2}}$ es la función de densidad de la distribución $Z=N(0,1)$, podemos escribir: $\\int_{-\\infty}^\\infty \\frac{1}{\\sqrt{2\\pi}}\\mathrm{e}^{-\\frac{x^2}{2}}=1,\\ \\Rightarrow \\int_{-\\infty}^\\infty \\mathrm{e}^{-\\frac{x^2}{2}}=\\sqrt{2\\pi}.$\n\nSi en la última integral hacemos el cambio $v=\\sqrt{\\frac{2}{1-\\rho}}u$, obtenemos:\n\n$$\n\\begin{array}{rl}\nf_S(s)  & = \\frac{\\mathrm{e}^{-\\frac{s^2}{4(1+\\rho)}}}{2\\pi\\sqrt{1-\\rho^2}}\\int_{v=-\\infty}^{v=\\infty}\\mathrm{e}^{-\\frac{v^2}{2}} \\sqrt{\\frac{1-\\rho}{2}}\\, dv= \\frac{\\mathrm{e}^{-\\frac{s^2}{4(1+\\rho)}}}{2\\pi\\sqrt{2(1+\\rho)}}\\int_{v=-\\infty}^{v=\\infty}\\mathrm{e}^{-\\frac{v^2}{2}} \\, dv \\\\ & = \\frac{\\mathrm{e}^{-\\frac{s^2}{4(1+\\rho)}}}{2\\pi\\sqrt{2(1+\\rho)}} \\sqrt{2\\pi}= \\frac{1}{\\sqrt{2\\pi 2(1+\\rho)}}\\mathrm{e}^{-\\frac{s^2}{4(1+\\rho)}},\\ s\\in\\mathbb{R}.\n\\end{array}\n$$\n\n\nDicha función de densidad corresponde a una distribución normal de parámetros $\\mu =0$ y $\\sigma = \\sqrt{2(1+\\rho)}$.\n\nEn resumen, la distribución de la suma de dos normales es una normal de parámetros $S=N(\\mu=0,\\sigma = \\sqrt{2(1+\\rho)})$.\n</div>\n</div>\n\n### Transformaciones lineales de variables aleatorias\n\nConsideremos una variable aleatoria bidimensional continua $(X,Y)$ con **función de densidad conjunta** $f_{XY}$.\n\nDefinimos la variable aleatoria bidimensional continua $(U,V)$ a partir de una transformación lineal de la variable $(X,Y)$. Es decir, existe una matriz $\\mathbf{M}=\\begin{pmatrix}a & b\\\\ c& d\\end{pmatrix}$ y un vector $\\mathbf{n}=\\begin{pmatrix}\\alpha\\\\\\beta \\end{pmatrix}$ tal que:\n\n\n$$\n\\begin{array}{rl}\n\\begin{pmatrix}U\\\\ V\\end{pmatrix} & =\\mathbf{M}\\cdot \\begin{pmatrix}X\\\\ Y\\end{pmatrix}+\\mathbf{n}=\\begin{pmatrix}a & b\\\\ c& d\\end{pmatrix}\\cdot\\begin{pmatrix}X\\\\ Y\\end{pmatrix}+\\begin{pmatrix}\\alpha\\\\\\beta \\end{pmatrix},\\\\  & \\Rightarrow \\left.\\begin{array}{rl}U & = aX+bY+\\alpha,\\\\ V & =cX+dY+\\beta.\\end{array}\\right\\}\n\\end{array}\n$$\n\nPara que $(U,V)$ sea una variable aleatoria bidimensional, necesitamos que la matriz $\\mathbf{M}$ sea no singular, o $\\mathrm{det}(\\mathbf{M})\\neq 0$.\n\nNos preguntamos cuál es la relación entre la **función de densidad** de la variable $(U,V)$, $f_{UV}$ y la **función de densidad** de la variable $(X,Y)$, $f_{XY}$. La expresión siguiente nos da dicha relación:\n\n$$\nf_{UV}(u,v)=\\frac{1}{|\\mathrm{det}(\\mathbf{M})|}f_{XY}\\left(\\mathbf{M}^{-1}\\begin{pmatrix}u-\\alpha\\\\ v-\\beta\\end{pmatrix}\\right), \\ (u,v)\\in\\mathbb{R}^2.\n$$\n\n<l class=\"observ\">Observación. </l> \nSi la variable $(X,Y)$ tiene una región $D$ donde $f_{XY}(x,y)\\neq 0$, para todo $(x,y)\\in D$, antes de aplicar la expresión anterior para hallar la **función de densidad** de la variable $(U,V)$ hemos de calcular cómo se transforma $D$ con la matriz $\\mathbf{M}$. Es decir, hay que hallar la región\n\n$$\nD'=\\mathbf{M}(D)=\\{(u,v)\\in\\mathbb{R}^2,\\ \\mbox{existe $(x,y)\\in D$ con } (u,v)=\\mathbf{M}(x,y)+\\mathbf{n}\\}.\n$$\n\n<div class=\"example\">\n**Ejemplo: transformación lineal**\n\nConsideremos la variable $(X,Y)$ continua con función de densidad:\n$$\nf_{XY}(x,y)=\\begin{cases}\n\\frac{1}{2}(1+x+y), & \\mbox{ si }(x,y)\\in R, \\\\\n0, & \\mbox{en caso contrario.}\n\\end{cases}\n$$\ndonde $R$ es el rombo de vértices $(1,0)$, $(0,1)$, $(-1,0)$ y $(0,-1)$, ver figura adjunta.\n\nOtra forma de definir la función anterior es:\n\n$$\nf_{XY}(x,y)=\\begin{cases}\n\\frac{1}{2}(1+x+y), & -1\\leq x\\leq 0,\\ -1-x\\leq y\\leq x+1, \\\\\n\\frac{1}{2}(1+x+y), & 0\\leq x\\leq 0,\\ x-1\\leq y\\leq 1-x, \\\\\n0, & \\mbox{en caso contrario.}\n\\end{cases}\n$$\nDejamos como ejercicio al lector comprobar que la función anterior es una **función de densidad**.\n\n<div class=\"center\">\n\n```{r, echo=FALSE, label=Ej2.png,fig.cap=\"\"}\nknitr::include_graphics(\"Images/EjTranLineal.png\",dpi=1200)\n```\n</div>\n\n<div class=\"example-sol\">\n\nConsideramos la variable aleatoria bidimensional $(U,V)$ definida a partir de la variable $(X,Y)$:\n\n$$\n\\begin{pmatrix}U\\\\ V\\end{pmatrix}=\\begin{pmatrix}1 & -1\\\\ 1& 1\\end{pmatrix}\\cdot\\begin{pmatrix}X\\\\ Y\\end{pmatrix},\\ \\Rightarrow \\left.\\begin{array}{rl}U & = X-Y,\\\\ V & =X+Y.\\end{array}\\right\\}\n$$\nLa región $R$ se transforma en el cuadrado $C$ de vértices $(1,1)$, $(-1,1)$, $(-1,-1)$ y $(1,-1)$ ya que si aplicamos la matriz a los vértices del rombo, obtenemos los vértices de cuadrado:\n$$\n\\begin{array}{rl}\n\\begin{pmatrix}1 & -1\\\\ 1& 1\\end{pmatrix}\\cdot \\begin{pmatrix}1\\\\ 0\\end{pmatrix} & =\\begin{pmatrix}1\\\\ 1\\end{pmatrix},\\qquad \n\\begin{pmatrix}1 & -1\\\\ 1& 1\\end{pmatrix}\\cdot \\begin{pmatrix}0\\\\ 1\\end{pmatrix}=\\begin{pmatrix}-1\\\\ 1\\end{pmatrix},\\\\ \n\\begin{pmatrix}1 & -1\\\\ 1& 1\\end{pmatrix}\\cdot \\begin{pmatrix}-1\\\\ 0\\end{pmatrix} & =\\begin{pmatrix}-1\\\\ -1\\end{pmatrix},\\qquad \n\\begin{pmatrix}1 & -1\\\\ 1& 1\\end{pmatrix}\\cdot \\begin{pmatrix}0\\\\ -1\\end{pmatrix}=\\begin{pmatrix}1\\\\ -1\\end{pmatrix}.\n\\end{array}\n$$\nVer la figura adjunta.\n\nPara hallar la **función de densidad** $f_{UV}$ necesitamos escribir $X$ e $Y$ en función de $U$ y $V$:\n$$\n\\begin{pmatrix}X\\\\ Y\\end{pmatrix}=\\begin{pmatrix}1 & -1\\\\ 1& 1\\end{pmatrix}^{-1}\\cdot\\begin{pmatrix}U\\\\ V\\end{pmatrix}=\\begin{pmatrix}\\frac{1}{2} & \\frac{1}{2}\\\\ -\\frac{1}{2}& \\frac{1}{2}\\end{pmatrix}\\cdot\\begin{pmatrix}U\\\\ V\\end{pmatrix},\\ \\Rightarrow \\left.\\begin{array}{rl}X & = \\frac{1}{2}(U+V),\\\\ Y & =\\frac{1}{2}(-U+V).\\end{array}\\right\\}\n$$\n\n\n<div class=\"center\">\n\n```{r, echo=FALSE, label=Ej3.png,fig.cap=\"\"}\nknitr::include_graphics(\"Images/EjTranLineal2.png\",dpi=1200)\n```\n</div>\n\nLa **función de densidad** $f_{UV}$ es,\n$$\n\\begin{array}{rl}\nf_{UV}(u,v) & =\\frac{1}{\\left|\\mathrm{det}\\begin{pmatrix}1 & -1\\\\ 1& 1\\end{pmatrix}\\right|}\\cdot f_{XY}\\left(\\frac{1}{2}(u+v),\\frac{1}{2}(-u+v)\\right) \\\\ & =\\frac{1}{2}\\cdot \\frac{1}{2}\\cdot \\left(1+\\frac{1}{2}(-u+v)+\\frac{1}{2}(u+v)\\right)=\\frac{1}{4}(1+v),\n\\end{array}\n$$\npara $(u,v)$ perteneciente al cuadrado $C$ de vértices $(1,1)$, $(-1,1)$, $(-1,-1)$ y $(1,-1)$, o si se quiere para $-1\\leq u\\leq 1$, $-1\\leq v\\leq 1$, y $f_{UV}(u,v)=0$, en caso contrario.\n\nObservamos que es más cómodo trabajar con las variables $(u,v)$ en vez de trabajar con las variables $(x,y)$ por dos razones:\n\n- La región donde la **función de densidad** no es nula es más simple, ya que trabajar con un cuadrado simplifica mucho más los cálculos que trabajar con un rombo a la hora de hallar la **función de distribución**, **densidades marginales**, **densidades condicionadas**, **valores esperados**, etc.\n- La expresión de la **función de densidad** también es más simple, ya que sólo depende de la segunda variable $v$; sin embargo, la **función de densidad** inicial $f_{XY}$ dependía de las dos variables $x$ e $y$.\n</div>\n</div>\n\n<div class=\"example\">\n**Ejemplo**\n\nConsideremos el caso en que la variable aleatoria $(X,Y)$ tenga distribución **normal bidimensional**. \n\nRecordemos que su **función de densidad conjunta** era:\n$$\nf_{XY}(x,y)=\\frac{1}{2\\pi\\sqrt{1-\\rho^2}}\\mathrm{e}^{-\\frac{(x^2-2\\rho xy+y^2)}{2(1-\\rho^2)}},\\ -\\infty <x,y<\\infty.\n$$\nRecordemos que las **distribuciones marginales** son distribuciones $N(0,1)$.\n\nLa idea es hallar la **función de densidad conjunta** de una distribución normal bidimensional para la que sus **distribuciones marginales** sean dos normales $N(\\mu_1,\\sigma_1)$ y $N(\\mu_2,\\sigma_2)$.\n\n<div class=\"example-sol\">\n\nRecordemos que si $Z=N(0,1)$, entonces $\\sigma_1\\cdot Z+\\mu_1 =N(\\mu_1,\\sigma_1)$. Este hecho, motiva que consideremos el cambio lineal siguiente a las variables $X$ e $Y$:\n$$\n\\begin{pmatrix}U\\\\ V\\end{pmatrix}=\\begin{pmatrix}\\sigma_1 & 0\\\\ 0& \\sigma_2\\end{pmatrix}\\cdot\\begin{pmatrix}X\\\\ Y\\end{pmatrix}+\\begin{pmatrix}\\mu_1\\\\\\mu_2\\end{pmatrix},\\ \\Rightarrow \\left.\\begin{array}{rl}U & = \\sigma_1\\cdot X+\\mu_1,\\\\ V & =\\sigma_2\\cdot Y+\\mu_2.\\end{array}\\right\\}\n$$\n\nLa función de densidad conjunta $f_{UV}$ es:\n$$\n\\begin{array}{rl}\nf_{UV}(u,v) & = \\frac{1}{\\left|\\begin{pmatrix}\\sigma_1 & 0\\\\ 0& \\sigma_2\\end{pmatrix}\\right|} f_{XY}\\left(\\frac{u-\\mu_1}{\\sigma_1},\\frac{v-\\mu_2}{\\sigma_2}\\right)\n=\\frac{1}{\\sigma_1\\cdot \\sigma_2}f_{XY}\\left(\\frac{u-\\mu_1}{\\sigma_1},\\frac{v-\\mu_2}{\\sigma_2}\\right)\\\\ & =\n\\frac{1}{2\\pi\\sigma_1\\sigma_2\\sqrt{1-\\rho^2}}\\mathrm{e}^{-\\frac{\\left(\\left(\\frac{u-\\mu_1}{\\sigma_1}\\right)^2-2\\rho \\left(\\frac{u-\\mu_1}{\\sigma_1}\\right)\\left(\\frac{v-\\mu_2}{\\sigma_2}\\right)+\\left(\\frac{v-\\mu_2}{\\sigma_2}\\right)^2\\right)}{2(1-\\rho^2)}},\n\\end{array}\n$$\npara $(u,v)\\in\\mathbb{R}^2$.\n\nSi llamamos $\\mathbf{\\Sigma}$ a la matriz $\\mathbf{\\Sigma}=\\begin{pmatrix}\\sigma_1^2 & \\rho\\sigma_1\\sigma_2\\\\ \\rho\\sigma_1\\sigma_2 & \\sigma_2^2\\end{pmatrix}$, llamada **matriz de covarianzas** de la distribución normal $(U,V)$ la **función de densidad** anterior puede escribirse como:\n$$\nf_{UV}(u,v)=\\frac{1}{2\\pi \\sqrt{\\left|\\mathrm{\\Sigma}\\right|}}\\mathrm{e}^{-\\frac{1}{2}(\\mathbf{u}-\\mathbf{\\mu})^\\top \\mathbf{\\Sigma}^{-1}(\\mathbf{u}-\\mathbf{\\mu})},\\ \\mbox{donde $\\mathbf{u}=\\begin{pmatrix}u \\\\ v\\end{pmatrix}$ y $\\mathbf{\\mu}=\\begin{pmatrix}\\mu_1\\\\\\mu_2\\end{pmatrix}$.}\n$$\nLa variable aleatoria bidimensional $(X,Y)$ es la **variable aleatoria tipificada** con respecto de la variable $(U,V)$.\n</div>\n</div>\n\n\n### Transformaciones generales de variables aleatorias\n\nConsideremos una variable aleatoria bidimensional continua $(X,Y)$ con **función de densidad conjunta** $f_{XY}$.\n\nDefinimos la variable aleatoria bidimensional continua $(U,V)$ a partir de una transformación general de la variable $(X,Y)$. Es decir, existen dos funciones de dos variables $g_1$ y $g_2$ tal que:\n$$\nU  = g_1 (X,Y),\\quad \nV  = g_2 (X,Y).\n$$\nVamos a suponer que las funciones $g_1$ y $g_2$ son invertibles, es decir, dados $(u,v)$, podemos encontrar $(x,y)$ tal que $x=h_1(u,v)$ e $y=h_2(u,v)$. Las funciones $h_1$ y $h_2$ son las inversas de las funciones $g_1$ y $g_2$, respectivamente.\n\nLa **función de densidad conjunta** $f_{UV}$ se puede expresar de la forma siguiente en función de la **función de densidad conjunta** $f_{XY}$:\n\n$$\n\\begin{array}{rl}\nf_{UV}(u,v) & =\\left|\\mathrm{det}\\begin{pmatrix}\\frac{\\partial h_1}{\\partial u} & \\frac{\\partial h_1}{\\partial v}\\\\ \\frac{\\partial h_2}{\\partial u} & \\frac{\\partial h_2}{\\partial v}\\end{pmatrix}\\right|f_{XY}(h_1(u,v),h_2(u,v))\\\\ & =\\frac{1}{\\left|\\mathrm{det}\\begin{pmatrix}\\frac{\\partial g_1}{\\partial x} & \\frac{\\partial g_1}{\\partial y}\\\\ \\frac{\\partial g_2}{\\partial x} & \\frac{\\partial g_2}{\\partial y}\\end{pmatrix}\\right|_{x=h_1(u,v),y=h_2(u,v)}}f_{XY}(h_1(u,v),h_2(u,v)).\n\\end{array}\n$$\n\nA la matriz $\\begin{pmatrix}\\frac{\\partial g_1}{\\partial x} & \\frac{\\partial g_1}{\\partial y}\\\\ \\frac{\\partial g_2}{\\partial x} & \\frac{\\partial g_2}{\\partial y}\\end{pmatrix}$ se le llama **matriz jacobiana del cambio** y a la matriz $\\begin{pmatrix}\\frac{\\partial h_1}{\\partial u} & \\frac{\\partial h_1}{\\partial v}\\\\ \\frac{\\partial h_2}{\\partial u} & \\frac{\\partial h_2}{\\partial v}\\end{pmatrix}$, **matriz jacobiana del cambio inverso**.\n\n<div class=\"example\">\n\n**Ejemplo: cambio a polares**\n\nSea $(X,Y)$ una variable aleatoria bidimensional cuya **función de densidad conjunta** es:\n\n$$\nf_{XY}(x,y)=\n\\begin{cases}\n\\frac{2}{\\pi}\\left(x^2 + y^2\\right), & \\mbox{si }(x,y)\\in D_1, \\\\\n0, & \\mbox{en caso contrario,}\n\\end{cases}\n$$\ndonde $D_1$ es el disco de radio $1$:\n$$\nD_1 = \\{(x,y)\\in\\mathbb{R}^2,\\ | \\ x^2+y^2\\leq 1\\}.\n$$\nEl cambio a polares consiste en considerar las coordenadas polares $(r,\\alpha)$ de un punto cualquiera $(x,y)$ del plano, ver figura adjunta. El cambio que pasa de $(r,\\alpha)$ a $(x,y)$ (fijaos que es el cambio inverso, según nuestra notación o $h_1$y $h_2$, respectivamente) es:\n$$\nx=h_1(r,\\alpha)=r\\cdot \\cos\\alpha,\\quad y=h_2(r,\\alpha)=r\\cdot \\sin\\alpha.\n$$\n\n\n<div class=\"center\">\n\n```{r, echo=FALSE, label=Ej4.png,fig.cap=\"\"}\nknitr::include_graphics(\"Images/Polares.png\",dpi=1200)\n```\n</div>\n\n<div class=\"example-sol\">\nFijémonos que, con el cambio a polares, el disco unidad $D_1$ se transforma en el rectángulo $[0,1]\\times [0,2\\pi]$. \n\nHallemos el **jacobiano del cambio inverso**:\n$$\n\\mathrm{det}\\begin{pmatrix}\\frac{\\partial h_1}{\\partial u} & \\frac{\\partial h_1}{\\partial v}\\\\ \\frac{\\partial h_2}{\\partial u} & \\frac{\\partial h_2}{\\partial v}\\end{pmatrix} =\\mathrm{det}\\begin{pmatrix}\\cos\\alpha & -r\\sin\\alpha\\\\ \\sin\\alpha & r\\cdot\\cos\\alpha\\end{pmatrix} = r.\n$$\nLa **función de densidad conjunta** $f_{r\\alpha}$ en las nuevas variables (polares) es:\n$$\nf_{r\\alpha}(r,\\alpha)=r\\cdot \\frac{2}{\\pi}\\left((r\\cos\\alpha)^2+(r\\sin\\alpha)^2\\right)=\\frac{2}{\\pi}\\cdot r^3,\n$$\n si $(r,\\alpha)\\in [0,1]\\times [0,2\\pi]$.\n\n\nPodemos comentar que, gracias al cambio a polares, en este caso, es mucho más sencillo y cómodo trabajar con las variables $(r,\\alpha)$ en vez de trabajar con las variables $(x,y)$ por dos razones:\n\n- La región donde la **función de densidad** no es nula es más simple, ya que trabajar con un rectángulo simplifica mucho más los cálculos que trabajar con un disco a la hora de hallar la **función de distribución**, **densidades marginales**, **densidades condicionadas**, **valores esperados**, etc.\n\nPor ejemplo, comprobar que el área de la **función de densidad conjunta** $f_{r\\alpha}$ da $1$ es trivial:\n$$\n\\int_{r=0}^{r=1}\\int_{\\alpha =0}^{\\alpha =2\\pi}\\frac{2}{\\pi} r^3\\, d\\alpha\\, dr = \\frac{2}{\\pi}\\cdot 2\\pi \\left[\\frac{r^4}{4}\\right]_{r=0}^{r=1}=4\\cdot \\frac{1}{4}=1.\n$$\n\n- La expresión de la **función de densidad** también es más simple, ya que sólo depende de la primera variable $r$; sin embargo, la **función de densidad** inicial $f_{XY}$ dependía de las dos variables $x$ e $y$.\n\n</div>\n</div>\n\n\n","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","output-file":"5.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.4.245","bibliography":["references.bib"],"theme":"cosmo"},"extensions":{"book":{"multiFile":true}}},"pdf":{"identifier":{"display-name":"PDF","target-format":"pdf","base-format":"pdf"},"execute":{"fig-width":5.5,"fig-height":3.5,"fig-format":"pdf","fig-dpi":300,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"pdf","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"pdf-engine":"xelatex","standalone":true,"variables":{"graphics":true,"tables":true},"default-image-extension":"pdf","to":"pdf","output-file":"5.pdf"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"block-headings":true,"bibliography":["references.bib"],"documentclass":"scrreprt"},"extensions":{"book":{"selfContainedOutput":true}}}},"projectFormats":["html","pdf"]}