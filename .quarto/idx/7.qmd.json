{"title":"Ley de los grandes números y Teorema Central del Límite","markdown":{"headingText":"Ley de los grandes números y Teorema Central del Límite","containsRefs":false,"markdown":"\n## Muestras aleatorias simples\n\nEl pilar básico sobre el que se sustenta la **estadística inferencial** es el concepto de **muestra aleatoria simple**.\n\nUna **muestra aleatoria simple**, desde el punto de vista de la probabilidad es una distribución $n$ variables aleatorias, $X_1,\\ldots, X_n$ todas independientes entre sí e idénticamente distribuidas ya que queremos simular la repetición de un experimento $n$ veces de forma independiente.\n\nPor tanto, estudiar una **muestra aleatoria simple** equivale a estudiar su distribución.\n\nEn muchos casos, nos bastará estudiar la distribución de una variable que \"represente\" a dicha **muestra aleatoria simple**: la media muestral definida como $\\overline{X}=\\frac{X_1+\\cdots + X_n}{n}$.\n\nLas **leyes de los grandes números** nos dicen que, de alguna manera (que concretaremos más adelante), la media muestral y la media poblacional se \"parecen\" a la larga o cuando el número de repeticiones $n$ tiende a infinito.\n\nEl **Teorema Central del Límite** nos dice que la distribución de la media muestral tiende, sea cual sea la distribución de las variables $X_i$, a una normal. De ahí que la **distribución normal** sea la más importante en probabilidades y estadística.\n\n\n### La distribución de la media muestral\n\nVamos cómo se distribuye la media de un conjunto de variables normales e idénticamente distribuidas:\n\n<l class=\"prop\">Proposición. Distribución de la media muestral de $n$ variables normales independientes e idénticamente distribuidas. </l>\nSean $X_1,\\ldots, X_n$ $n$ variables normales de media $\\mu$ y varianza $\\sigma^2$, todas normales e independientes. Consideramos la variable $\\overline{X}=\\frac{X_1+\\cdots + X_n}{n}$ la media muestral. Entonces la distribución de la variable aleatoria $\\overline{X}$ es normal de la misma media $\\mu$ de las $X_i$ y varianza $\\frac{\\sigma^2}{n}$.\n\n<div class=\"dem\">\n**Demostración**\n\nConsideramos la variable aleatoria $n$-dimensional $\\mathbf{X}=(X_1,\\ldots,X_n)$. Dicha variable tendrá la distribución normal $n$-dimensional con vector de medias $\\mathbf{\\mu}=(\\mu,\\ldots,\\mu)^\\top$ y matriz de covarianzas $\\mathbf{\\Sigma}$ diagonal ya que recordemos que las $X_i$ son independientes y, por tanto, incorreladas o de covarianza nula:\n$$\n\\mathbf{\\Sigma}=\\begin{pmatrix}\n\\sigma^2 & 0 & \\ldots & 0 \\\\\n0 & \\sigma^2 & \\ldots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\vdots \\\\\n0 & 0 & \\ldots & \\sigma^2\n\\end{pmatrix}.\n$$\n\n\nPara hallar la variable  $\\overline{X}$, realizamos la transformación afín siguiente:\n$$\n\\overline{X}=\\left(\\frac{1}{n},\\ldots,\\frac{1}{n}\\right)\\cdot\\begin{pmatrix} X_1 \\\\ X_2\\\\\\vdots \\\\ X_n \\end{pmatrix}.\n$$\nAplicando la proposición sobre la transformación afín sobre una variable normal $n$-dimensional que vimos en el capítulo de distribuciones $n$-dimensionales con matriz de cambio $\\mathbf{C}=\\left(\\frac{1}{n},\\ldots,\\frac{1}{n}\\right)$ y $\\mathbf{c}=0$, tenemos que la distribución de $\\overline{X}$ será normal de media $\\mathbf{c}+\\mathbf{C}\\mathbf{\\mu} = \\mu$ y varianza (o matriz de covarianzas $1\\times 1$):\n$$\n\\mathbf{C}\\cdot\\mathbf{\\Sigma}\\cdot\\mathbf{C}^\\top =\\left(\\frac{1}{n},\\ldots,\\frac{1}{n}\\right)\\cdot\\begin{pmatrix}\n\\sigma^2 & 0 & \\ldots & 0 \\\\\n0 & \\sigma^2 & \\ldots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\vdots \\\\\n0 & 0 & \\ldots & \\sigma^2\n\\end{pmatrix}\\cdot \\begin{pmatrix}\\frac{1}{n}\\\\\\frac{1}{n}\\\\\\vdots\\\\\\frac{1}{n}\\end{pmatrix} =\\frac{\\sigma^2}{n}.\n$$\n</div>\n\n## Convergencia de sucesiones de variables aleatorias\n\n\nEn esta sección vamos a intentar concretar cómo la media muestral y la media poblacional de una **muestra aleatoria simple** se van pareciendo, así como la distribución de la **media muestral** se va \"acercando\" a la normalidad.\n\nPara ello, necesitamos introducir un conjunto de conceptos relacionados con la convergencia de variables aleatorias. \n\nEn primer lugar, introduciremos el concepto de **sucesión de variables aleatorias**:\n\n<l class=\"definition\"> Definición de sucesión de variables aleatorias. </l>\nConsideremos un experimento aleatorio sobre un **espacio muestral** $\\Omega$. Sea $P$ una probabilidad definida sobre el conjunto de sucesos de $\\Omega$. Entonces, si $X_1,X_2,\\ldots,X_n,\\ldots$ son variables aleatorias definidas sobre $\\Omega,P$, diremos que forman una **sucesión de variables aleatorias** y lo denotaremos por $\\{X_n\\}_{n=1}^\\infty$.\n\n<div class=\"example\">\n**Ejemplo: lanzamiento de un dado**\n\nConsideremos el experimento aleatorio de ir lanzando un dado no trucado. Definimos la variable aleatoria $X_n$ como el resultado del dado el lanzamiento $n$-ésimo. \n\nEntonces, la sucesión de variables aleatorias $X_1,\\ldots,X_n,\\ldots$ es la asociada al lanzamiento del dado.\n<div class=\"example-sol\">\n¡Ojo! no confundir la sucesión de variables aleatorias $X_1,\\ldots,X_n,\\ldots$ con la sucesión de resultados de dichas variables aleatorias $x_1,\\ldots, x_n,\\ldots$. Lo primero correspondería a variables aleatorias con su función de probabilidad, esperanza, varianza, etc., y lo segundo sería simplemente una sucesión numérica de valores enteros entre 1 y 6.\n</div>\n</div>\n\n### Convergencia casi segura\n<l class=\"definition\"> Definición de convergencia casi segura. </l>\nSea $X_1,\\ldots,X_n,\\ldots$ una sucesión de variables aleatorias y sea $X$ una variable aleatoria definida sobre el mismo espacio muestral $\\Omega$ y con la misma probabilidad de sucesos. Diremos que la sucesión $\\{X_n\\}_{n=1}^\\infty$ converge **casi seguramente** hacia $X$ si\n$$\nP\\left(\\{w\\in \\Omega\\ |\\ \\lim_{n\\to\\infty} X_n(w)=X(w)\\}\\right)=1.\n$$\nLo denotaremos por $X_n\\stackrel{c.s.}{\\longrightarrow}X$.\n\n\nEs decir, si el conjunto de elementos $w$ del espacio muestral $\\Omega$ que cumplen que el límite de la sucesión de números reales $(X_n(w))_n$ tiende a $X(w)$ tiene probabilidad $1$.\n\nDe ahí viene el nombre de **casi segura**: el conjunto de valores $w$ del espacio muestral tal que la sucesión numérica $(X_n(w))_n$ **no converge** a $X(w)$ tiene probabilidad 0.\n\nComprobar la **convergencia casi segura** a partir de la definición puede ser muy complicado. Por suerte, existe la proposición siguiente que nos hace la vida más fácil:\n\n<l class=\"prop\"> Proposición. </l>\nSea $X_1,\\ldots,X_n,\\ldots$ una sucesión de variables aleatorias y sea $X$ una variable aleatoria definida sobre el mismo espacio muestral $\\Omega$ y con la misma probabilidad de sucesos. \nEntonces $X_n\\stackrel{c.s.}{\\longrightarrow}X$ si, y sólo si, para todo valor $\\epsilon >0$, la serie siguiente\n$$\n\\sum_{n=1}^\\infty P(|X_n-X|>\\epsilon),\n$$\nes convergente.\n\n** Ejemplo: convergencia casi segura frecuencias de un dado**\n<div class=\"example\">\nVeamos si la sucesión $\\{X_n\\}_{n=1}^\\infty$ tiene convergencia **casi segura** hacia la variable $X$ cuya **función de probabilidad** es:\n<div class=\"center\">\n| $X$| 1 | 2 | 3 | 4 | 5 | 6\n|--|--|--|--|--|--|--|\n| $P_X$  | $\\frac{1}{6}$ | $\\frac{1}{6}$ | $\\frac{1}{6}$ | $\\frac{1}{6}$ | $\\frac{1}{6}$ | $\\frac{1}{6}$\n</div>\n<div class=\"example\">\n\nEn este caso el espacio muestral $\\Omega$ es $\\Omega=\\{1,2,3,4,5,6\\}$ y la **función de probabilidad** de cada $X_i$ corresponde con la tabla anterior. \n\nSeguidamente, de cara a aplica la proposición anterior, vamos a hallar la **función de probabilidad** de la variable $D_n=X_n-X$.\nLos valores de la variable anterior son: $D_n(\\Omega)=\\{-5,-4,-3,-2,-1,0,1,2,3,4,5\\}$.\n\nLa **función de probabilidad** conjunta de la variable $(X_n,X)$ será al ser $X_n$ y $X$ independientes:\n$$\nP_{X_nX}(x_n,x)=P_{X_n}(x_n)\\cdot P_X(x)=\\frac{1}{6}\\cdot \\frac{1}{6}=\\frac{1}{36},\n$$\npara todo $x_n=1,2,3,4,5,6$ y para todo $x=1,2,3,4,5,6$.\n\n\nLa **función de probabilidad** de la variable $D_n$ será:\n$$\n\\scriptsize{\n\\begin{array}{rl}\nP_{D_n}(-5) & =P_{X_nX}(1,6)=\\frac{1}{36}, \\\\\nP_{D_n}(-4) & =P_{X_nX}(2,6)+P_{X_nX}(1,5)=\\frac{2}{36}, \\\\\nP_{D_n}(-3) & =P_{X_nX}(3,6)+P_{X_nX}(2,5)+P_{X_nX}(1,4)=\\frac{3}{36}, \\\\\nP_{D_n}(-2) & =P_{X_nX}(4,6)+P_{X_nX}(3,5)+P_{X_nX}(2,4)+P_{X_nX}(1,3)=\\frac{4}{36}, \\\\\nP_{D_n}(-1) & =P_{X_nX}(5,6)+P_{X_nX}(4,5)+P_{X_nX}(3,4)+P_{X_nX}(2,3)+P_{X_nX}(1,2)=\\frac{5}{36}, \\\\\nP_{D_n}(0) & =P_{X_nX}(6,6)+P_{X_nX}(5,5)+P_{X_nX}(4,4)+P_{X_nX}(3,3)+P_{X_nX}(2,2)+P_{X_nX}(1,1)=\\frac{6}{36}, \\\\\nP_{D_n}(1) & =P_{X_nX}(6,5)+P_{X_nX}(5,4)+P_{X_nX}(4,3)+P_{X_nX}(3,2)+P_{X_nX}(2,1)=\\frac{5}{36}, \\\\\nP_{D_n}(2) & =P_{X_nX}(6,4)+P_{X_nX}(5,3)+P_{X_nX}(4,2)+P_{X_nX}(3,1)=\\frac{4}{36}, \\\\\nP_{D_n}(3) & =P_{X_nX}(6,3)+P_{X_nX}(5,2)+P_{X_nX}(4,1)=\\frac{3}{36}, \\\\\nP_{D_n}(4) & =P_{X_nX}(6,2)+P_{X_nX}(5,1)=\\frac{2}{36}, \\\\\nP_{D_n}(5) & =P_{X_nX}(6,1)=\\frac{1}{36}.\n\\end{array}\n}\n$$\n\nSea $\\epsilon$ un valor real entre 0 y 1: $0<\\epsilon <1$. Entonces el suceso $\\{|D_n|>\\epsilon\\}$ será el complementario del suceso $\\{D_n=0\\}$ ya que el único valor entre $-5$ y $5$ que no cumple $|D_n|>\\epsilon$ es el valor $D_n=0$. Por tanto:\n$$\nP(|D_n|>\\epsilon)=1-P(D_n=0)=1-P_{D_n}(0)=1-\\frac{1}{6}=\\frac{5}{6}.\n$$\nLa serie $\\sum\\limits_{n=1}^\\infty \\frac{5}{6}$ no es convergente de forma obvia. Por tanto, deducimos que la sucesión $\\{X_n\\}_{n=1}^\\infty$ no converge **casi seguramente** hacia la variable $X$.\n\n</div>\n</div>\n\n\n### Convergencia en probabilidad\n<l class=\"definition\"> Definición de convergencia en probabilidad. </l>\nSea $X_1,\\ldots,X_n,\\ldots$ una sucesión de variables aleatorias y sea $X$ una variable aleatoria definida sobre el mismo espacio muestral $\\Omega$ y con la misma probabilidad de sucesos. Diremos que la sucesión $\\{X_n\\}_{n=1}^\\infty$ converge **en probabilidad** hacia $X$ si para cualquier valor $\\epsilon >0$,\n$$\n\\lim_{n\\to\\infty} P(|X_n-X|>\\epsilon \\})=0.\n$$\nLo denotaremos por $X_n\\stackrel{c.p.}{\\longrightarrow}X$.\n\nEl límite de la probabilidad de los sucesos formados por los $w\\in\\Omega$ tal que $|X_n(w)-X(w)|>\\epsilon$ vale 0.\n\n<l class=\"observ\">Observación.</l>\nUna definición equivalente de **convergencia en probabilidad** es que para todo valor $\\epsilon >0$,\n$$\n\\lim_{n\\to\\infty} P(|X_n(w)-X(w)|\\leq \\epsilon \\})=1.\n$$\n<l class=\"observ\">Observación. </l>\nLa convergencia **casi segura** implica la convergencia **en probabilidad** ya que si la sucesión $\\{X_n\\}_{n=1}^\\infty$ converge **casi seguramente** hacia $X$, la serie $\\sum_{n=1}^\\infty P(|X_n-X|>\\epsilon)$ será convergente y, por tanto, el límite de su término $P(|X_n-X|>\\epsilon)$ tenderá a cero, hecho que equivale a la convergencia **en probabilidad**.\n\nEl siguiente resultado nos puede ayudar algunas veces a comprobar la **convergencia en probabilidad**:\n\n<l class=\"prop\">Proposición. </l>\nSea $X_1,\\ldots,X_n,\\ldots$ una sucesión de variables aleatorias. Sea $\\mu_n$ el valor medio de la variable $X_n$, $E(X_n)=\\mu_n$ y $\\sigma_n^2$ su varianza: $\\mathrm{Var}(X_n)=\\sigma_n^2$. Supongamos que $\\lim_{n\\to\\infty}\\sigma_n^2=0$. Entonces,\n$$\nX_n-\\mu_n\\stackrel{c.p.}{\\longrightarrow} 0.\n$$\n\n<div class=\"dem\">\n**Demostración**\n\nUsando la desigualdad de Chebyschev, podemos escribir:\n$$\nP(|X_n-\\mu_n|>\\epsilon \\}) \\leq \\frac{\\sigma_n^2}{\\epsilon^2}.\n$$\nTomando límite a cada parte de la desigualdad anterior tenemos:\n$$\n0\\leq \\lim_{n\\to\\infty} P(|X_n-X|>\\epsilon \\}) \\leq \\lim_{n\\to\\infty}\\frac{\\sigma_n^2}{\\epsilon^2}=0,\n$$\nde donde deducimos que $\\lim_{n\\to\\infty} P(|X_n-X|>\\epsilon \\})=0$, tal como queríamos ver.\n</div>\n\n<div class=\"example\">\n**Ejemplo del lanzamiento de un dado (continuación)**\n\nEn el ejemplo anterior del lanzamiento de un dado, no hay convergencia en probabilidad ya que comprobamos que para $0<\\epsilon<1$,\n\n<div class=\"example-sol\">\n\n$$\nP(|X_n-X|>\\epsilon)=\\frac{5}{6}.\n$$\nPor tanto, para $0<\\epsilon<1$,  $\\lim_{n\\to\\infty} P(|X_n-X|>\\epsilon \\})=\\frac{5}{6}\\neq 0.$\n</div>\n</div>\n\n<div class=\"example\">\n**Ejemplo: covergencia en probabilidad distribucione exponenciales**\n\nConsideremos las variables aleatorias $X_n$ con función de densidad:\n$$\nf_{X_n}(x)=\\begin{cases}\n\\lambda\\cdot  n\\cdot\\mathrm{e}^{-\\lambda\\cdot n\\cdot  x}, & \\mbox{si }x>0,\\\\\n0, & \\mbox{en caso contrario.}\n\\end{cases}\n$$\nEstas  variables $X_n$ tienen distribución exponencial de parámetro $\\lambda\\cdot  n$.\n\nVeamos que $\\{X_n\\}_{n=1}^\\infty\\stackrel{c.p}{\\longrightarrow} 0$.\n\n<div class=\"example-sol\">\n\nDado $\\epsilon >0$, calculemos $P(|X_n|>\\epsilon \\})$:\n$$\nP(|X_n|>\\epsilon \\}) = \\int_\\epsilon^\\infty \\lambda\\cdot  n\\cdot \\mathrm{e}^{-\\lambda\\cdot  n x}\\, dx =\\lambda\\cdot  n\\cdot  \\left[\\frac{1}{-\\lambda \\cdot n}\\cdot\\mathrm{e}^{-\\lambda\\cdot  n\\cdot  x}\\right]_\\epsilon^\\infty =\\mathrm{e}^{-\\lambda\\cdot  n\\cdot  \\epsilon}\\stackrel{n\\to\\infty}{\\longrightarrow} 0,\n$$\ntal como queríamos ver.\n\n</div>\n\n</div>\n\n### Convergencia en ley o en distribución \n\n<l class=\"definition\"> Definición de convergencia en ley o distribución. </l>\nSea $X_1,\\ldots,X_n,\\ldots$ una sucesión de variables aleatorias y sea $X$ una variable aleatoria definida sobre el mismo espacio muestral $\\Omega$ y con la misma probabilidad de sucesos. Sea $F_{X_n}$ y $F_X$ las funciones de distribución de la variable $X_n$ y $X$, respectivamente. Diremos que la sucesión $\\{X_n\\}_{n=1}^\\infty$ converge **en ley, o en distribución** hacia $X$ si,\n$$\n\\lim_{n\\to\\infty} F_{X_n}(x)=F(x),\n$$\npara todo valor $x\\in\\mathbb{R}$.\n\nLo denotaremos por $X_n\\stackrel{{\\cal L}}{\\longrightarrow}X$.\n\nEl resultado siguiente simplifica algunas veces comprobar que la sucesión $X_n$ converge en ley hacia $X$:\n\n<l class=\"prop\"> Proposición. </l>\nSea $\\{X_n\\}_{n=1}^\\infty$ una sucesión de variables aleatorias y sea $X$ una variable aleatoria definida sobre el mismo espacio muestral $\\Omega$ y con la misma probabilidad de sucesos. Sean $\\phi_{X_n}$ y $\\phi_X$ las funciones características de $X_n$ y $X$, respectivamente. Entonces, la sucesión converge **en ley** hacia $X$, $X_n\\stackrel{{\\cal L}}{\\longrightarrow}X$, si, y sólo si,\n$$\n\\lim_{n} \\phi_{X_n}(t) = \\phi_X(t),\n$$\npara cualquier número $t\\in\\mathbb{R}$.\n\n\n<div class=\"example\">\n**Ejemplo de la distribución binomial $B(n,p)$**\n\nVeamos que si $X_n=B(n,p_n)$ tiene distribución binomial de parámetros $n$ y $p_n$, con $p_n=\\frac{\\lambda}{n}$, con $\\lambda$ fijo,\n$$\nB(n,p)\\stackrel{{\\cal L}}{\\longrightarrow}Poiss(\\lambda).\n$$\n<div class=\"example-sol\">\nEn el tema de distribuciones notables demostramos que para todo $k\\in\\{0,\\ldots,n\\}$,\n$$\nP(X_n = k)=\\binom{n}{k}\\cdot p_n^k\\cdot (1-p_n)^{n-k}\\stackrel{n\\to\\infty}{\\longrightarrow} P(X=k)=\\frac{\\lambda^k}{k!}\\cdot\\mathrm{e}^{-\\lambda}.\n$$\nEntonces tenemos que dado $x\\in\\mathbb{R}$, existe $k\\in\\{0,\\ldots,n\\}$, tal que $k\\leq x< k+1$. Por tanto,\n$$\n\\begin{array}{rl}\n\\lim\\limits_{n\\to\\infty} F_{X_n}(x) \n& = \\lim\\limits_{n\\to\\infty} F_{X_n}(k)=\\lim\\limits_{n\\to\\infty} P(X_n=0)+\\cdots + P(X_n=k) \\\\ \n& =\\lim\\limits_{n\\to\\infty} P(X_n=0)+\\cdots + \\lim\\limits_{n\\to\\infty} P(X_n=k)\\\\\n& = P(X=0)+\\cdots + P(X=k)\\\\ &  =F_X(k)=F_X(x),\n\\end{array}\n$$\ntal como queríamos demostrar.\n</div>\n\n### Relaciones entre las distintas convergencias\nEl resultado siguiente nos dice cuando un tipo de convergencia implica la otra:\n\n<l class=\"prop\"> Proposición. </l>\nSea $\\{X_n\\}_{n=1}^\\infty$ una sucesión de variables aleatorias y sea $X$ una variable aleatoria definida sobre el mismo espacio muestral $\\Omega$ y con la misma probabilidad de sucesos. Entonces:\n\n* Si $X_n\\stackrel{c.s.}{\\longrightarrow} X$, entonces $X_n\\stackrel{c.p.}{\\longrightarrow} X$.\n\n* Si $X_n\\stackrel{c.p.}{\\longrightarrow} X$, entonces $X_n\\stackrel{{\\cal L}}{\\longrightarrow} X$.\n\nEn resumen, la convergencia más fuerte es la **casi segura**, luego vendría la convergencia **en probabilidad** y, por último, la convergencia **en ley**:\n\n$$\n\\mbox{Conv. casi segura }\\Rightarrow \\mbox{ Conv. en probabilidad }\\Rightarrow\\mbox{ Conv. en ley.}\n$$\n\n## Leyes de los grandes números\n\n\nComo ya comentamos al principio del tema, las **leyes de los grandes números** estudian el comportamiento de la **media muestral** $\\overline{X}_n$ cuando la sucesión de variables aleatorias $\\{X_n\\}_{n=1}^\\infty$ se va hacia infinito.\n\nMás concretamente, diremos que una sucesión de variables aleatorias $\\{X_n\\}_{n=1}^\\infty$ cumple una **ley de los grandes números** si existe un sucesión numérica $(a_n)_n$ tal que la sucesión de variables aleatorias $\\{\\overline{X}_n-a_n\\}$ converge \"de alguna manera\" de las que hemos visto hacia 0.\n\nSi este \"alguna manera\" es la convergencia más fuerte, o la **casi segura**, tendremos la **ley fuerte de los grandes números**. \n\nEn cambio, si la convergencia es **en probabilidad**, tendremos la **ley débil de los grandes números**.\n\n### Leyes débiles de los grandes números\n\n<l class=\"prop\"> Teorema. Ley débil de los grandes números. </l>\nSea $\\{X_n\\}_{n=1}^\\infty$ una sucesión de variables aleatorias independientes dos a dos tal que sus varianzas existen y están acotadas por una constante independiente de $n$. Entonces,\n$$\n\\overline{X}_n-\\frac{1}{n}\\sum_{i=1}^n \\mu_i\\stackrel{c.p.}{\\longrightarrow} 0,\n$$\ndonde $\\mu_i = E(X_i)$.\n\nDicho en otras palabras: en las condiciones de la proposición anterior, la diferencia entre la sucesión de **medias muestrales** como variables aleatorias y la sucesión numérica de la medias poblacionales de dichas variables aleatorias tiende en **probabilidad** hacia 0.\n\n<div class=\"dem\">\n**Demostración**\n\nComo las variables son independientes dos a dos la varianza de la suma es la suma de las varianzas:\n$$\n\\mathrm{Var}(\\overline{X}_n)=\\frac{1}{n^2}\\mathrm{Var}(\\sum_{i=1}^n X_i)=\\frac{1}{n^2}\\sum_{i=1}^n \\sigma_i^2,\n$$\ndonde $\\sigma_i^2 = \\mathrm{Var}(X_i)$.\n\nSabemos por hipótesis que existe una constante $M$ tal que $\\sigma_i^2\\leq M$ para todo $i$. Por tanto,\n$$\n\\mathrm{Var}(\\overline{X}_n)=\\frac{1}{n^2}\\sum_{i=1}^n \\sigma_i^2\\leq \\frac{1}{n^2}Mn =\\frac{M}{n}.\n$$\n\nEl valor del valor medio de la media muestral será:\n$$\nE(\\overline{X}_n)=\\frac{1}{n}\\sum_{i=1}^n E(X_i)=\\frac{1}{n}\\sum_{i=1}^n \\mu_i. \n$$\nUsando la desigualdad de Chebyschev, deducimos, dado un $\\epsilon >0$:\n$$\nP\\left(\\left|\\overline{X}_n-\\frac{1}{n}\\sum_{i=1}^n \\mu_i\\right|>\\epsilon\\right) \\leq \\frac{\\mathrm{Var}(\\overline{X}_n)}{\\epsilon^2}\\leq \\frac{M}{n\\epsilon^2}.\n$$\nPor tanto, tomando límites en las dos partes de la desigualdad anterior, deducimos\n$$\n\\lim_{n\\to \\infty}P\\left(\\left|\\overline{X}_n-\\frac{1}{n}\\sum_{i=1}^n \\mu_i\\right|>\\epsilon\\right) =0,\n$$\ntal como queríamos ver.\n\n</div>\n\n\nDel teorema anterior obtenemos las consecuencias siguientes:\n\n<l class=\"prop\">Corolario. </l>\nSea $\\{X_n\\}_{n=1}^\\infty$ una sucesión de variables aleatorias independientes dos a dos tal que todas tienes la misma esperanza $\\mu$ y la misma varianza $\\sigma^2$. Entonces,\n$$\n\\overline{X}_n\\stackrel{c.p.}{\\longrightarrow} \\mu,\n$$\n\n<div class=\"dem\">\n**Demostración**\n\nEn este caso tenemos que $\\mu_i=\\mu$ y, por tanto, $\\frac{1}{n}\\sum\\limits_{i=1}^n \\mu_i =\\frac{1}{n}\\cdot n\\mu=\\mu$. Si aplicamos el teorema de la **ley débil de los grandes números** nos sale el resultado enunciado.\n</div>\n\n\n\n<l class=\"prop\">Corolario. </l>\nSea $\\{X_n\\}_{n=1}^\\infty$ una sucesión de variables aleatorias independientes dos a dos e idénticamente distribuidas tal que todas tienes la misma esperanza $\\mu$. Entonces,\n$$\n\\overline{X}_n\\stackrel{c.p.}{\\longrightarrow} \\mu,\n$$\n\n<div class=\"dem\">\n**Demostración**\n\nTrivial a partir del Corolario anterior.\n</div>\n\n\n<div class=\"example\">\n**Ejemplo: lanzamiento de una moneda**\n\nVamos a simular la **ley débil de los grandes números** en el caso en que el experimento aleatorio sea el lanzamiento de una moneda.\n\nEn este caso, tendremos que las variables aleatorias $X_n$ tendrán distribución de Bernoulli de parámetro $p=\\frac{1}{2}$.\n\nLa variable $\\overline{X}_n$ representa la proporción de caras ($X_n=1$) en el lanzamiento de la moneda $n$ veces. Nos preguntamos si dicha proporción de caras tiende al parámetro $p$ en probabilidad.\n\n<div class=\"example-sol\">\nVamos a hallar una muestra para cada variable $\\overline{X}_n=\\frac{\\sum\\limits_{i=1}^n X_i}{n}$.\n\nPara ello, vamos a repetir el experimento de lanzar la moneda $N=100$ veces y lo repetimos $k=500$ ocasiones. \n\nLos resultados estarán en una matriz $k\\times N =500\\times 100$ donde cada fila de la matriz simulará una repetición del experimento de lanzar la moneda $N=100$ veces.\n\nDada la fila $i$-ésima, iremos calculando $\\overline{X}_1^{(i)},\\overline{X}_2^{(i)},\\ldots,\\overline{X}_{N=100}^{(i)}$.\n\nLuego, fijado un $\\epsilon$, para cada $n$, aproximaremos la probabilidad $P\\left(\\left|\\overline{X}_n-\\frac{1}{2}\\right|>\\epsilon\\right)$ usando la fórmula de Laplace: \n$$\np_n=P\\left(\\left|\\overline{X}_n-\\frac{1}{2}\\right|>\\epsilon\\right) \\approx\\frac{\\#\\left\\{\\mbox{$i$ tal que  $\\left|\\overline{X}_n^{(i)}-\\frac{1}{2}\\right|>\\epsilon$}\\right\\}}{k}.\n$$\n\nPara comprobar dicha afirmación, la idea es hallar para cada valor $n$, una muestra para cada variable $\\overline{X}_n=\\frac{\\sum\\limits_{i=1}^n X_i}{n}$.\n\nPara hallar una muestra de cada variable $\\overline{X}_n$, seguimos los pasos siguientes:\n\n* En primer lugar, simulamos la repetición del experimento de lanzar la moneda $N=100$ veces y lo repetimos $k=500$ ocasiones. \nLos resultados estarán en una matriz $k\\times N =500\\times 100$ donde cada fila de la matriz simulará una repetición del experimento de lanzar la moneda $N=100$ veces:\n\n```{r}\nN=100\nk=500\nset.seed(2019) \n## fijamos la semila de aleatoriedad\n## por reproducibilidad\nvalores.experimento=matrix(sample(c(0,1),N*k,replace=TRUE),k,N)\n```\n\nLos primeros resultados son:\n\n```{r,echo=FALSE}\nvalores.experimento[1:5,1:12]\n```\n\n...\n\n* En segundo lugar, dada la fila $i$-ésima de la matriz anterior, iremos calculando $\\overline{X}_1^{(i)},\\overline{X}_2^{(i)},\\ldots,\\overline{X}_{N=100}^{(i)}$ guardando los resultados en una matriz de medias muestrales.\nAntes de nada, creamos la función que nos realizará la operación anterior dado un vector cualquiera `x`:\n\n```{r}\ncálculo.xnbarra = function(x){\n  return(cumsum(x)/(1:length(x)))\n}\n```\n\nA partir de la matriz de los resultados, aplicamos la función anterior a cada fila y hallaremos una matriz con todas las $\\overline{X}_n^{(i)}$:\n\n```{r}\nmatriz.medias.muestrales = t(apply(valores.experimento,\n                                   1,cálculo.xnbarra))\n```\n\nLa columna $j$-ésima de la matriz `matriz.medias.muestrales` contiene una muestra de $k=500$ valores de la variable $\\overline{X}_j$.\n\n\n* En último lugar, fijado un $\\epsilon$, para cada $n$, aproximaremos la probabilidad $P\\left(\\left|\\overline{X}_n-\\frac{1}{2}\\right|>\\epsilon\\right)$ usando la fórmula de Laplace: \n$$\np_n=P\\left(\\left|\\overline{X}_n-\\frac{1}{2}\\right|>\\epsilon\\right) \\approx\\frac{\\#\\left\\{\\mbox{$i$ tal que  $\\left|\\overline{X}_n^{(i)}-\\frac{1}{2}\\right|>\\epsilon$}\\right\\}}{k}.\n$$\nLa columna $j$-ésima de la matriz `matriz.medias.muestrales` es una muestra de la variable $\\overline{X}_j$. Por tanto, para hallar la aproximación de $p_n$, miramos cuántos valores de la columna $j$-ésima de la matriz anterior verifican $\\left|\\overline{X}_j^{l}-\\frac{1}{2}\\right|>\\epsilon$, para $l=1,\\ldots, k$:\n```{r}\nepsilon=0.1\nprobabilidades.pn= colSums(abs(matriz.medias.muestrales-0.5) > epsilon)/k\n```\n\nPara ver los resultados, dibujamos el gráfico $n$ vs. $p_n$:\n\n<div class=\"center\">\n```{r, echo=FALSE,fig=TRUE,fig.height=5.8}\nepsilon=0.1\nprobabilidades.pn= colSums(abs(matriz.medias.muestrales-0.5) > epsilon)/k\nplot(1:N,probabilidades.pn,type=\"l\",xlab=expression(N),\n     ylab=expression(p[n]),\n     main=\"Simulación de la convergencia en probabilidad\",col=\"red\")\n```\n</div>\n\nObservamos que las probabilidades tienden a cero tal como nos dice el **Teorema de la ley débil de los grandes números**.\n</div>\n\n</div>\n\n\n### Convergencia de los momentos muestrales\n\n´Dada una sucesión de variables aleatorias, definimos los momentos muestrales de la forma siguiente:\n\n<l class=\"definition\"> Definición de los momentos muestrales de una sucesión de variables aleatorias.</l>\nSea $\\{X_n\\}_{n=1}^\\infty$ una sucesión de variables aleatorias. Dado $k$ valor entero positivo, definimos el **momento muestral** de orden $k$ como la sucesión de variables aleatorias siguientes:\n$$\nM_k^{(n)} = \\frac{1}{n}\\sum_{i=1}^n X_i^k.\n$$\n\n<l class=\"observ\"> Observación: </l>\nel **momento muestral** de orden $k=1$ es la **media muestral** $\\overline{X}_n$.\n\n\n<l class=\"definition\"> Definición de los momentos muestrales centrados de una sucesión de variables aleatorias.</l>\nSea $\\{X_n\\}_{n=1}^\\infty$ una sucesión de variables aleatorias. Dado $k$ valor entero positivo, definimos el **momento muestral centrado en la media** de orden $k$ como la sucesión de variables aleatorias siguientes:\n$$\nMC_k^{(n)} = \\frac{1}{n}\\sum_{i=1}^n (X_i-\\overline{X}_n)^k.\n$$\n\n<l class=\"observ\"> Observación: </l>\nel **momento muestral centrado en la media** de orden $k=2$ es la **varianza muestral** $S_{X_n}^2$.\n\n\n<l class=\"definition\"> Definición de la covarianza y el coeficiente de correlación muestral de una sucesión de variables aleatorias.</l>\nSea $\\{(X_n,Y_n)\\}_{n=1}^\\infty$ una sucesión de variables aleatorias bidimensionales. Definimos la  **covarianza muestral** como la sucesión de variables aleatorias siguientes:\n$$\nS_{X_n,Y_n} = \\frac{1}{n}\\sum_{i=1}^n (X_i-\\overline{X}_n)(Y_i-\\overline{Y}_n),\n$$\ny el **coeficiente de correlación muestral** como la sucesión de variables aleatorias siguientes:\n$$\nR_{X_n,Y_n}=\\frac{S_{X_n,Y_n}}{\\sqrt{S_{X_n}^2 S_{Y_n}^2}}.\n$$\n\nDada $\\{X_n\\}_{n=1}^\\infty$ una sucesión de variables aleatorias y $k$ un valor entero positivo, en el tema de Complementos de variables aleatorias, definimos los momentos y los momentos centrales de orden $k$ para cada de dichas variables como:\n$$\nm_k^{(n)} = E\\left(X_n^k\\right),\\quad\\mu_k^{(n)}=E\\left(\\left(X_n-\\mu_n\\right)^k\\right),\n$$\ndonde $\\mu_n$ es el valor medio de la variable $X_n$: $\\mu_n = E(X_n)$.\n\nAsí mismo, dada $\\{(X_n,Y_n)\\}_{n=1}^\\infty$ una sucesión de variables aleatorias bidimensionales, en el tema de variables aleatorias bidimensionales definimos para cada variable $(X_n,Y_n)$ la covarianza $\\sigma_{X_nY_n}$ y el coeficiente de correlación $\\rho_{X_nY_n}$:\n$$\n\\sigma_{X_nY_n}=E((X_n-\\mu_{X_n})(Y_n-\\mu_{Y_n})),\\quad \\rho_{X_nY_n}=\\frac{\\sigma_{X_nY_n}}{\\sqrt{\\sigma_{X_n}^2\\sigma_{Y_n}^2}}.\n$$\n\nDada una sucesión de variables aleatorias $\\{X_n\\}_{n=1}^\\infty$, el resultado siguiente nos relaciona los **momentos muestrales** y los **momentos muestrales centrados en la media** con los **momentos** y los **momentos centrales** de cada variable:\n\n<l class=\"prop\"> Teorema. Convergencia de los momentos muestrales y los momentos muestrales centrados en la media. </l>\nSea $\\{X_n\\}_{n=1}^\\infty$ una sucesión de variables aleatorias **independientes dos a dos e idénticamente distribuidas** y dado un entero positivo $k$, supongamos que para cada $n$, existe el **momento de orden $k$**, $m_k$ y el **momento central de orden $k$**, $\\mu_k$, que no dependerán de $n$ al ser idénticamente distribuidas. Entonces las sucesiones de variables aleatorias $\\{M_n^{(k)}\\}_{n=1}^\\infty$ y $\\{MC_n^{(k)}\\}_{n=1}^\\infty$ tienden a $m_k$ y $\\mu_k$, respectivamente, en **probabilidad**\n$$\nM_n^{(k)}\\stackrel{c.p.}{\\longrightarrow} m_k,\\quad MC_n^{(k)}\\stackrel{c.p.}{\\longrightarrow} \\mu_k.\n$$\n\n\n<div class=\"dem\">\n**Demostración**\n\nConsideremos la sucesión de variables aleatorias $\\{X_n^k\\}_{n=1}^\\infty$. Como las variables aleatorias de la sucesión  $\\{X_n\\}_{n=1}^\\infty$ son independientes dos a dos e idénticamente distribuidas, las variables de la sucesión $\\{X_n^k\\}_{n=1}^\\infty$ también lo serán.\n\nLa idea es aplicar la **ley débil de los grandes números** a la sucesión anterior.\n\nEl valor medio de cada variable de la sucesión $\\{X_n^k\\}_{n=1}^\\infty$ será: $\\tilde{\\mu}_n^{(k)}= E(X_n^{k})=m_k$ el momento de orden $k$.\n\nEntonces, si hacemos $\\frac{1}{n}\\sum\\limits_{i=1}^n \\tilde{\\mu}_n^{(k)}$ obtenemos:\n$\\frac{1}{n} n\\cdot m_k=m_k.$\n\nAplicando la **ley débil de los grandes números** a la sucesión $\\{X_n^k\\}_{n=1}^\\infty$, tendremos que\n$$\n\\overline{X^k}_n \\stackrel{c.p.}{\\longrightarrow}m_k,\n$$\npero $\\overline{X^k}_n$ vale:\n$$\n\\overline{X^k}_n=\\frac{1}{n}\\sum_{i=1}^n X_i^k,\n$$\nvariable aleatoria que coincide con el momento muestral de orden $k$, $M_n^{(k)}$, tal como queríamos demostrar.\n\nDejamos como ejercicio la demostración de los momentos centrales. Razonando de la misma manera, no tiene dificultad alguna.\n</div>\n\nEnunciemos ahora el resultado para las covarianzas y las correlaciones muestrales:\n\n<l class=\"prop\"> Teorema: convergencia de la covarianza y el coeficiente de correlación muestrales. </l>\nSea $\\{(X_n,Y_n)\\}_{n=1}^\\infty$ una sucesión de variables aleatorias bidimensionales independientes dos a dos e idénticamente distribuidas. \nSea $\\sigma_{X,Y}, \\rho_{XY}$ la covarianza y el coeficiente de correlación de cada par de variables que, al ser idénticamente distribuidas, no dependen de $n$. Entonces las sucesiones de las covarianzas muestrales $\\{S_{X_n,Y_n}\\}_{n=1}^\\infty$ y los coeficientes de correlación muestrales $\\{R_{X_nY_n}\\}_{n=1}^\\infty$ tienden en probabilidad hacia $\\sigma_{XY}$ y $\\rho_{XY}$, respectivamente:\n$$\nS_{X_n,Y_n}\\stackrel{c.p.}{\\longrightarrow}\\sigma_{XY},\\quad R_{X_nY_n}\\stackrel{c.p.}{\\longrightarrow}\\rho_{XY}.\n$$\n\n\n<div class=\"dem\">\n**Demostración**\n\nPara la demostración basta aplicar la **ley débil de los grandes números** a las sucesiones $\\{S_{X_n,Y_n}\\}_{n=1}^\\infty$ y $\\{R_{X_nY_n}\\}_{n=1}^\\infty$. Dejamos los detalles como ejercicio.\n</div>\n\n### Leyes fuertes de los grandes números\n\nVamos a dar una versión de la ley débil de los grandes números pero en lugar de tener convergencia **en probabilidad**, tendremos convergencia **casi segura**.\n\n\n<l class=\"prop\"> Teorema de Kolmogorov. Ley fuerte de los grandes números. </l>\nSea $\\{X_n\\}_{n=1}^\\infty$ una sucesión de variables aleatorias independientes y con varianza $\\sigma_n^2$. Supongamos que la serie\n$\\sum\\limits_{n=1}^\\infty \\frac{\\sigma_n^2}{n^2},$\nes convergente. Entonces la sucesión de las medias muestrales $\\{\\overline{X}_n\\}_{n=1}^\\infty$ cumplen la\nllamada **ley fuerte de los grandes números**:\n$$\n\\overline{X}_n-\\frac{1}{n}\\sum_{i=1}^n \\mu_i \\stackrel{c.s.}{\\longrightarrow} 0.\n$$\n\nAsociados al resultado anterior tenemos los corolarios siguientes:\n\n<l class=\"prop\"> Corolario. </l>\nSea $\\{X_n\\}_{n=1}^\\infty$ una sucesión de variables aleatorias independientes y con varianza $\\sigma_n^2$. Supongamos que existe una constante $M$ tal que todas las varianzas están acotadas por $M$: $\\sigma_n^2\\leq M$, para todo $n$.\nEntonces la sucesión de las medias muestrales $\\{\\overline{X}_n\\}_{n=1}^\\infty$ cumplen la\nllamada **ley fuerte de los grandes números**:\n$$\n\\overline{X}_n-\\frac{1}{n}\\sum_{i=1}^n \\mu_i \\stackrel{c.s.}{\\longrightarrow} 0.\n$$\n\n<div class=\"dem\">\n**Demostración**\n\nSi $\\sigma_n^2\\leq M$ para todo $n$, la serie numérica $\\sum\\limits_{n=1}^\\infty \\frac{\\sigma_n^2}{n^2}$ será convergente ya que, por el criterio de acotación,\n$$\n\\sum\\limits_{n=1}^\\infty \\frac{\\sigma_n^2}{n^2}\\leq M\\sum\\limits_{n=1}^\\infty \\frac{1}{n^2},\n$$\nque es convergente.\n\nEntonces aplicando el **Teorema de Kolmogorov** o la **ley fuerte de los grandes números**, tenemos el resultado.\n\n</div>\n\n<l class=\"prop\"> Corolario. </l>\nSea $\\{X_n\\}_{n=1}^\\infty$ una sucesión de variables aleatorias independientes de Bernoulli con el mismo parámetro $p$ que es lo mismo que decir que son idénticamente distribuidas.\nEntonces la sucesión de las medias muestrales convergen **casi seguramente** hacia $p$:\n$$\n\\overline{X}_n \\stackrel{c.s.}{\\longrightarrow} p.\n$$\n\n<div class=\"dem\">\n**Demostración**\n\nEn este caso: \n$$\n\\frac{1}{n}\\sum_{i=1}^n \\mu_i = \\frac{1}{n}\\cdot n\\cdot p=p.\n$$\nTambién se verifica que $\\sigma_n^2 =p(1-p)$. Por tanto, existe una constante $M$ ($M=p(1-p)$) tal que $\\sigma_n^2\\leq M$. Aplicando el Corolario anterior, obtenemos el resultado.\n\n</div>\n\n\n<div class=\"example\">\n**Ejemplo: lanzamiento moneda (continuación)**\n\nVamos a repetir el ejemplo de las variables aleatorias de Bernoulli $X_n$, todas de parámetro $p=\\frac{1}{2}$ y comprobar que las proporciones de caras cuando lanzamos la moneda $n$ veces, es decir, las medias muestrales $\\overline{X}_n$ tienden **casi seguramente** hacia $p=\\frac{1}{2}$.\n\n\n<div class=\"example-sol\">\nLa comprobación anterior es equivalente a ver que la serie:\n$$\n\\sum_{n=1}^\\infty P(|\\overline{X}_n-p|>\\epsilon),\n$$\nes convergente fijado $\\epsilon >0$.\n\nRecordemos que en la variable `probabilidades.pn` calculábamos las probabilidades $P(|\\overline{X}_n-p|>\\epsilon)$ para un $\\epsilon =0.1$. \n\nComprobar que la serie anterior es convergente es equivalente a comprobar que las sumas parciales convergen:\n\n```{r,eval=FALSE}\ncumsum(probabilidades.pn)\n```\n\nEl problema es que la `n` y la `N` escogidas son demasiado pequeñas. Para realizar el experimento actual tenéis que considerar `n=1000` y `N=5000`. Id con cuidado que el programa os tardará un rato.\n\n```{r,echo=FALSE}\nprobabilidades.pn = read.table(\"book-files/Probabilidadesn.txt\")\nprobabilidades.pn = t(probabilidades.pn)\n```\n\nEl gráfico de las sumas parciales se muestra a continuación:\n\n<div class=\"center\">\n```{r}\nN=1000\nplot(1:N,cumsum(probabilidades.pn),xlab=expression(n),\n     ylab=\"Sumas parciales\",col='red', type='l')\n```\n</div>\n\nComo se puede observar, la serie parece que converge.\n\n\n</div>\n</div>\n\n## Teorema Central del Límite\n\nSabemos que si una sucesión $\\{X_n\\}$ está formada por variables normales, la sucesión de medias muestrales $\\left\\{\\overline{X}_n=\\frac{\\sum\\limits_{i=1}^n X_i}{n}\\right\\}_{n=1}^\\infty$ también son normales ya que vimos en el tema de variables multidimensionales que si aplicamos una transformación afín (y, en particular, lineal) a una variable normal multidimensional, el resultado es una normal.\n\nPara calcular la variable $\\overline{X}_n$, es obvio que la transformación lineal es la siguiente:\n$$\n\\overline{X}_n = \\left(\\frac{1}{n},\\ldots,\\frac{1}{n}\\right)\\cdot \\begin{pmatrix}X_1 \\\\\\vdots\\\\ X_n\\end{pmatrix}.\n$$\nSi además la sucesión de variables $X_n$ son normales todas con media $\\mu$ y varianza $\\sigma^2$, la sucesión $\\left\\{\\overline{X}_n\\right\\}_{n=1}^\\infty$ serán normales de media $\\mu$ y varianza $\\frac{\\sigma^2}{n}$.\n\nEstandarizando las variables anteriores, podemos concluir que las variables medias estandarizadas $Z_n =\\left\\{\\frac{\\overline{X}_n-\\mu}{\\frac{\\sigma}{\\sqrt{n}}}\\right\\}_{n=1}^\\infty$ todas son $N(0,1)$.\n\nEl **Teorema Central del Límite** generaliza el resultado anterior en el sentido de que si las variables $X_n$ no tienen por qué tener la distribución normal pero son independientes e idénticamente distribuidas, las variables $Z_n$ correspondientes tienden **en ley** a una distribución normal estándar $N(0,1)$.\n\nEn general, se dice que los valores medios de cualquier secuencia de números  aproximadamente corresponde a una muestra de una normal.\n\n### Teorema Central del Límite\n<l class=\"prop\"> Teorema Central del Límite </l>\nSea $\\{X_n\\}_{n=1}^\\infty$ una sucesión de variables aleatorias independientes e idénticamente distribuidas con $E(X_n)=\\mu$ y $\\mathrm{Var}(X_n)=\\sigma^2$ para todo $n$. Entonces:\n$$\n\\frac{\\sum\\limits_{i=1}^n X_i-n\\mu}{\\sigma\\sqrt{n}}\\stackrel{{\\cal L}}{\\longrightarrow} N(0,1).\n$$\n\n<l class=\"observ\"> Observación. </l>\nUna condición equivalente a la tesis del **Teorema Central del Límite** es:\n$$\n\\frac{\\overline{X}_n-\\mu}{\\frac{\\sigma}{\\sqrt{n}}}\\stackrel{{\\cal L}}{\\longrightarrow} N(0,1).\n$$\nBasta dividir por $n$ el numerador y el denominador de la tesis original del **Teorema Central del Límite**.\n\nPara la demostración, usaremos dos propiedades de la función característica:\n\n<l class=\"prop\"> Proposición. </l>\nSea $Y_1,\\ldots, Y_n$ $n$ variables aleatorias independientes. Sea $S_n$ la variable aleatoria suma de las variables anteriores, $S_n=\\sum\\limits_{i=1}^n Y_i$. Entonces, para calcular $\\phi_{S_n}$, podemos usar la expresión siguiente::\n$$\n\\phi_{S_n}(w)=\\phi_{Y_1}(w)\\cdots \\phi_{Y_n}(w),\n$$\ndonde $w$ es cualquier valor real.\n\n\n<div class=\"dem\">\n**Demostración de la proposición**\n\nPor definición:\n$$\n\\begin{array}{rl}\n\\phi_{S_n}(w) & =E\\left(\\mathrm{e}^{\\mathrm{i} w S_n}\\right)=E\\left(\\mathrm{e}^{\\mathrm{i} w \\sum\\limits_{i=1}^n Y_i}\\right) = E\\left(\\mathrm{e}^{i w Y_1}\\cdots \\mathrm{e}^{i w Y_n}\\right)\\\\ & \\stackrel{\\mbox{$Y_1,\\ldots,Y_n$ son independientes}}{=} E\\left(\\mathrm{e}^{i w Y_1}\\right)\\cdots E\\left(\\mathrm{e}^{i w Y_n}\\right) \\\\\n& =\\phi_{Y_1}(w)\\cdots \\phi_{Y_n}(w).\n\\end{array}\n$$\n</div>\n\n<l class=\"prop\"> Proposición. </l>\nSea $Y$ una variable aleatoria. Sea $U=kY$ la variable aleatoria $Y$ multiplicada por un valor real $k$. Entonces, para calcular $\\phi_{U}$, podemos usar la expresión siguiente:\n$$\n\\phi_{U}(w)=\\phi_Y(kw),\n$$\ndonde $w$ es cualquier valor real.\n\n\n<div class=\"dem\">\n**Demostración de la proposición**\n\nPor definición:\n$$\n\\phi_{U}(w)=E\\left(\\mathrm{e}^{\\mathrm{i} w U}\\right) = E\\left(\\mathrm{e}^{\\mathrm{i} w k Y}\\right)=\\phi_Y(kw).\n$$\n</div>\n\n<div class=\"dem\">\n**Demostración del Teorema Central del Límite**\n\nUsando la proposición que vimos al introducir la **convergencia en ley** que dice que una sucesión $\\{X_n\\}$ converge **en ley** hacia $X$ si, y sólo si, $\\lim\\limits_{\\phi_{X_n}(t)}=\\phi_{X}(t)$, donde $\\phi$ representa la función característica y la condición anterior tiene que verificarse para todo valor $t\\in\\mathbb{R}$, basta demostrar que, si tomamos\n$Z_n = \\frac{\\sum\\limits_{i=1}^n X_i-n\\mu}{\\sigma\\sqrt{n}},$\n\n$$\n\\lim_{n\\to \\infty}\\phi_{Z_n}(w)=\\phi_Z(w),\n$$\npara cualquier valor $w\\in\\mathbb{R}$, siendo $Z=N(0,1)$.\n\n\nSeguidamente, simplifiquemos la expresión $\\phi_{Z_n}(w)=\\phi_{\\frac{\\sum\\limits_{i=1}^n X_i-n\\mu}{\\sigma\\sqrt{n}}}(w)$ usando las dos proposiciones anteriores. En primer lugar, teniendo en cuenta que las variables $\\left\\{\\frac{X_i-\\mu}{\\sigma\\sqrt{n}}\\right\\}$ son independientes e idénticamente distribuidas, usando la primera proposición podemos escribir:\n$$\n\\phi_{Z_n}(w) = \\left(\\phi_{\\frac{X-\\mu}{\\sigma\\sqrt{n}}}(w)\\right)^n,\n$$\ndonde $X$ representa cualquiera de las variables $X_i$.\n\nUsando la segunda proposición, podemos simplificar la expresión anterior aún más:\n$$\n\\phi_{Z_n}(w) = \\left(\\phi_{\\frac{X-\\mu}{\\sigma\\sqrt{n}}}(w)\\right)^n = \\left(\\phi_{X-\\mu}\\left(\\frac{w}{\\sigma\\sqrt{n}}\\right)\\right)^n.\n$$\n\nSi desarrollamos por Taylor alrededor del valor $\\hat{w}=0$ la función característica $\\phi_{X-\\mu}\\left(\\hat{w}\\right)$ hasta segundo orden, obtenemos:\n$$\n\\scriptsize{\n\\phi_{X-\\mu}\\left(\\hat{w}\\right) = \\phi_{X-\\mu}\\left(0\\right)+ \\phi_{X-\\mu}'\\left(0\\right) \\hat{w}+ \\phi_{X-\\mu}''\\left(0\\right)\\frac{\\hat{w}^2}{2}+O(\\hat{w}^3),\n}\n$$\ndonde $O(\\hat{w}^3)$ simboliza los términos de orden $\\hat{w}^3$ y superiores.\n\nLos valores $\\phi_{X-\\mu}\\left(0\\right)$, $\\phi_{X-\\mu}'\\left(0\\right)$ y $\\phi_{X-\\mu}''\\left(0\\right)$ valen: (ver tema de Complementos de variables aleatorias)\n\n$$\n\\scriptsize{\n\\phi_{X-\\mu}\\left(0\\right)=1, \\ \\phi_{X-\\mu}'\\left(0\\right)=\\frac{1}{\\mathrm{i}}E(X-\\mu)=0,\\ \\phi_{X-\\mu}''\\left(0\\right)=\\frac{1}{\\mathrm{i}^2}E\\left((X-\\mu)^2\\right)=-\\sigma^2.\n}\n$$\nEl desarrollo anterior será:\n$$\n\\phi_{X-\\mu}\\left(\\hat{w}\\right) =1 - \\frac{1}{2}\\hat{w}^2\\sigma^2+O(\\hat{w}^3),\n$$\n\nAplicando la expresión anterior para $\\hat{w}=\\frac{w}{\\sigma\\sqrt{n}}$, obtenemos:\n\n$$\n\\scriptsize{\n\\phi_{X-\\mu}\\left(\\frac{w}{\\sigma\\sqrt{n}}\\right) =1 - \\frac{1}{2}\\left(\\frac{w}{\\sigma\\sqrt{n}}\\right)^2\\sigma^2+O\\left(\\frac{w}{\\sigma\\sqrt{n}}\\right)^3= 1-\\frac{w^2}{2n}+O\\left(\\frac{w^3}{n^{\\frac{3}{2}}}\\right),\n}\n$$\n\nLa función característica de la variable $Z_n$ será usando la expresión anterior:\n$$\n\\scriptsize{\\phi_{Z_n}(w)=\\left(\\phi_{X-\\mu}\\left(\\frac{w}{\\sigma\\sqrt{n}}\\right)\\right)^n = \\left(1-\\frac{w^2}{2n}+O\\left(\\frac{w^3}{n^{\\frac{3}{2}}}\\right)\\right)^n}\n$$\nEl objetivo es calcular el límite de la expresión anterior:\n$$\n\\lim_{n\\to \\infty}\\phi_{Z_n}(w) = \\lim_{n\\to\\infty} \\left(1-\\frac{w^2}{2n}+O\\left(\\frac{w^3}{n^{\\frac{3}{2}}}\\right)\\right)^n = \n\\lim_{n\\to \\infty}\\mathrm{e}^{n\\cdot \\ln \\left(1-\\frac{w^2}{2n}+O\\left(\\frac{w^3}{n^{\\frac{3}{2}}}\\right)\\right)}.\n$$\n\nUsando que para $z\\approx 0$, $\\ln(1-z)=z+O(z^2)$, el límite anterior será:\n$$\n\\lim_{n\\to \\infty}\\phi_{Z_n}(w) = \n\\lim_{n\\to \\infty}\\mathrm{e}^{n\\cdot \\left(-\\frac{w^2}{2n}+O\\left(\\frac{w^4}{n^{2}}\\right)\\right)} = \\lim_{n\\to \\infty}\\mathrm{e}^{ \\left(-\\frac{w^2}{2}+O\\left(\\frac{w^4}{n}\\right)\\right)} = \\mathrm{e}^{-\\frac{w^2}{2}},\n$$\ny dicha expresión coincide con la función característica de la variable $N(0,1)$, $\\phi_{Z}(w)$.\n\nRecordad que en el tema de Complementos de variables aleatorias vimos que si la variable $U$ era $N(\\mu,\\sigma)$, $\\phi_{U}(w)=\\mathrm{e}^{\\mathrm{i}w\\mu-\\frac{w^2\\sigma^2}{2}}$. Aplicando la fórmula anterior para $\\mu=0$ y $\\sigma=1$, obtenemos $\\phi_{Z}(w)=\\mathrm{e}^{-\\frac{w^2}{2}}.$\n</div>\n\n### Teorema Central del Límite en la práctica\nEl **Teorema Central del Límite** se aplica a la práctica en la forma siguiente:\n\nSea $\\{X_n\\}_{n=1}^\\infty$ una sucesión de variables aleatorias independientes e idénticamente distribuidas con $E(X_i)=\\mu$ y $\\mathrm{Var}(X_i)=\\sigma^2$. Entonces, podemos aproximar para $n$ grande ($n\\geq 30$), la media muestral $\\overline{X}_n$ por:\n$$\n\\overline{X}_n =\\frac{1}{n}\\sum_{i=1}^n X_i \\approx N\\left(\\mu,\\frac{\\sigma}{\\sqrt{n}}\\right),\n$$\no también:\n$$\n\\sum_{i=1}^n X_i \\approx N\\left(n\\mu,\\sigma\\sqrt{n}\\right),\n$$\n\nLas aproximaciones anteriores se pueden obtener teniendo en cuenta que el **Teorema Central del Límite** nos dice que la variable \n$Z_n= \\frac{\\sum\\limits_{i=1}^n X_i-n\\mu}{\\sigma\\sqrt{n}}$ es aproximadamente una $N(0,1)$. Por tanto,\n$$\n\\frac{\\sum\\limits_{i=1}^n X_i-n\\mu}{\\sigma\\sqrt{n}} \\approx N(0,1),\\ \\Rightarrow \\sum_{i=1}^n X_i\\approx \\sigma\\sqrt{n}\\cdot N(0,1)+n\\mu = N\\left(n\\mu,\\sigma\\sqrt{n}\\right).\n$$\n\nDividiendo por $n$ la aproximación anterior, obtenemos:\n$$\n\\overline{X}_n =\\frac{1}{n}\\sum_{i=1}^n X_i \\approx \\frac{1}{n}N\\left(n\\mu,\\sigma\\sqrt{n}\\right) =N\\left(\\mu,\\frac{\\sigma}{\\sqrt{n}}\\right).\n$$\n\n\n### Teorema de Moivre-Laplace\nSi aplicamos el **Teorema Central del Límite** en el caso en que las variables $X_n$ son de Bernoulli de parámetro $p$, obtenemos el llamado **Teorema de Moivre-Laplace**:\n\n<l class=\"prop\"> Teorema de Moivre-Laplace </l>\nSea $\\{X_n\\}_{n=1}^\\infty$ una sucesión de variables aleatorias independientes Bernoulli de parámetro $p$. La variable $\\sum\\limits_{i=1}^n X_i$ será binomial de parámetros $n$ y $p$, $B(n,p)$. Entonces:\n$$\n\\frac{B(n,p)-np}{\\sqrt{n\\cdot p\\cdot (1-p)}}\\stackrel{{\\cal L}}{\\longrightarrow} N(0,1).\n$$\n\nEn la práctica, decimos que podemos aproximar una variable binomial de parámetros $n$ y $p$ por una distribución normal de parámetros $\\mu=np$ y $\\sigma =\\sqrt{n\\cdot p\\cdot (1-p)}$:\n$$\nB(n,p)\\approx N(np,\\sqrt{n\\cdot p\\cdot (1-p)}).\n$$\n\n### Aproximación de una suma de variables Poisson\n\nSi aplicamos el **Teorema Central del Límite** en el caso en que las variables $X_n$ son de Poisson de parámetro $\\lambda$, obtenemos el resultado siguiente:\n\n<l class=\"prop\"> Proposición. </l>\nSea $\\{X_n\\}_{n=1}^\\infty$ una sucesión de variables aleatorias independientes Poisson de parámetro $\\lambda$. Entonces:\n$$\n\\frac{\\sum\\limits_{i=1}^n X_i -n\\lambda}{\\sqrt{n\\cdot \\lambda}}\\stackrel{{\\cal L}}{\\longrightarrow} N(0,1).\n$$\n\nAntes de ver la aplicación práctica del resultado anterior, veamos que suma de variables Poisson independientes de parámetro $\\lambda$ es una variable Poisson de parámetro $n\\lambda$:\n\n<l class=\"prop\"> Proposición. </l>\nSea $\\{X_n\\}_{n=1}^\\infty$ una sucesión de variables aleatorias independientes Poisson de parámetro $\\lambda$. Entonces la variable $\\sum\\limits_{i=1}^n X_i$ sigue la distribución de Poisson de parámetro $n\\lambda$.\n\n<div class=\"dem\">\n**Demostración**\n\nEn primer lugar, hallemos la función característica de la distribución de Poisson de parámetro $\\lambda$. Sea $X=Poiss(\\lambda)$. Su función característica en un valor $w$ será:\n$$\n\\scriptsize{\n\\phi_X(w)=E\\left(\\mathrm{e}^{\\mathrm{i} w X}\\right)=\\sum_{k=0}^\\infty \\mathrm{e}^{i w k}\\frac{\\lambda^k}{k!}\\mathrm{e}^{-\\lambda}=\\mathrm{e}^{-\\lambda} \\sum_{k=0}^\\infty \\frac{\\left(\\lambda\\mathrm{e}^{iw}\\right)^k}{k!}=\\mathrm{e}^{-\\lambda}\\cdot \\mathrm{e}^{\\lambda\\mathrm{e}^{iw}}=\\mathrm{e}^{\\lambda \\left(\\mathrm{e}^{iw}-1\\right)}.\n}\n$$\nSea ahora la variable $S_n=\\sum\\limits_{i=1}^n X_i$. Usando la proposición anterior que nos calcula la función característica de sumas de variables independientes, podemos escribir:\n$$\n\\phi_{S_n}(w)=\\phi_{X_1}(w)\\cdots \\phi_{X_n}(w)=\\left(\\mathrm{e}^{\\lambda \\left(\\mathrm{e}^{iw}-1\\right)}\\right)^n =\\mathrm{e}^{n\\lambda \\left(\\mathrm{e}^{iw}-1\\right)},\n$$\nfunción característica que corresponde a una variable de Poisson de parámetro $n\\lambda$, $Poiss(n\\lambda)$.\n</div>\n\nUsando la proposición anterior, tenemos que la suma de variables Poisson independientes de parámetro $\\lambda$ sigue una distribución Poisson de parámetro $n\\lambda$. Por tanto, podemos escribir usando el corolario del **Teorema Central del Límite** aplicado a variables Poisson:\n$$\nPoiss(n\\lambda)\\approx N(n\\lambda,\\sqrt{n\\lambda}).\n$$\n\n\n<div class=\"example\">\n**Ejemplo: aplicación del Teorema de Moivre-Laplace**\n\nSea $X$ una distribución binomial de parámetros $n=50$ y $p=\\frac{1}{3}$. \n\nQueremos conocer $P(X < 15)$ y $P(10\\leq X\\leq 20)$.\n\nVamos a calcular las probabilidades anteriores usando el **Teorema de Moivre-Laplace**. \n\n<div class=\"example-sol\">\n\nLa variable $X$ es aproximadamente una distribución normal $X_N$ de parámetros $\\mu = np=\\frac{50}{3}=`r round(50/3,4)`$ y $\\sigma=\\sqrt{50\\cdot\\frac{1}{3}\\cdot \\frac{2}{3}}=`r round(sqrt(50*2/9),4)`$. \n\nPor tanto:\n$$\n\\begin{array}{rl}\nP(X< 15) \n& = P(X\\leq 14) \\approx P(X_N \\leq 14)=P\\left(Z\\leq \\frac{14-`r round(50/3,4)`}{`r round(sqrt(50*2/9),4)`}\\right)\\\\& \n=P(Z\\leq `r round((14-50/3)/sqrt(50*2/9),4)`) = `r round(pnorm(14,50/3,sqrt(50*2/9)),4)`,\\\\\n\\end{array}\n$$\n$$\n\\begin{array}{rl}\nP(10\\leq X\\leq 20) \n& \\approx P(10\\leq X_N \\leq 20) = P\\left(\\frac{10-`r round(50/3,4)`}{`r round(sqrt(50*2/9),4)`}\\leq  Z\\leq \\frac{20-`r round(50/3,4)`}{`r round(sqrt(50*2/9),4)`}\\right) \\\\ \n& = P(`r round((10-50/3)/sqrt(50*2/9),4)`\\leq Z\\leq `r round((20-50/3)/sqrt(50*2/9),4)`) \\\\ \n& = P(Z\\leq `r round((20-50/3)/sqrt(50*2/9),4)`)-P(Z\\leq `r round((10-50/3)/sqrt(50*2/9),4)`)=`r pnorm(20,50/3,sqrt(50*2/9),4)`-`r pnorm(10,50/3,sqrt(50*2/9),4)` = `r round(pnorm(20,50/3,sqrt(50*2/9))-pnorm(10,50/3,sqrt(50*2/9)),4)`,\n\\end{array}\n$$\ndonde $Z=N(0,1)$.\n\n\nComparemos los valores aproximados anteriores con los valores \"exactos\" proporcionados por `R`:\n```{r}\npbinom(14,50,1/3)\npbinom(20,50,1/3)-pbinom(9,50,1/3)\n```\nTenemos errores de `r round(abs(pbinom(14,50,1/3)-pnorm(14,50/3,sqrt(50*2/9))),5)` y `r round(pbinom(20,50,1/3)-pbinom(9,50,1/3)-(pnorm(20,50/3,sqrt(50*2/9))-pnorm(10,50/3,sqrt(50*2/9))),5)`, respectivamente.\n\nAunque $n$ no es pequeño, $n=50$, los errores anteriores no son demasiado pequeños.\n\nUna razón por la que dichos errores no son pequeños es que aproximamos una distribución discreta (Binomial) cuyos valores van de 1 en 1 por una distribución normal, que es continua.\n\nLa corrección de continuidad de Fisher nos mejora la aproximación disminuyendo dichos errores.\n\n</div>\n\n### Corrección de continuidad de Fisher\nCuando aplicamos el **Teorema Central del Límite** y aproximamos una distribución discreta que tiene valores enteros por una normal, hemos de aplicar lo que se llama **corrección de continuidad de Fisher**. \n\nSea $X$ la variable discreta que queremos aproximar y $X_N$ la variable normal que nos aparece cuando aplicamos el **Teorema Central del Límite**. Supongamos que queremos calcular $P(X\\leq k)$, para un $k$ entero. Entonces debemos hacer:\n$$\nP(X\\leq k)\\approx P(X_N\\leq k+0.5).\n$$\n\nEs decir, para tener en cuenta el valor $k$ en la aproximación $X_N$ hay que sumarle la mitad entre dos valores consecutivos (0.5 si los valores son enteros) de la variable $X$.\n\nId con cuidado, si queremos calcular $P(X<k)$, hay que hacer $P(X<k) =P(X\\leq k-1)\\approx P(X_N \\leq k-1+0.5)=P(X_N\\leq k-0.5)$.\n\n<div class=\"example\">\n**Ejemplo: continuación ejemplo anterior** \n\nSi aplicamos la continuidad de Fisher en el ejemplo anterior, obtenemos:\n<div class=\"example-sol\">\n$$\n\\begin{array}{rl}\nP(X< 15) & = P(X\\leq 14) \\approx P(X_N \\leq 14.5)=P\\left(Z\\leq \\frac{14.5-`r round(50/3,4)`}{`r round(sqrt(50*2/9),4)`}\\right) =P(Z\\leq `r round((14.5-50/3)/sqrt(50*2/9),4)`) = `r round(pnorm(14.5,50/3,sqrt(50*2/9)),4)`,\n\\end{array}\n$$\n\n$$\n\\begin{array}{rl}\nP(10\\leq X\\leq 20) &= P(X\\leq 20)-P(X\\leq 9)\\approx P(X_N \\leq 20.5)-P(X_N\\leq 9.5) \\\\ & = P\\left(Z\\leq \\frac{20.5-`r round(50/3,4)`}{`r round(sqrt(50*2/9),4)`}\\right) - P\\left(Z\\leq \\frac{9.5-`r round(50/3,4)`}{`r round(sqrt(50*2/9),4)`}\\right)=  P(Z\\leq `r round((20.5-50/3)/sqrt(50*2/9),4)`)-P(Z\\leq `r round((9.5-50/3)/sqrt(50*2/9),4)`)\\\\ & =`r pnorm(20.5,50/3,sqrt(50*2/9),4)`-`r pnorm(9.5,50/3,sqrt(50*2/9),4)` = `r round(pnorm(20.5,50/3,sqrt(50*2/9))-pnorm(9.5,50/3,sqrt(50*2/9)),4)`,\n\\end{array}\n$$\nobteniendo unos errores de sólo `r round(abs(pbinom(14,50,1/3)-pnorm(14.5,50/3,sqrt(50*2/9))),5)` y `r round(pbinom(20,50,1/3)-pbinom(9,50,1/3)-(pnorm(20.5,50/3,sqrt(50*2/9))-pnorm(9.5,50/3,sqrt(50*2/9))),5)`, respectivamente.\n</div>\n\n### Simulación del Teorema Central del Límite\n\n<div class=\"example\">\n**Ejemplo de simulación de la aproximación de una variable binomial a una distribución normal**\n\nPara realizar la simulación anterior, consideremos una distribución binomial de parámetros $n=100$ y $p=\\frac{1}{2}$.\n\nSegún el **Teorema Central del Límite**, tenemos que \n$$\n\\overline{X}_n=\\frac{1}{n}B\\left(n=100,p=\\frac{1}{2}\\right)\\approx N\\left(\\mu = p=\\frac{1}{2}=`r round(1/2,4)`,\\sigma=\\sqrt{\\frac{\\frac{1}{2}\\cdot \\frac{1}{2}}{100}}=`r round(sqrt(1/(400)),4)`\\right).\n$$\n\nPara ver dicha aproximación, en primer lugar vamos a generar una muestra de $N=1000$ valores de una binomial de parámetros $n=100$ y $p=\\frac{1}{2}$ y dividiendo por $n=100$, tenemos una muestra de $\\overline{X}_n$:\n```{r}\nn=100\np=1/2\nsigma=p*(1-p)\nset.seed(2019)\nmuestra.binomial = rbinom(1000,n,p)\nmuestra.xnbarra = muestra.binomial/n\n```\n\nPara ver si la aproximación funciona, dibujaremos en una misma gráfica el histograma de frecuencias relativas de la muestra anterior y la curva de la función de densidad de la distribución normal de parámetros $\\mu =\\frac{1}{2}$ y $\\sigma = `r round(sqrt(1/(400)),4)`$:\n\n```{r}\nhist(muestra.xnbarra,freq=FALSE,\n     breaks=seq(from=min(muestra.xnbarra)-0.1,\n                to=max(muestra.xnbarra)+0.1,by=0.01),\n     main=\"Histograma de la distribución de las medias muestrales\",\n     xlab=\"valores variable\",ylab=\"frecuencias relativas\")\nmu=p\nsigma.xnbarra=sqrt(p*(1-p)/n)\nx=seq(from=min(muestra.xnbarra),to=max(muestra.xnbarra),by=0.01)\nlines(x,dnorm(x,mu,sigma.xnbarra),col='red')\n```\n\nObservamos que la aproximación es bastante buena.\n</div>\n","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","output-file":"7.html"},"language":{"toc-title-document":"Tabla de contenidos","toc-title-website":"En esta página","related-formats-title":"Otros formatos","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Fuente","other-links-title":"Otros Enlaces","article-notebook-label":"Cuaderno de Artículo","notebook-preview-download":"Descargar Cuaderno","notebook-preview-download-src":"Descargar código fuente","notebook-preview-back":"Volver al Artículo","manuscript-meca-bundle":"Archivo MECA","section-title-abstract":"Resumen","section-title-appendices":"Apéndices","section-title-footnotes":"Notas","section-title-references":"Referencias","section-title-reuse":"Reutilización","section-title-copyright":"Derechos de autor","section-title-citation":"Cómo citar","appendix-attribution-cite-as":"Por favor, cita este trabajo como:","appendix-attribution-bibtex":"BibTeX","title-block-author-single":"Autor/a","title-block-author-plural":"Autores/as","title-block-affiliation-single":"Afiliación","title-block-affiliation-plural":"Afiliaciones","title-block-published":"Fecha de publicación","title-block-modified":"Fecha de modificación","title-block-keywords":"Palabras clave","callout-tip-title":"Tip","callout-note-title":"Nota","callout-warning-title":"Advertencia","callout-important-title":"Importante","callout-caution-title":"Precaución","code-summary":"Código","code-tools-menu-caption":"Código","code-tools-show-all-code":"Mostrar todo el código","code-tools-hide-all-code":"Ocultar todo el código","code-tools-view-source":"Ver el código fuente","code-tools-source-code":"Ejecutar el código","code-line":"Línea","code-lines":"Líneas","copy-button-tooltip":"Copiar al portapapeles","copy-button-tooltip-success":"Copiado","repo-action-links-edit":"Editar esta página","repo-action-links-source":"Ver el código","repo-action-links-issue":"Informar de un problema","back-to-top":"Volver arriba","search-no-results-text":"Sin resultados","search-matching-documents-text":"documentos encontrados","search-copy-link-title":"Copiar el enlace en la búsqueda","search-hide-matches-text":"Ocultar resultados adicionales","search-more-match-text":"resultado adicional en este documento","search-more-matches-text":"resultados adicionales en este documento","search-clear-button-title":"Borrar","search-detached-cancel-button-title":"Cancelar","search-submit-button-title":"Enviar","search-label":"Buscar","toggle-section":"Alternar sección","toggle-sidebar":"Alternar barra lateral","toggle-dark-mode":"Alternar modo oscuro","toggle-reader-mode":"Alternar modo lector","toggle-navigation":"Navegación de palanca","crossref-fig-title":"Figura","crossref-tbl-title":"Tabla","crossref-lst-title":"Listado","crossref-thm-title":"Teorema","crossref-lem-title":"Lema","crossref-cor-title":"Corolario","crossref-prp-title":"Proposición","crossref-cnj-title":"Conjetura","crossref-def-title":"Definición","crossref-exm-title":"Ejemplo","crossref-exr-title":"Ejercicio","crossref-ch-prefix":"Capítulo","crossref-apx-prefix":"Apéndice","crossref-sec-prefix":"Sección","crossref-eq-prefix":"Ecuación","crossref-lof-title":"Listado de Figuras","crossref-lot-title":"Listado de Tablas","crossref-lol-title":"Listado de Listados","environment-proof-title":"Prueba","environment-remark-title":"Observación","environment-solution-title":"Solución","listing-page-order-by":"Ordenar por","listing-page-order-by-default":"Por defecto","listing-page-order-by-date-asc":"Menos reciente","listing-page-order-by-date-desc":"Más reciente","listing-page-order-by-number-desc":"De mayor a menor","listing-page-order-by-number-asc":"De menor a mayor","listing-page-field-date":"Fecha","listing-page-field-title":"Título","listing-page-field-description":"Descripción","listing-page-field-author":"Autor/a","listing-page-field-filename":"Nombre de archivo","listing-page-field-filemodified":"Fecha de modificación","listing-page-field-subtitle":"Subtítulo","listing-page-field-readingtime":"Tiempo de lectura","listing-page-field-categories":"Categorías","listing-page-minutes-compact":"{0} minutos","listing-page-category-all":"Todas","listing-page-no-matches":"No hay resultados"},"metadata":{"lang":"es","fig-responsive":true,"quarto-version":"1.4.245","bibliography":["references.bib"],"theme":"cosmo"},"extensions":{"book":{"multiFile":true}}},"pdf":{"identifier":{"display-name":"PDF","target-format":"pdf","base-format":"pdf"},"execute":{"fig-width":5.5,"fig-height":3.5,"fig-format":"pdf","fig-dpi":300,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"pdf","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"pdf-engine":"xelatex","standalone":true,"variables":{"graphics":true,"tables":true},"default-image-extension":"pdf","to":"pdf","output-file":"7.pdf"},"language":{"toc-title-document":"Tabla de contenidos","toc-title-website":"En esta página","related-formats-title":"Otros formatos","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Fuente","other-links-title":"Otros Enlaces","article-notebook-label":"Cuaderno de Artículo","notebook-preview-download":"Descargar Cuaderno","notebook-preview-download-src":"Descargar código fuente","notebook-preview-back":"Volver al Artículo","manuscript-meca-bundle":"Archivo MECA","section-title-abstract":"Resumen","section-title-appendices":"Apéndices","section-title-footnotes":"Notas","section-title-references":"Referencias","section-title-reuse":"Reutilización","section-title-copyright":"Derechos de autor","section-title-citation":"Cómo citar","appendix-attribution-cite-as":"Por favor, cita este trabajo como:","appendix-attribution-bibtex":"BibTeX","title-block-author-single":"Autor/a","title-block-author-plural":"Autores/as","title-block-affiliation-single":"Afiliación","title-block-affiliation-plural":"Afiliaciones","title-block-published":"Fecha de publicación","title-block-modified":"Fecha de modificación","title-block-keywords":"Palabras clave","callout-tip-title":"Tip","callout-note-title":"Nota","callout-warning-title":"Advertencia","callout-important-title":"Importante","callout-caution-title":"Precaución","code-summary":"Código","code-tools-menu-caption":"Código","code-tools-show-all-code":"Mostrar todo el código","code-tools-hide-all-code":"Ocultar todo el código","code-tools-view-source":"Ver el código fuente","code-tools-source-code":"Ejecutar el código","code-line":"Línea","code-lines":"Líneas","copy-button-tooltip":"Copiar al portapapeles","copy-button-tooltip-success":"Copiado","repo-action-links-edit":"Editar esta página","repo-action-links-source":"Ver el código","repo-action-links-issue":"Informar de un problema","back-to-top":"Volver arriba","search-no-results-text":"Sin resultados","search-matching-documents-text":"documentos encontrados","search-copy-link-title":"Copiar el enlace en la búsqueda","search-hide-matches-text":"Ocultar resultados adicionales","search-more-match-text":"resultado adicional en este documento","search-more-matches-text":"resultados adicionales en este documento","search-clear-button-title":"Borrar","search-detached-cancel-button-title":"Cancelar","search-submit-button-title":"Enviar","search-label":"Buscar","toggle-section":"Alternar sección","toggle-sidebar":"Alternar barra lateral","toggle-dark-mode":"Alternar modo oscuro","toggle-reader-mode":"Alternar modo lector","toggle-navigation":"Navegación de palanca","crossref-fig-title":"Figura","crossref-tbl-title":"Tabla","crossref-lst-title":"Listado","crossref-thm-title":"Teorema","crossref-lem-title":"Lema","crossref-cor-title":"Corolario","crossref-prp-title":"Proposición","crossref-cnj-title":"Conjetura","crossref-def-title":"Definición","crossref-exm-title":"Ejemplo","crossref-exr-title":"Ejercicio","crossref-ch-prefix":"Capítulo","crossref-apx-prefix":"Apéndice","crossref-sec-prefix":"Sección","crossref-eq-prefix":"Ecuación","crossref-lof-title":"Listado de Figuras","crossref-lot-title":"Listado de Tablas","crossref-lol-title":"Listado de Listados","environment-proof-title":"Prueba","environment-remark-title":"Observación","environment-solution-title":"Solución","listing-page-order-by":"Ordenar por","listing-page-order-by-default":"Por defecto","listing-page-order-by-date-asc":"Menos reciente","listing-page-order-by-date-desc":"Más reciente","listing-page-order-by-number-desc":"De mayor a menor","listing-page-order-by-number-asc":"De menor a mayor","listing-page-field-date":"Fecha","listing-page-field-title":"Título","listing-page-field-description":"Descripción","listing-page-field-author":"Autor/a","listing-page-field-filename":"Nombre de archivo","listing-page-field-filemodified":"Fecha de modificación","listing-page-field-subtitle":"Subtítulo","listing-page-field-readingtime":"Tiempo de lectura","listing-page-field-categories":"Categorías","listing-page-minutes-compact":"{0} minutos","listing-page-category-all":"Todas","listing-page-no-matches":"No hay resultados"},"metadata":{"block-headings":true,"bibliography":["references.bib"],"documentclass":"scrreprt","linkcolor":"blue","lang":"es"},"extensions":{"book":{"selfContainedOutput":true}}}},"projectFormats":["html","pdf"]}