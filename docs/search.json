[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introducción a la probabilidad para el análisis de datos",
    "section": "",
    "text": "Prefacio\nHemos escrito estas notas entre varios colaboradores desde lugar donde vive el ferreret."
  },
  {
    "objectID": "0.html#teoría-de-conjuntos",
    "href": "0.html#teoría-de-conjuntos",
    "title": "1  Prerrequisitos: Teoría de conjuntos y combinatoria",
    "section": "1.1 Teoría de conjuntos",
    "text": "1.1 Teoría de conjuntos\n Definición de conjunto \nLa definición de conjunto es una idea o noción primitiva. Es decir, es una idea básica del pensamiento humano: un conjunto es una colección de objetos (números, imágenes, jugadores de fútbol, palabras, colores…).\nLa Teoría de Conjuntos básica es simple y natural y es la que necesitamos para este curso. Por otro lado, la Teoría de Conjuntos matemática es más compleja y presenta varias paradojas como la paradoja de Russell.\nLa idea o noción práctica de conjunto es la de una colección de objetos de un cierto tipo.\nEstas colecciones o conjuntos se pueden definir por:\n\nCompresión: reuniendo los objetos que cumplen una propiedad \\(p\\)\nExtensión: dando una lista exhaustiva de los miembros del conjunto\n\n\n1.1.1 Conjuntos básicos\nLos conjuntos suelen tener un conjunto madre como por ejemplo\n\n\\(\\mathbb{N}=\\{0,1,2,\\ldots\\}\\); los números naturales.\n\\(\\mathbb{Z}=\\{\\ldots,-2,-1,0,1,2,\\ldots\\}\\); los números enteros.\n\\(\\mathbb{Q}=\\left\\{\\frac{p}{q}\\ \\Big|\\ p,q\\in \\mathbb{Z} \\mbox{ y } q \\not= 0\\right\\}\\); los números racionales.\n\\(\\mathbb{R}=\\{\\mbox{Todos los puntos de una recta}\\}\\); los números reales.\n\\(\\mathbb{C}= \\left\\{a+b\\cdot i\\  \\big|\\  a,b\\in \\mathbb{R}\\right\\}\\); los números complejos.\nAlfabeto = \\(\\{a,b,c,\\ldots, A,B,C,\\ldots\\}\\)\nPalabras = \\(\\{paz, guerra, amor, probabilidad,\\ldots\\}\\)\n\nRecordemos que \\(i\\) es la unidad imaginaria que satisface \\(i = +\\sqrt{-1}\\).\n\n\n1.1.2 Características y propiedades básicas de los conjuntos\n\nA cada objeto \\(x\\) de \\(\\Omega\\) lo llamaremos elemento del conjunto \\(\\Omega\\) y diremos que \\(x\\) pertenece a \\(\\Omega\\). Lo denotaremos por \\(x\\in \\Omega\\).\nUn conjunto de un elemento, por ejemplo \\(\\{1\\}\\), recibe el nombre de conjunto elemental (o singleton del inglés).\nSean \\(A\\) y \\(B\\) conjuntos, diremos que \\(A\\) es igual a \\(B\\) si todos los elementos \\(A\\) están en \\(B\\) y todos los elementos de \\(B\\) están en \\(A\\). Por ejemplo, \\(A=\\{1,2,3\\}\\) es igual a \\(B=\\{3,1,2\\}\\).\nSi \\(A\\) es un conjunto tal que si \\(x\\in A\\) entonces \\(x\\in B\\), diremos que \\(A\\) es un subconjunto de o que está contenido en \\(B\\). Lo denotaremos por \\(A\\subseteq B.\\)\nEl conjunto que no tiene elementos se denomina conjunto vacío y se denota por el símbolo \\(\\emptyset\\).\nDado \\(A\\) un conjunto cualquiera, obviamente \\(\\emptyset\\subseteq A.\\)\n\nTomemos como conjunto base \\(\\Omega=\\{1,2,3\\}\\)\n\n\\(\\Omega\\) es un conjunto de cardinal 3, se denota por \\(\\#(\\Omega)=3\\) o por \\(|\\Omega|=3\\)\nEl conjunto \\(\\Omega\\) tiene \\(2^3=8\\) subconjuntos.\n\nEl vacio \\(\\emptyset\\) y los elementales \\(\\{1\\},\\{2\\},\\{3\\}\\).\nLos subconjuntos de dos elementos: \\(\\{1,2\\},\\{1,3\\},\\{2,3\\}\\).\nEl conjunto total de tres elementos \\(\\Omega=\\{1,2,3\\}.\\)\n\n\nDado un conjunto \\(\\Omega\\) podemos construir el conjunto de todas sus partes (todos sus subconjuntos) al que denotamos por \\(\\mathcal{P}(\\Omega)\\). También se denomina de forma directa partes de \\(\\Omega\\).\n Cardinal de las partes de un conjunto \nEl cardinal de las partes de un conjunto es \\(\\#(\\mathcal{P}(\\Omega))=2^{\\#(\\Omega)}.\\)\nPor ejemplo \\(\\#\\left(\\mathcal{P}(\\{1,2,3\\})\\right)=2^{\\#(\\{1,2,3\\})}=2^3=8.\\)\nEfectivamente,\n\\[\\mathcal{P}(\\{1,2,3\\})=\\{\\emptyset,\\{1\\},\\{2\\},\\{3\\},\\{1,2\\},\\{1,3\\},\\{2,3\\},\\{1,2,3\\}\\}.\\]\nDado un subconjunto \\(A\\) de \\(\\Omega\\) podemos construir la función característica de \\(A\\) \\[\\chi_A:\\Omega \\to \\{0,1\\}\\]\ndonde dado un \\(\\omega\\in \\Omega\\)\n\\[\n\\chi_A(\\omega)=\n\\left\\{\n\\begin{array}{ll}\n1 &  \\mbox{si }\\omega \\in A\\\\\n0 &  \\mbox{si }\\omega \\not\\in A\n\\end{array}\n\\right.\n\\]\n\n\n1.1.3 Operaciones con conjuntos\nIntersección.\nSea \\(\\Omega\\) un conjunto y \\(A\\) y \\(B\\) dos subconjuntos de \\(\\Omega\\).\nEl conjunto intersección de \\(A\\) y \\(B\\) es el formado por todos los elementos que pertenecen a \\(A\\) y \\(B\\). Se denota por \\(A\\cap B\\).\nMás formalmente\n\\[\nA\\cap B=\\left\\{x\\in\\Omega \\ \\big|\\  x\\in A \\mbox{ y } x\\in B\\right\\}.\n\\]\nUnión.\nEl conjunto unión de \\(A\\) y \\(B\\) es el formado por todos los elementos que pertenecen a \\(A\\) o que pertenecen a \\(B\\). Se denota por \\(A\\cup B\\).\nMás formalmente\n\\[\nA\\cup B=\\left\\{x\\in\\Omega\\ \\big|\\  x\\in A \\mbox{ o } x\\in B\\right\\}.\n\\]\nDiferencia.\nEl conjunto diferencia de \\(A\\) y \\(B\\) es el formado por todos los elementos que pertenecen a \\(A\\) y no pertenecen a \\(B\\). Se denota por \\(A-B=A-(A\\cap B)\\).\nMás formalmente\n\\[\nA- B=\\left\\{x\\in\\Omega \\ \\big|\\ x\\in A \\mbox{ y } x\\notin B\\right\\}.\n\\]\nComplementario\nEl complementario de un subconjunto \\(A\\) de \\(\\Omega\\) es \\(\\Omega-A\\) y se denota por \\(A^c\\) o \\(\\bar{A}\\).\nMás formalmente\n\\[\nA^c=\\left\\{x\\in\\Omega \\ \\big|\\  x\\not\\in A\\right\\}.\n\\]\n\n\n1.1.4 Más propiedades y definiciones\nSea \\(\\Omega\\) un conjunto y \\(A\\), \\(B\\), \\(C\\) tres subconjuntos de \\(\\Omega\\)\n\nSe dice que dos conjuntos \\(A\\) y \\(B\\) son disjuntos si \\(A\\cap B=\\emptyset.\\)\n\\(\\Omega^c=\\emptyset\\).\n\\(\\emptyset^c=\\Omega\\).\n\\(A\\cup B=B \\cup A\\) , \\(A\\cap B=B\\cap A\\) (propiedad conmutativa).\n\\((A\\cup B) \\cup C = A \\cup( B \\cup C)\\) , \\((A\\cap B) \\cap C = A \\cap( B \\cap C)\\) (propiedad asociativa).\n\\(A\\cup (B\\cap C)=(A\\cup B) \\cap (A\\cup C)\\) , \\(A\\cap (B\\cup C)=(A\\cap B) \\cup (A\\cap C)\\) (propiedad distributiva).\n\\(\\left(A^c\\right)^c=A\\) (doble complementario).\n\\(\\left(A\\cup B\\right)^c=A^c \\cap B^c\\), \\(\\left(A\\cap B\\right)^c=A^c \\cup B^c\\) (leyes de De Morgan).\n\n\n\n1.1.5 Con R, ejemplos\nCon R, los conjuntos de pueden definir como vectores\n\n(Omega = c(1,2,3,4,5,6,7,8,9,10))\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n(A = c(1,2,3,4,5))\n\n[1] 1 2 3 4 5\n\n(B = c(1,4,5))\n\n[1] 1 4 5\n\n(C = c(4,6,7,8))\n\n[1] 4 6 7 8\n\n\n\\(A\\cap B\\)\n\nintersect(A,B)\n\n[1] 1 4 5\n\n\n\\(A\\cup B\\)\n\nunion(A,B)\n\n[1] 1 2 3 4 5\n\n\n\\(B-C\\)\n\nsetdiff(B,C)\n\n[1] 1 5\n\n\n\\(A^c=\\Omega-A\\)\n\nsetdiff(Omega,A)\n\n[1]  6  7  8  9 10\n\n\n\n\n1.1.6 Con Python, ejemplos\n\nOmega = set([1,2,3,4,5,6,7,8,9,10])\nOmega\n\n{1, 2, 3, 4, 5, 6, 7, 8, 9, 10}\n\nA = set([1,2,3,4,5])\nA\n\n{1, 2, 3, 4, 5}\n\nB = set([1,4,5])\nB\n\n{1, 4, 5}\n\nC = set([4,6,7,8])\nC\n\n{8, 4, 6, 7}\n\nA & B   ## intersección (&: and/y)\n\n{1, 4, 5}\n\nA | B   ## unión (|: or/o)\n\n{1, 2, 3, 4, 5}\n\nA - C   ## diferencia \n\n{1, 2, 3, 5}\n\nOmega-C ## complementario\n\n{1, 2, 3, 5, 9, 10}"
  },
  {
    "objectID": "0.html#combinatoria",
    "href": "0.html#combinatoria",
    "title": "1  Prerrequisitos: Teoría de conjuntos y combinatoria",
    "section": "1.2 Combinatoria",
    "text": "1.2 Combinatoria\nLa combinatoria es una rama de la Matemática Discreta que, entre otras cosas, cuenta distintas configuraciones de objetos de un conjunto.\nPor ejemplo, si tenemos un equipo de baloncesto con 7 jugadores ¿cuántos equipos de 5 jugadores distintos podemos formar?\n\n1.2.1 Combinaciones\nNúmero combinatorio o número binomial\nNos da el número de subconjuntos de tamaño \\(k\\) de un conjunto de tamaño \\(n\\). Este número es\n\\[\nC_n^k={n\\choose k} = \\frac{n!}{k!\\cdot (n-k)!}.\n\\]\nRecordemos que \\[\nn!=1\\cdot 2\\cdot 3\\cdots n\n\\]\nEn nuestro caso con 7 jugadores, \\(n=7\\), el número de equipos distintos de \\(k=5\\) es\n\\[\n\\begin{array}{rl}\nC_7^5&={7\\choose 5} = \\frac{7!}{5!\\cdot (7-5)!}=\\frac{7!}{5!\\cdot 2!} \\\\\n&=\\frac{1\\cdot 2\\cdot 3 \\cdot 4\\cdot 5\\cdot 6\\cdot 7}{1\\cdot 2\\cdot 3 \\cdot 4\\cdot 5\\cdot 1\\cdot 2}=\\frac{6\\cdot 7}{2}=\\frac{42}{2}=21.\n\\end{array}\n\\]\nPodemos formar 21 equipos distintos.\n\nEjercicio\nCarga el paquete gtools de R e investiga la función combinations(n, r, v, set, repeats.allowed) para calcular todas las combinaciones anteriores.\n\n\n\n1.2.2 Combinaciones con repetición\nEn combinatoria, las combinaciones con repetición de un conjunto son las distintas formas en que se puede hacer una selección de elementos de un conjunto dado, permitiendo que las selecciones puedan repetirse.\nEl número \\(CR_n^k\\) de multiconjuntos con \\(k\\) elementos escogidos de un conjunto con \\(n\\) elementos satisface:\n\nEs igual al número de combinaciones con repetición de \\(k\\) elementos escogidos de un conjunto con \\(n\\) elementos.\nEs igual al número de formas de repartir \\(k\\) objetos en \\(n\\) grupos.\n\n\\[CR_n^k = \\binom{n+k-1}{k} = \\frac{(n+k-1)!}{k!(n-1)!}.\\]\n\nEjemplo\nVamos a imaginar que vamos a repartir 12 caramelos entre Antonio, Beatriz, Carlos y Dionisio (que representaremos como A, B, C, D). Una posible forma de repartir los caramelos sería: dar 4 caramelos a Antonio, 3 a Beatriz, 2 a Carlos y 3 a Dionisio. Dado que no importa el orden en que se reparten, podemos representar esta selección como ‘AAAABBBCCDDD’.\nOtra forma posible de repartir los caramelos podría ser: dar 1 caramelo a Antonio, ninguno a Beatriz y Carlos, y los 11 restantes se los damos a Dionisio. Esta repartición la representamos como ‘ADDDDDDDDDDD’.\nRecíprocamente, cualquier serie de 12 letras A, B, C, D se corresponde a una forma de repartir los caramelos. Por ejemplo, la serie ‘AAAABBBBBDDD’ corresponde a: Dar 4 caramelos a Antonio, 5 caramelos a Beatriz, ninguno a Carlos y 3 a Dionisio.\nDe esta forma, el número de formas de repartir los caramelos es:\n\\[CR_{4}^{12} = \\binom{4+12-1}{4}\\]\n\n\n\n1.2.3 Variaciones.\nVariaciones\nCon los números \\(\\{1,2,3\\}\\) ¿cuántos números de dos cifras distintas podemos formar sin repetir ninguna cifra?\nLos podemos escribir\n\\[12,13,21,23,31,32\\]\nLuego hay seis casos.\n\n\n1.2.4 Variaciones (sin repetición).\nDenotaremos las variaciones (sin repetición) de \\(k\\) elementos (de orden \\(k\\)) de un conjunto de \\(n\\) elementos por \\(V^n_k\\) su valor es\n\\[\nV_n^k=\\frac{n!}{(n-k)!}=(n-k+1)\\cdot (n-k+2)\\cdots n.\n\\]\nEn nuestro ejemplo con \\(n=3\\) dígitos podemos escribir las siguientes variaciones de orden \\(k=2\\)\n\\[\nV^{k=2}_{n=3}=\\frac{3!}{(3-2)!}=\\frac{1\\cdot 2\\cdot 3}{1}=6.\n\\]\n\nEjercicio\nCarga el paquete gtools de R e investiga la función permutations(n, r, v, set, repeats.allowed) para calcular todas las variaciones anteriores.\n\nVariaciones con repetición\n¿Y repitiendo algún dígito?\n\\[VR_n^k=n^k\\]\nEfectivamente, en nuestro caso\n\\[11,12,13,21,22,23,31,32,33\\]\n\\[\nVR^{k=2}_{n=3}=n^k.\n\\]\n\n\n1.2.5 Permutaciones\nLas permutaciones de un conjunto de cardinal \\(n\\) son todas las variaciones de orden máximo, \\(n\\). Las denotamos y valen:\n\\[\nP_n=V_n^n=n!\n\\]\nPor ejemplo, todos los números que se pueden escribir ordenando todos los dígitos \\(\\{1,2,3\\}\\) sin repetir ninguno.\n\nlibrary(combinat)\nfor(permutacion in permn(3)) print(permutacion)\n\n[1] 1 2 3\n[1] 1 3 2\n[1] 3 1 2\n[1] 3 2 1\n[1] 2 3 1\n[1] 2 1 3\n\n\nEfectivamente, \\[\nP_3=3!=1\\cdot  2\\cdot 3.\n\\]\n\nEjercicio\nCarga el paquete combinat de R e investiga la funcion permn para calcular todas las permutaciones anteriores.\n\n\nEjercicio\nInvestiga el paquete itertools y la función comb de scipy.misc de Python e investiga sus funciones para todas las formas de contar que hemos visto en este tema.\n\n\nEjercicio\nLa función gamma de Euler cobrará mucha importancia en el curso de estadística. Comprueba que la función gamma(x+1) da el mismo valor que la función factorial(x) en R para todo \\(x = \\{1,2,3\\cdots,10\\}\\).\n\n\n\n1.2.6 Números multinomiales. Permutaciones con repetición.\nConsideremos un conjunto de elementos \\(\\{a_1, a_2, \\ldots, a_k\\}\\).\nEntoces, si cada uno de los objetos \\(a_i\\) de un conjunto, aparece repetido \\(n_i\\) veces para cada \\(i\\) desde 1 hasta \\(k\\), entonces el número de permutaciones con elementos repetidos es:\n\\[PR_n^{n_1,n_2,\\ldots,n_k} = {{n}\\choose {n_1\\quad n_2 \\quad\\ldots \\quad n_k}}=\\frac{n!}{n_1!\\cdot n_2!\\cdot \\ldots \\cdot n_k!},\\] donde \\(n=n_1+n_2+\\cdots+n_k\\).\n\nEjemplo\n¿Cuántas palabras diferentes se pueden formar con las letras de la palabra PROBABILIDAD?\nEl conjunto de letras de la palabra considerada es el siguiente: \\(\\{A, B, D, I, L, O, P, R\\}\\) con las repeticiones siguientes: las letras A, B, D, e I, aparecen 2 veces cada una; y las letras L, O, P, R una vez cada una de ellas.\nPor tanto, utilizando la fórmula anterior, tenemos que el número de palabras (permutaciones con elementos repetidos) que podemos formar es\n\\[PR^{2,2,2,2,1,1,1,1}_{12} = \\frac{12!}{(2!)^4(1!)^4} = 29937600.\\]"
  },
  {
    "objectID": "0.html#para-acabar",
    "href": "0.html#para-acabar",
    "title": "1  Prerrequisitos: Teoría de conjuntos y combinatoria",
    "section": "1.3 Para acabar",
    "text": "1.3 Para acabar\n\n1.3.1 Principios básicos para contar cardinales de conjuntos\nEl principio de la suma\nSean \\(A_1, A_2,\\ldots, A_n\\) conjuntos disjuntos dos a dos, es decir \\(A_i\\cap A_j=\\emptyset\\) para todo \\(i\\not= j\\), \\(i,j=1,2,\\ldots n\\). Entonces\n\\[\\#(\\cup_{i=1}^n A_i)=\\sum_{i=1}^n \\#(A_i).\\]\nPrincipio de unión exclusión\nConsideremos dos conjuntos cualesquiera \\(A_1, A_2\\), entonces el cardinal de su unión es\n\\[\\#(A_1\\cup A_2)=\\#(A_1)+\\#(A_2)-\\#(A_1\\cap A_2).\\]\nEl principio del producto\nSean \\(A_1,A_2,\\ldots A_n\\)\n\\[\n\\begin{array}{ll}\n\\#(A_1\\times A_2\\times \\cdots A_n)=&\\#\\left(\\{(a_1,a_2,\\ldots a_n)| a_i\\in A_i, i=1,2,\\ldots n\\}\\right)\\\\\n&=\\prod_{i=1}^n \\#(A_i).\n\\end{array}\n\\]"
  },
  {
    "objectID": "0.html#otros-aspectos-a-tener-en-cuenta",
    "href": "0.html#otros-aspectos-a-tener-en-cuenta",
    "title": "1  Prerrequisitos: Teoría de conjuntos y combinatoria",
    "section": "1.4 Otros aspectos a tener en cuenta",
    "text": "1.4 Otros aspectos a tener en cuenta\nEvidentemente, nos hemos dejado muchas otras propiedades básicas de Teoría de Conjuntos y de Combinatoria como:\n\nPropiedades de los números combinatorios.\nBinomio de Newton.\nMultinomio de Newton.\n\nSi nos son necesarias las volveremos a repetir a lo largo del curso o bien daremos enlaces para que las podáis estudiar en paralelo.\n\nNota\nPuedes repasar todos esos conceptos con ejercicios y más en el Curso de estadística descriptiva con R y Python con M. Santos y J.G. Gomila."
  },
  {
    "objectID": "1.html#probabilidades-básicas",
    "href": "1.html#probabilidades-básicas",
    "title": "2  Probabilidad",
    "section": "2.1 Probabilidades Básicas",
    "text": "2.1 Probabilidades Básicas\n Experimento aleatorio: experimento que repetido en las mismas condiciones puede dar resultados diferentes, pero que a largo plazo son predecibles.\n\nEjemplo\nTirar un dado de 6 caras y anotar el número de puntos de la cara superior.\n\nSuceso elemental: cada uno de los posibles resultados del experimento aleatorio.\n\nEjemplo\nLos sucesos elementales del ejemplo anterior serían:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Espacio muestral: el conjunto \\(\\Omega\\) formado por todos los sucesos elementales del experimento aleatorio.\n\nEjemplo\nEl espacio muestral del ejemplo anterior del dado es\n\n\\(\\Omega=\\Big\\{\\)::: {.cell-output-display}  :::\n, ::: {.cell-output-display}  :::\n, ::: {.cell-output-display}  :::\n, ::: {.cell-output-display}  :::\n, ::: {.cell-output-display}  :::\n, ::: {.cell-output-display}  :::\n\\(\\Big\\}\\)\n\npero por comodidad, a partir de ahora pondremos \\[\\Omega = \\{1,2,3,4,5,6\\}\\]\n\n Suceso  : Cualquier subconjunto del espacio muestral.\nAlguno sucesos notables que merece la pena nombrar son:\n\nSuceso seguro o cierto: \\(\\Omega\\).\nSuceso imposible o vacio: \\(\\emptyset\\).\nPartes de un conjunto: \\(\\mathcal{P}(\\Omega)\\), conjunto de todos los sucesos del experimento aleatorio (es decir, el conjunto de todos los subconjuntos de \\(\\Omega\\)).\n\n\nEjercicio\n¿Cuántos elementos contiene el conjunto de partes de \\(\\Omega\\) del experimento anterior?\n\n\nEjemplo \\(n\\)-grama\nSe define un \\(n\\)-grama de una palabra como el conjunto de \\(n\\) letras consecutivas de la misma (contando los blancos de inicio y final de palabra que marcamos como “_”).\nConsideremos el experimento aleatorio que consiste en escoger al azar un 3-grama de la palabra “_Baleares_”. Vamos a escribir el espacio muestral y algunos sucesos elementales del mismo.\n\nEn este caso, si consideramos la palabra “_Baleares_”, el espacio muestral del experimento sería:\n\\[\\Omega=\\{\\_Ba, Bal, ale, lea, ear, are, res, es\\_\\}\\]\nAlgunos sucesos serían:\n\n3-gramas que empiezan por \\(a\\): \\(\\{ale,are\\}\\).\n3-gramas de inicio y final de palabra: \\(\\{\\_Ba,es\\_\\}\\).\n3-gramas que contengan una \\(l\\): \\(\\{Bal,ale,lea\\}\\).\n\n\n\n\n2.1.1 Operaciones con sucesos\nSi tenemos dos sucesos \\(A,B\\subseteq \\Omega\\), podemos definir:\n\n\\(\\Omega\\): suceso total o seguro.\n\\(\\emptyset\\): suceso vacío o imposible.\n\\(A\\cup B\\): suceso unión; el que ocurre si sucede \\(A\\) o \\(B\\).\n\\(A\\cap B\\): suceso intersección; el que ocurre si sucede \\(A\\) y \\(B\\).\n\\(A^c\\): suceso complementario; el que sucede si NO sucede \\(A\\).\n\\(A- B=A\\cap B^c\\): suceso diferencia; el que acontece si sucede \\(A\\) y NO sucede \\(B\\).\n\n Sucesos incompatibles: \\(A\\) y \\(B\\) son incompatibles (o disjuntos) cuando \\(A\\cap B=\\emptyset\\).\n\nEjemplo género\nSupongamos que una clase se divide entre Mujeres y Hombres. Vamos a definir el espacio muestral, los sucesos elementales y a realizar algunas operaciones entre ellos.\n\n\nEstudiantes de esta clase: \\(\\Omega\\).\nMujeres de esta clase: \\(A\\).\nEstudiantes que son zurdos: \\(B\\).\n\nAlgunas operaciones entre los conjuntos:\n\n\\(A\\cup B\\): Estudiantes que son mujeres o que son zurdos.\n\\(A\\cap B\\): Mujeres de esta clase que son zurdas.\n\\(A^c\\): Hombres de esta clase.\n\\(A-B\\): Mujeres de la clases que NO son zurdas.\n\\(B-A\\): Hombres de la clase que son zurdos.\n¡Cuidado! No son incompatibles.\n\n\n\n\n\n2.1.2 Propiedades\nConmutativas:\n\\[A\\cup B=B\\cup A, \\quad A\\cap B=B\\cap A\\]\nAsociativas:\n\\[A\\cup(B\\cup C)=(A\\cup B)\\cup C, \\quad A\\cap(B\\cap C)=(A\\cap B)\\cap C\\]\nDistributivas:\n\\[A\\cap(B\\cup C)=(A\\cap B)\\cup (A\\cap C), \\quad A\\cup(B\\cap C)=(A\\cup B)\\cap (A\\cup C)\\]\n\n\n\n\n\n\n\n\n\\(A\\)\n\\(B\\cap C\\)\n\\(A\\cup (B\\cap C)\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(A\\cup B\\)\n\\(A\\cup C\\)\n\\((A\\cup B)\\cap (A\\cup C)\\)\n\n\n\n\n\n\n\n\n\n\n Complementario del complementario  \\[(A^c)^c=A\\]\n\n\n\n\n\n\n\n\n\\(A\\)\n\\(A^c\\)\n\\((A^c)^c\\)\n\n\n\n\n\n\n\n\n\n\nLeyes de De Morgan\n\\[(A\\cup B)^c=A^c\\cap B^c\\]\n\n\n\n\n\n\n\n\\(A\\cup B\\)\n\\((A\\cup B)^c\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(A^c\\)\n\\(B^c\\)\n\\(A^c\\cap B^c\\)\n\n\n\n\n\n\n\n\n\n\nLeyes de De Morgan\n\\[(A\\cap B)^c=A^c\\cup B^c\\]\n\n\n\n\n\n\n\n\\(A\\cap B\\)\n\\((A\\cap B)^c\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(A^c\\)\n\\(B^c\\)\n\\(A^c\\cup B^c\\)\n\n\n\n\n\n\n\n\n\n\n\n\n2.1.3 Definición de probabilidad\nLa probabilidad de un suceso es una puntuación (score) numérica entre 0 y 1 que mide la verosimilitud de que este evento se produzca.\nEsta verosimilitud puede estar justificada por:\n\nEstimación personal\nEstimación de expertos\nLa frecuencia con la que se da\nCálculo formal\n\nSea \\(\\Omega\\) el espacio muestral de un experimento aleatorio. Supongamos que el número de posibles resultados, por el momento, es finito.\nUna probabilidad sobre \\(\\Omega\\) es una aplicación \\(P:\\mathcal{P}(\\Omega)\\to [0,1]\\) con las siguientes propiedades:\n\n\\(0\\leq P(A)\\leq 1\\), para todo suceso \\(A\\).\n\\(P(\\Omega)=1\\).\nSi \\(\\{A_1,A_2,\\ldots,A_n\\}\\) son sucesos disjuntos dos a dos, entonces\n\n\\[\nP(A_1\\cup A_2\\cup \\cdots \\cup A_n)=P(A_1)+P(A_2)+\\cdots +P(A_n).\n\\]\nSi \\(a\\in \\Omega\\) es un suceso elemental, cometeremos el abuso de notación de poner \\(P(a)\\) en lugar de \\(P(\\{a\\})\\).\n\nEjemplo: grupos sanguíneos\nEn la página de la Fundación Banco de Sangre y Tejidos de las Islas Baleares podemos encontrar información sobre los porcentajes de tipos de sangre de los donantes de las Islas Baleares:\n\\[A: 46\\%;\\ B: 7.5\\%;\\ AB: 3.5\\%;\\ O: 43\\%.\\]\n¿Cuál es la probabilidad de que un balear donante de sangre no sea del tipo O?\n\nExperimento aleatorio: tipo de sangre de un paciente humano\n\\[\\Omega=\\{\\mbox{A,B,AB,O}\\}.\\]\nProbabilidad de un suceso: se asimila al porcentaje observado de individuos\nSuceso: \\(\\{\\mbox{O}\\}^c=\\{\\mbox{A,B,AB}\\}\\)\n\\[P(\\{\\mbox{O}\\}^c)\\!=\\!P(\\{\\mbox{A,B,AB}\\})\\!=\\!\nP(\\mbox{A})+P (\\mbox{B})+P(\\mbox{AB})\\!=\\!0.57.\\]\n\n\n\n\n2.1.4 Propiedades\n\nPropiedades básicas de la probabilidad\n\n\n\\(P(\\emptyset)=0\\).\n\\(P(A-B)=P(A)-P(A\\cap B)\\) porque \\(P(A)=P(A-B)+P(A\\cap B)\\).\n\n\n\n\n\n\n\n\n\n\n\nSi \\(B\\subseteq A\\), entonces \\(0\\leq P(B)\\leq P(A)\\).\n\\(P(A^c)=1-P(A)\\).\n\\(P(A\\cup B)=P(A)+P(B)-P(A\\cap B)\\) porque\n\n\\[\\begin{eqnarray*}\nP(A)+P(B)-P(A\\cap B) &=& P(A-B)+P(A\\cap B)+\\\\\n& & P(B-A)+ P(A\\cap  B)-P(A\\cap  B)\\\\\n&=& P(A-B)+P(A\\cap B)+ P(B-A) \\\\\n&=& P(A\\cup B).\n\\end{eqnarray*}\\]\n\n\n\n\n\n\n\n\n\n\n\\(P(A\\cup B\\cup C):\\)\n\n\\[\\begin{eqnarray*}\nP(A\\cup B\\cup C)&=&P(A)+P(B)+P(C)  \\\\ &&-P(A\\cap B)-P(A\\cap C)-P(B\\cap C)  +P(A\\cap B\\cap C).\n\\end{eqnarray*}\\]\n\n\n\n\n\n\n\n\n\n\\[P(A\\cup B\\cup C)=P(1)+P(2)+P(3)+P(4)+P(5)+P(6)+P(7).\\]\n\nSi \\(A=\\{a_1,a_2,\\ldots,a_k\\}\\), entonces \\[\nP(A)=P(a_1)+P(a_2)+\\cdots+P(a_k).\n\\]\nSi todos los sucesos elementales tienen la misma probabilidad, \\[\nP(A)=\\frac{|A|}{|\\Omega|}\\Big(=\\frac{\\mbox{casos favorables}}{\\mbox{casos posibles}}\\Big).\n\\]\n\n\nEjemplo: Frecuencia de vocales\nLos porcentajes de vocales de un determinado idioma (de alfabeto latino) según la Wikipedia son:\n\\[A: 18.7\\%;\\ E: 26.1\\%;\\ I: 25.7\\%;\\ O: 24.4\\%;\\  U: 5.1\\%.\\]\n¿Cuál es la probabilidad que una vocal escogida al azar de este idioma sea una E o una O?\n\nEl espacio muestral del experimento es \\(\\Omega=\\{A,E,I,O,U\\}\\).\nEl suceso que deseamos analizar es \\(\\{E,O\\}\\).\nY su probabilidad es\n\\[P(\\{E,O\\})=P(E)+P(O)=0.261+0.244=0.505.\\]\n\n\n\nEjemplo: Consumo de drogas\nSegún un artículo de El País, en un control especial de la policía el \\(0.1\\%\\) de todos los conductores analizados en un control de tráfico dan positivo en un el test en cocaína, y el \\(1\\%\\) da positivo en cannabis. Un \\(1.05\\%\\) da positivo en alguno de los dos test.\n¿Cuál es la probabilidad que un individuo analizado en el control de drogas escogido al azar no dé positivo en ninguno de lo dos test?\n\nLos sucesos elementales del enunciado del problema son:\n\n\\(A\\): dar positivo en cocaína; \\(P(A)=0.001\\).\n\\(B\\): dar positivo en cannabis; \\(P(B)=0.01\\).\n\nEn este caso nos interesa estudiar los sucesos:\n\n\\(A\\cup B\\): dar positivo en alguno de los dos test; \\(P(A\\cup B)=0.0105\\).\n\\((A\\cup B)^c\\): no dar positivo en ninguno de los test,\n\nde donde, por tanto: \\[P((A\\cup B)^c)=1-P(A\\cup B)=1-0.0105=0.9895.\\]\n\n\n\nEjemplo\nEn un control especial de la policía el \\(0.1\\%\\) de todos los conductores analizados en un control de tráfico dan positivo en un el test en cocaína, y el \\(1\\%\\) da positivo en cannabis. Un \\(1.05\\%\\) da positivo en alguno de los dos test.\n¿Cuál es la probabilidad que un analizado al azar dé positivo en los dos test en cocaína y cannabis?\n\nLos sucesos elementales son:\n\n\\(A\\): dar positivo en cocaína; \\(P(A)=0.001\\).\n\\(B\\): dar positivo en cannabis; \\(P(B)=0.01\\).\n\nEn este caso nos interesa estudiar los sucesos:\n\n\\(A\\cup B\\): dar positivo en algún de los dos test; \\(P(A\\cup B)=0.0105\\).\n\\(A\\cap B\\): dar positivo en los dos test,\n\nde donde, por tanto:\n\\[\\begin{array}{rl}\n{P(A\\cap B)} &{=P(A)+P(B)-P(A\\cup B)}\\\\ &{=0.001+0.01-0.0105=0.0005}.\n\\end{array}\\]\n\n¿Cuál es la probabilidad de que un conductor analizado dé positivo en cocaína pero no en cannabis?\n\nLos sucesos elementales son:\n\n\\(A\\): dar positivo en cocaína; \\(P(A)=0.001\\).\n\\(B\\): dar positivo en cannabis; \\(P(B)=0.01\\).\n\nEn este caso nos interesa estudiar los sucesos:\n\n\\(A\\cap B\\): dar positivo en los dos test; \\(P(A\\cap B)=0.0005\\).\n\\(A-B\\): dar positivo en cocaína, pero no en cannabis,\n\nde donde, por tanto:\n\\[P(A-B) =P(A)-P(A\\cap B) =0.001-0.0005=0.0005.\\]"
  },
  {
    "objectID": "1.html#probabilidad-condicionada",
    "href": "1.html#probabilidad-condicionada",
    "title": "2  Probabilidad",
    "section": "2.2 Probabilidad condicionada",
    "text": "2.2 Probabilidad condicionada\n Probabilidad condicionada: Dados dos sucesos \\(A\\) y \\(B\\), con \\(P(A)&gt;0\\), la probabilidad \\(P(B|A)\\) de \\(B\\) condicionado a \\(A\\) es la probabilidad\n\nde que suceda \\(B\\) suponiendo que pasa \\(A\\),\nde que si pasa \\(A\\), entonces suceda \\(B\\),\nde que un resultado de \\(A\\) también pertenezca a \\(B\\).\n\nSe calcula a través de la definición:\n\\[\nP(B|A)=\\frac{P(A\\cap B)}{P(A)}.\n\\]\n\nEjemplo: frecuencia género y gafas\nEn una clase de 20 hombres y 30 mujeres, 15 hombres y 18 mujeres llevan gafas. Contestemos las siguientes preguntas:\n\n\n¿Cuál es la probabilidad de que un alumno lleve gafas?\n\n\\[\n\\frac{33}{50}.\n\\]\n\n¿Cuál es la probabilidad de que un alumno sea mujer y lleve gafas?\n\n\\[\n\\frac{18}{50}.\n\\]\n\n¿Cuál es la probabilidad de que un chica lleve gafas?\n\n\\[\n\\frac{18}{30}=\\frac{18/50}{30/50}=\\frac{P(\\mbox{mujer  y gafas})}{P(\\mbox{mujer})}.\n\\]\n\nSi escogemos un estudiante al azar ¿Cuál es la probabilidad que si es mujer, entonces lleve gafas?\n\n\\[\n\\frac{18}{30}.\n\\]\n\n¿Cuál es la probabilidad de que un alumno que lleve gafas sea mujer?\n\n\\[\n\\frac{18}{33}=\\frac{18/50}{33/50}=\\frac{P(\\mbox{mujer y gafas})}{P(\\mbox{gafas})}.\n\\]\n\nSi escogemos un estudiante al azar ¿Cuál es la probabilidad de que si lleva gafas, entonces sea mujer? \\[\n\\frac{18}{33}.\n\\]\n\n\n\n\n2.2.1 ¡Atención!\nHay que distinguir bien entre\n\n\\(P(A\\cap B)\\): probabilidad de \\(A\\) \\(\\color{red}{\\text{y}}\\) \\(B\\), Probabilidad de que lleve gafas y sea mujer.\n\\(P(A|B)\\): probabilidad de que \\(\\color{red}{\\text{si}}\\) pasa \\(B\\), \\(\\color{red}{\\text{entonces}}\\) pase \\(A\\), Probabilidad de que, si es mujer, lleve gafas.\n\nCuando utilizamos probabilidad condicional \\(P(A|B)\\) estamos restringiendo el espacio muestral a \\(B\\).\n\n\n2.2.2 Propiedades\nLa probabilidad condicionada es una probabilidad\n\nProposición\n\nSea \\(A\\subseteq \\Omega\\) un suceso tal que \\(P(A)&gt;0\\), entonces\n\\[\n\\begin{array}{rccl}\nP(-|A):& \\mathcal{P}(\\Omega) & \\to & [0,1]\\\\\n&B & \\mapsto & P(B|A).\n\\end{array}\n\\] satisface las propiedades de las probabilidades, como por ejemplo:\n\\[\n\\begin{array}{l}\nP(B^c|A)=1-P(B|A),\\\\\nP(B_1\\cup B_2|A)=P(B_1|A)+P(B_2|A)-P(B_1\\cap B_2|A).\n\\end{array}\n\\]\n\nEjercicio\nEscribid el resto de propiedades que cumpliría una probabilidad condicionada al evento \\(A\\).\n\n\nEjemplo: Hipertensos\nUn 15% de los adultos son hipertensos, un 25% de los adultos creen que son hipertensos, y un 9% de los adultos son hipertensos y creen que lo son.\n\nSi un adulto cree que es hipertenso, ¿cuál es la probabilidad que lo sea?\nSi un adulto es hipertenso, ¿cuál es la probabilidad que crea que lo es?\n\n\nSean los sucesos\n\n\\(A\\): ser hipertenso; \\(P(A)=0.15\\).\n\\(B\\): creer ser hipertenso; \\(P(B)=0.25\\),\n\nentonces podemos definir el suceso:\n\n\\(A\\cap B\\): ser hipertenso y creerlo; \\(P(A\\cap B)=0.09\\),\n\nde donde, la probabilidad condicionada de ser hipertenso creyéndonos que lo somos es:\n\\[P(A|B)=\\dfrac{P(A\\cap B)}{P(B)}=\\dfrac{0.09}{0.25}=0.36.\\]\nSi un adulto es hipertenso, ¿cuál es la probabilidad que crea que lo es?\nSi tenemos los sucesos:\n\n\\(A\\): ser hipertenso,\n\\(B\\): creer ser hipertenso,\n\nentonces buscamos la probabilidad \\(P(B|A)\\):\n\\[\n\\begin{array}{rl}\nP(B|A) & =\\dfrac{P(A\\cap B)}{P(A)}=\\dfrac{0.09}{0.15}=\n0.6.\n\\end{array}\n\\]\n\n\n\nEjemplo: dígitos de control\nUn dígito de control de error toma el valor 0 en el 99% de los casos en que hay un error. Si la probabilidad de error en un mensaje es del \\(0.5\\%\\). ¿cuál es la probabilidad de que el mensaje sea erróneo y el código de error tenga valor 0?\n\n\n\\(B\\): mensaje con error; \\(P(B)=0.005\\),\n\\(A\\): código de error vale 0,\n\\(P(A|B)=0.99\\),\n\nentonces: \\[P(A\\cap B)=P(B)\\cdot P(A|B)=0.005\\cdot 0.99=0.00495.\\]\n\n\n\nEjemplo: SPAM\nUn 50% de correos recibidos en un servidor llevan adjuntos y un 65% son publicidad no deseada (SPAM). Sólo un 15% de estos correos no llevan adjuntos y no son SPAM.\n\n¿Cuál es la probabilidad que un correo lleve adjunto si es SPAM?\n¿Cuál es la probabilidad que un correo no tenga adjuntos si no es SPAM?\n\n\n¿Cuál es la probabilidad que un correo lleve adjunto si es SPAM?\n\n\\(A\\): llevar adjuntos, \\(P(A)=0.5\\),\n\\(S\\): SPAM, \\(P(S)=0.65\\),\n\\(A^c\\cap S^c=(A\\cup S)^c\\): no llevar adjunto y no ser SPAM, \\(P((A\\cup S)^c)=0.15\\).\n\n\\[P(A|S)=\\dfrac{P(A\\cap S)}{P(S)}=?\\]\n\n\\(P(A)=0.5, P(S)=0.65, P(A^c\\cap S^c)=P((A\\cup S)^c)=0.15\\),\n\\(P(A\\cup S)=1-P((A\\cup S)^c)=0.85\\),\n\\(P(A\\cap S)=P(A)+P(S)-P(A\\cup S)=0.3\\),\n\n\\[P(A|S)=\\dfrac{P(A\\cap S)}{P(S)}=\\dfrac{0.3}{0.65}\\approx 0.46.\\]\n¿Cuál es la probabilidad de que un correo no lleve adjuntos si no es SPAM?\n\\[P(A)=0.5, P(S)=0.65, P(A^c\\cap S^c)=P((A\\cup S)^c)=0.15\\]\n\\[P(A^c|S^c)=\\dfrac{P(A^c\\cap S^c)}{P(S^c)}=\\dfrac{P(A^c\\cap S^c)}{1-P(S)}=\\dfrac{0.15}{0.35}\\approx 0.43.\\]\n\n\n\n\n2.2.3 Teorema de la probabilidad total\nTeorema de la probabilidad total\nDados dos sucesos \\(A\\) y \\(B\\) se tiene que\n\\[\n\\begin{array}{rl}\nP(B)&= P(B\\cap A) +P(B\\cap A^c)\\\\\n& =P(A)\\cdot P(B|A)+ P(A^c)\\cdot P(B|A^c).\n\\end{array}\n\\]\nPartición del espacio espacio muestral\nLos sucesos \\(A_1,A_2,\\ldots, A_n\\) son una partición del espacio muestral \\(\\Omega\\) de un determinado experimento aleatorio, si cumplen las condiciones siguientes:\n\n\\(A_1\\cup A_2\\cup\\ldots\\cup A_n=\\Omega\\),\n\\(A_1,A_2,\\ldots,A_n\\) son incompatibles dos a dos (\\(A_i\\cap A_j=\\emptyset\\)).\n\nTeorema de la probabilidad total (generalización) Sea \\(A_1,A_2,\\ldots,A_n\\) una partición de \\(\\Omega\\). Sea \\(B\\) un suceso cualquiera. Entonces\n\\[\n\\begin{array}{rl}\nP(B)&= P(B\\cap A_1)+\\cdots +P(B\\cap A_n)\\\\\n& =P(A_1)\\cdot P(B|A_1)+\\ldots+P(A_n)\\cdot P(B|A_n).\n\\end{array}\n\\]\n\nEjemplo: Dígito de control de error\nUn dígito de control de error toma el valor 0 en un \\(99\\%\\) de los casos en que hay un error y en un \\(5\\%\\) de los mensajes sin error. La probabilidad de error en un mensaje es del \\(0.5\\%\\).\n¿Cuál es la probabilidad de que un mensaje escogido al azar tenga el dígito de control a 0?\n\nSean los sucesos del enunciado:\n\n\\(B\\): mensaje con error; \\(P(B)=0.005\\),\n\\(A\\): código de error vale 0,\n\nentonces obtenemos las probabilidades a partir del enunciado:\n\n\\(P(A|B)=0.99\\),\n\\(P(A|B^c)= 0.05\\),\n\ny por tanto,\n\\[\n\\begin{array}{rl}\nP(A) & =P(B)\\cdot P(A|B)+P(B^c)\\cdot P(A|B^c)\\\\ &\n=0.005\\cdot 0.99+0.995\\cdot 0.05=0.0547.\\end{array}\n\\]\n\n\n\n\n2.2.4 Clasificación o Diagnósticos\nConsideremos alguna de las siguientes situaciones:\n\nUn algoritmo detecta si una transacción con tarjeta de crédito es fraude o no.\nUn algoritmo detecta si tiene o no que mostrar un anuncio en una web.\nUna prueba de embarazo.\nUna prueba médica para una enfermedad concreta.\n\nNos ceñiremos a la casuística más elemental, donde el algoritmo de clasificación o la diagnosis solo da dos resultados: Positivo (sí tienes la enfermedad, sí es un fraude) o Negativo (en caso contrario).\nEn todas estas situaciones podemos calcular lo que se llama matriz de confusión que representa todas las situaciones posibles. En el caso de estudiar una condición de tipo binario,\n\n\n\n\nEl Test da Positivo\nEl Test da Negativo\n\n\n\n\nCondición Positiva\nCorrecto\nError\n\n\nCondición Negativa\nError\nCorrecto\n\n\n\nEn general, los modelos y algoritmos de clasificación suelen aportar puntuaciones (scores) que determinan el grado de pertenencia a una clase, o que miden si dos objetos están en la misma clase.\nAsí, el resultado del clasificador o del diagnóstico puede ser:\n\nun número real, en cuyo caso el clasificador entre cada clase debe determinarse por un valor umbral (threshold). Por ejemplo, para determinar si una persona está estresada podemos dar un scores entre 0 y 1 (1 máximo estrés 0 estrés nulo),\nun resultado discreto que indica directamente una de las clases (esto es necesario si es un algoritmo que debe decidir qué hacer con el objeto.\n\n Positivos y Negativos en Clasificación Consideremos un problema de predicción de clases binario, en el que los resultados se etiquetan positivos (P) o negativos (N). Hay cuatro posibles resultados a partir de un clasificador binario como el propuesto.\n\nSi el resultado de una exploración es P y el valor dado es también P, entonces se conoce como un Verdadero Positivo (VP).\nSin embargo, si el valor real es N entonces se conoce como un Falso Positivo (FP).\nDe igual modo, tenemos un Verdadero Negativo (VN) cuando tanto la exploración como el valor dado son N.\nUn Falso Negativo (FN) cuando el resultado de la predicción es N pero el valor real es P.\n\nUn ejemplo aproximado de un problema real es el siguiente: consideremos una prueba diagnóstica que persiga determinar si una persona tiene una cierta enfermedad.\n\nUn falso positivo en este caso ocurre cuando la prueba predice que el resultado es positivo, cuando la persona no tiene realmente la enfermedad.\nUn falso negativo, por el contrario, ocurre cuando el resultado de la prueba es negativo, sugiriendo que no tiene la enfermedad cuando realmente sí la tiene.\n\nEn un diagnóstico de una cierta condición (por ejemplo, test embarazo, test de enfermedad), tenemos dos tipos de sucesos:\n\n\\(T\\): el test da positivo,\n\\(M\\): el sujeto satisface la condición,\nFalsos positivos \\(T\\cap M^c\\): El test da positivo, pero la condición no se da,\nCoeficiente de falsos positivos \\(P(T|M^c)\\),\nFalsos negativos \\(T^c\\cap M\\): El test da negativo, pero la condición sí que se da,\nCoeficiente de falsos negativos: \\(P(T^c|M)\\).\n\n\nEjemplo: Prueba médica\nUn test diseñado para diagnosticar una determinada enfermedad tiene un coeficiente de falsos negativos de 0.06, y un coeficiente de falsos positivos de 0.04. En un estudio masivo se observa que un 15% de la población da positivo al test.\n¿Cuál es la probabilidad que una persona escogida aleatoriamente tenga esta enfermedad?\n\nLos datos del problema son:\n\n\\(T\\): dar positivo al test; \\(P(T)=0.15\\),\n\\(M\\): tener la enfermedad,\n\\(P(T)=0.15\\), \\(P(T^c|M)=0.06\\), \\(P(T|M^c)=0.04\\),\n¿\\(P(M)\\)?\n\n\\[\nP(T) =P(M)\\cdot P(T|M)+P(M^c)\\cdot P(T|M^c),\n\\]\ndonde\n\\[\n\\begin{array}{l}\nP(T|M)=1-P(T^c|M)=0.94, \\\\[1ex]\nP(M^c)=1-P(M).\n\\end{array}\n\\]\nPor lo tanto\n\\[\n\\begin{array}{rl}\n0.15 & = P(M)\\cdot 0.94+(1-P(M))\\cdot 0.04\\\\\n& =0.04+0.9\\cdot P(M),\\\\[1ex]\nP(M) & =\\dfrac{0.11}{0.9}\\approx 0.1222.\n\\end{array}\n\\]"
  },
  {
    "objectID": "1.html#bayes",
    "href": "1.html#bayes",
    "title": "2  Probabilidad",
    "section": "2.3 Bayes",
    "text": "2.3 Bayes\n\n2.3.1 Fórmula de Bayes\n Teorema de Bayes \nSean \\(A\\) y \\(B\\) dos sucesos. Si \\(P(B)&gt;0\\), entonces\n\\[\\begin{eqnarray*}\nP(A|B) & = & \\frac{P(A)\\cdot P(B\\big|A)}{P(B)}\\\\\n&=& \\frac{P(A)\\cdot P(B\\big|A)}{P(A)\\cdot P(B\\big|A)+P(A^c)\\cdot P(B\\big|A^c)}.\n\\end{eqnarray*}\\]\n\nEjercicio\nDemostrar el teorema de Bayes utilizando que\n\\[P(A|B) =\\frac{P(A\\cap B)}{P(B)}=\\cdots\\]\n\n Teorema de Bayes generalizado\nSea \\(A_1,A_2,\\ldots,A_n\\) una partición de \\(\\Omega\\). Sea \\(B\\) un suceso tal que \\(P(B)&gt;0\\). Entonces (para cualquier \\(i=1,2,\\ldots,n\\)):\n\\[\\begin{eqnarray*}\nP(A_i|B) & =& \\dfrac{P(A_i)\\cdot P(B|A_i)}{P(B)}\\\\\n& =& \\dfrac{P(A_i)\\cdot P(B|A_i)}{P(A_1)\\cdot P(B|A_1)+\\cdots+P(A_n)\\cdot P(B|A_n)}.\n\\end{eqnarray*}\\]"
  },
  {
    "objectID": "1.html#independencia-de-sucesos",
    "href": "1.html#independencia-de-sucesos",
    "title": "2  Probabilidad",
    "section": "2.4 Independencia de sucesos",
    "text": "2.4 Independencia de sucesos\n\n2.4.1 Sucesos independientes\n Sucesos Independientes \nDiremos que los sucesos \\(A\\) y \\(B\\) son independientes si \\(P(A\\cap B)=P(A)\\cdot P(B)\\).\n\\(A_1,\\ldots, A_n\\) son sucesos independientes cuando, para toda subfamilia \\(A_{i_1},\\ldots,A_{i_k}\\), \\[\nP(A_{i_1}\\cap \\cdots\\cap A_{i_k})=P(A_{i_1})\\cdots P(A_{i_k}).\n\\]\n Proposición: \nDados dos sucesos \\(A\\) y \\(B\\) con \\(P(A),P(B)&gt;0\\), las siguientes afirmaciones son equivalentes:\n\n\\(A\\) y \\(B\\) son independientes.\n\\(P(A|B)=P(A)\\).\n\\(P(B|A)=P(B)\\).\n\n Proposición: \nLas siguientes afirmaciones son equivalentes:\n\n\\(A\\) y \\(B\\) son independientes.\n\\(A^c\\) y \\(B\\) son independientes.\n\\(A\\) y \\(B^c\\) son independientes.\n\\(A^c\\) y \\(B^c\\) son independientes.\n\n\nEjemplo: Compra billete avión\nEn la web de viajes WEBTravel, el 55% de los clientes compra billete de avión, el \\(20\\%\\) alojamiento en hotel, y el \\(60\\%\\) billete de avión o alojamiento en hotel. ¿Son los sucesos comprar billete de avión y comprar alojamiento en hotel independientes?\n\nLos sucesos y datos del ejemplo son:\n\n\\(A\\): comprar billete de avión, \\(P(A)=0.55\\),\n\\(B\\): comprar alojamiento, \\(P(B)=0.2\\),\n\npor tanto, podemos calcular las probabilidades siguientes\n\\[\\begin{eqnarray*}\nP(A\\cap B) & = &P(A)+P(B)-P(A\\cup B)\\\\\n& = &0.55+0.2-0.6=0.15,\\\\\nP(A)\\cdot P(B) & = & 0.55\\cdot 0.2=0.11.\n\\end{eqnarray*}\\]\nPor tanto, concluimos que son dependientes, ya que \\(P(A\\cap B)\\neq P(A)\\cdot P(B)\\).\n\n\n\n\n2.4.2 Sucesos independientes vs disjuntos\n\nEjercicio\n\nDos sucesos \\(A\\) y \\(B\\) disjuntos, ¿son necesariamente independientes?\nDos sucesos \\(A\\) y \\(B\\) independientes, ¿son necesariamente disjuntos?\n\\(\\emptyset\\) y un suceso cualquiera \\(A\\), ¿son necesariamente independientes?\n\\(\\Omega\\) y un suceso cualquiera \\(A\\), ¿son necesariamente independientes?\n¿Qué condiciones se tienen que dar para que un suceso \\(A\\) sea independiente de sí mismo?"
  },
  {
    "objectID": "2.html#introducción-a-las-variables-aleatorias",
    "href": "2.html#introducción-a-las-variables-aleatorias",
    "title": "3  Variables Aleatorias",
    "section": "3.1 Introducción a las variables aleatorias",
    "text": "3.1 Introducción a las variables aleatorias\n\nHasta ahora nuestros sucesos han sido de varios tipos: \\(\\{C,+\\}\\) en la moneda, nombres de periódicos, ángulos en una ruleta, número de veces que sale cara en el lanzamiento de una moneda etc.\nNecesitamos estandarizar de alguna manera todos estos sucesos. Una solución es asignar a cada suceso un cierto conjunto de números reales, es decir, convertir todos los sucesos en sucesos de números reales para trabajar con ellos de forma unificada.\nPara conseguirlo utilizaremos unas funciones que transformen los elementos del espacio muestral en números; estas funciones son las variables aleatorias.\n\n\n3.1.1 Definición de variable aleatoria\nComenzaremos dando una definición poco rigurosa, pero suficiente, de variable aleatoria.\n Variable Aleatoria (definición práctica) \nUna variable aleatoria (v.a.) es una aplicación que toma valores numéricos determinados por el resultado de un experimento aleatorio.\nNotación:\n\nNormalmente representaremos las v.a. por letras mayúsculas \\(X,Y,Z\\ldots\\)\nLos valores que “toman” las v.a. los representaremos por letras minúsculas (las mismas en principio) \\(x,y,z\\ldots\\)\n\n\nEjemplo: Dado seis caras\nLanzamos un dado convencional de parchís y el espacio muestral del experimento es\n\\[\\Omega=\\{1,2, 3, 4,  5, 6\\}.\\]\n\nUna v.a. \\(X:\\Omega\\to\\mathbb{R}\\) sobre este espacio queda definida por\n\\[\\begin{equation*}\n\\begin{split}\nX(1)&=1,X(2)=2,X(3)=3,\\\\\nX(4)&=4,X(5)=5,X(6)=6.\n\\end{split}\n\\end{equation*}\\]\n\nAhora el suceso \\(A=\\{2, 4, 6\\}\\), es decir “salir número par”, es equivalente a \\(\\{X=2,X=4,X=6\\}\\).\nEl suceso \\(B=\\{1,2,3\\}\\), es decir “salir número inferior o igual a \\(3\\)” es en términos de la v.a. \\(\\{X=1,X=2,X=3\\}\\) o también \\(\\{X\\leq 3\\}\\).\n\n\n\n\nEjemplo: Juego lanzamiento anilla\nConsideremos el experimento lanzar una anilla al cuello de una botella. Si acertamos a ensartar la anilla en la botella, el resultado del experimento es éxito y fracaso en caso contrario.\n\nEl espacio muestral asociado a este experimento será \\(\\Omega=\\{\\mbox{éxito, fracaso}\\}\\). Construyamos la siguiente variable aleatoria:\n\\[X:\\{\\mbox{éxito, fracaso}\\}\\to\\mathbb{R}\\]\ndefinida por\n\\[X(\\mbox{éxito})=1 \\mbox{ y } X(\\mbox{fracaso})=0.\\]\n\n\n\n\n3.1.2 Tipos de variables aleatorias\nHay dos tipos fundamentales de variables aleatorias, las discretas y las continuas.\nDamos a continuación una definición informal.\nVariables Aleatorias Discretas y Continuas \n\nUna variable aleatoria es discreta si sólo puede tomar una cantidad numerable de valores con probabilidad positiva.\nLas variables aleatorias continuas toman valores en intervalos.\nTambién existen las variables aleatorias mixtas; con una parte discreta y otra continua.\n\n\nEjemplo: Tipos de variables aleatorias\nSon variables aleatorias discretas:\n\nNúmero de artículos defectuosos en un cargamento.\nNúmero de clientes que llegan a una ventanilla de un banco en una hora.\nNúmero de errores detectados en las cuentas de una compañía.\nNúmero de reclamaciones de una póliza de un seguro médico.\n\nSon variables aleatorias continuas:\n\nRenta anual de una familia.\nCantidad de petróleo importado por un país.\nVariación del precio de las acciones de una compañía de telecomunicaciones.\nPorcentaje de impurezas en un lote de productos químicos."
  },
  {
    "objectID": "2.html#variables-aleatorias-discretas",
    "href": "2.html#variables-aleatorias-discretas",
    "title": "3  Variables Aleatorias",
    "section": "3.2 Variables aleatorias discretas",
    "text": "3.2 Variables aleatorias discretas\n\n3.2.1 Distribuciones de probabilidad para v.a. discretas.\n\nPasamos ahora a describir el comportamiento de la v.a. Para ello utilizaremos distintas funciones que nos darán algunas probabilidades de la variable aleatoria.\nEn el caso discreto, estas funciones son la de probabilidad y la función de distribución o de probabilidad acumulada.\nEn el caso discreto, la función de probabilidad es la que nos da las probabilidades de los sucesos elementales de la v.a. que definimos a continuación.\n\n Función de Probabilidad\nLa función de probabilidad (probability mass function o incluso abusando de notación probability density function) de una variable aleatoria discreta \\(X\\) a la que denotaremos por \\(P_{X}(x)\\) está definida por \\[P_{X}(x)=P(X=x),\\] es decir, la probabilidad de que \\(X\\) tome el valor \\(x\\).\nSi \\(X\\) no asume ese valor \\(x\\), entonces \\(P_{X}(x)=0\\).\n Dominio de una variable aleatoria discreta \nEl conjunto \\[D_X=\\{ x\\in\\mathbb{R} \\mid P_X(x)&gt;0\\}\\] recibe el nombre de dominio de la v.a. y son los valores posibles de esta variable.\nEn el caso discreto lo más habitual es que \\(X(\\Omega)=D_X\\).\n\nEjemplo: Juego del parchís\nLanzamos un dado de parchís una vez, en esta ocasión representaremos los sucesos elementales por el número de puntos de la cara obtenida, tenemos que \\[\\Omega=\\{\\mbox{1-puntos,2-puntos,3-puntos,4-puntos,5-puntos,6-puntos}\\},\\] y la variable aleatoria \\(X:\\Omega\\to \\mathbb{R}\\) viene definida por\n\\[X(\\mbox{i-puntos})=i\\mbox{ para } i=1,2,3,4,5,6.\\]\n\nSupongamos que el dado está bien balanceado. Entonces \\[P_{X}(1)=P_{X}(2)=P_{X}(3)=P_{X}(4)=P_{X}(5)=P_{X}(6)=\\frac16.\\] Concretamente: \\[\nP_{X}(x)=\n  \\left\\{\n  \\begin{array}{ll}\n   \\frac16 & \\mbox{si } x=1,2,3,4,5,6\\\\\n  0 & \\mbox{en otro caso. }\n  \\end{array}\n  \\right.\n\\]\nSu dominio es \\[D_X=\\{1,2,3,4,5,6\\}.\\]\n\n\n\nEjemplo: Lanzamiento moneda\nSea \\(X\\) la v.a. asociada al lanzamiento de una moneda. Su espacio muestral es \\(\\Omega=\\{c,+\\}\\), la v.a. queda definida por:\n\\[X(\\omega)=\\left\\{\\begin{array}{ll} 1 & \\mbox{si } \\omega=c, \\\\\n0 & \\mbox{si }\\omega=+.\\end{array}\\right.\\]\n\nSu función de probabilidad es:\n\\[P_{X}(x)=P(X=x)=\\left\\{\\begin{array}{ll} \\frac12, & \\mbox{si } x=0,1,\\\\\n0, & \\mbox{en otro caso.}\\end{array}\\right.\\]\nFinalmente su dominio es \\(D_X=\\{0,1\\}.\\)\n\n\n\nEjemplo: Urna con bolas\nTenemos una urna con tres bolas rojas, una negra y dos blancas. Realizamos una extracción y observamos el color de la bola. Entonces, un espacio muestral es \\[\\Omega=\\{roja, blanca, negra\\}.\\]\n\nUna variable aleatoria asociada al experimento es:\n\\[X(\\omega)=\\left\\{\\begin{array}{ll} 1, & \\mbox{si } \\omega=roja,  \\\\\n2, & \\mbox{si }\\omega=negra, \\\\ 3, & \\mbox{si } \\omega=blanca. \\end{array}\\right.\\]\nLa función de probabilidad es\n\\[P_{X}(x)=\\left\\{\\begin{array}{ll} \\frac36, & \\mbox{si } x=1,\\\\[1ex]\n\\frac16, & \\mbox{si } x=2,\\\\[1ex] \\frac26, & \\mbox{si } x=3,\\\\[1ex] 0, & \\mbox{en otro\ncaso.}\\end{array}\\right.\\]\nEl dominio de la v.a. \\(X\\) es \\(D_X=\\{1,2,3\\}.\\)\n\n\n Propiedades básicas de la función de probabilidad \nSea \\(X\\) una v.a. discreta \\(X:\\Omega\\to\\mathbb{R}\\) con dominio \\(D_X\\). Su función de probabilidad \\(P_{X}\\) verifica las siguientes propiedades:\n\n\\(0\\leq P_{X}(x)\\leq 1\\), para todo \\(x\\in\\mathbb{R}\\).\n\\(\\sum\\limits_{x\\in D_X} P_{X}(x)=1\\).\n\n\nEjemplo: Lanzamiento moneda\nLanzamos al aire tres veces, de forma independiente, una moneda perfecta. El espacio muestral de este experimento es \\[\\Omega=\\{ccc,cc+,c+c,+cc,c++,+c+,++c,+++\\}\\] (expresados en orden de aparición).\n\nEste espacio tiene todos los sucesos elementales equiprobables.\nConsideremos la variable aleatoria asociada a este experimento:\n\\[X=\\mbox{ número de caras en los tres lanzamientos}.\\]\nSu función de probabilidad es:\n\\[\n\\begin{array}{l}\nP(X=0)=P(\\{+++\\})=\\frac18,\\\\ P(X=1)=P(\\{c++,+c+,++c\\})=\\frac38,\\\\\n    P(X=2)=P(\\{cc+,c+c,+cc\\})=\\frac38,\\\\\n    P(X=3)=P(\\{ccc\\})=\\frac18.\n\\end{array}\n\\]\nPodemos reescribir la función de probabilidad de \\(X\\) de forma simplificada:\n\\[P_{X}(x)=\\left\\{\\begin{array}{ll} \\frac18, & \\mbox{si } x=0, 3,\\\\[1ex]\n\\frac38, & \\mbox{si } x=1,2,\\\\[1ex] 0, & \\mbox{en otro caso.}\\end{array}\\right.\\]\nEfectivamente los valores de la función de distribución suman 1:\n\\[\\sum_{x=0}^3 P_X(x)= \\frac18+\\frac38+\\frac38+\\frac18=1.\\]\n\n\n Distribución de Probabilidad\nLa función de distribución de probabilidad (acumulada) de la v.a. \\(X\\) (de cualquier tipo; discreta o continua) \\(F_{X}(x)\\) representa la probabilidad de que \\(X\\) tome un menor o igual que \\(x\\), es decir,\n\\[F_{X}(x)=P(X\\leq x).\\]\nEsta función también se denomina función de distribución de probabilidad o simplemente función de distribución de una v.a., y en inglés cumulative distribution function por lo que se abrevia con el acrónimo cdf.\n Propiedades de la Función de Distribución\nSea \\(X\\) una v.a. y \\(F_{X}\\) su función de distribución:\n\n\\(P(X&gt;x)=1-P(X\\leq x)=1-F_{X}(x).\\)\nSea a y b tales que \\(a&lt;b\\), \\(P(a&lt;X\\leq b)=P(X\\leq b)-P(X\\leq a)=F_{X}(b)-F_{X}(a).\\)\n\n\nDemostración:\nTenemos que el complementario de \\(X\\) mayor que \\(x\\) es: \\(\\overline{\\left\\{X&gt;x\\right\\}}=\\left\\{X&gt;x\\right\\}^c=\\left\\{X\\leq x\\right\\}\\). Además,\n\\[P(X&gt;x)=1-P(\\overline{\\left\\{X&gt;x\\right\\}})=1-P(X\\leq x)=1-F_{X}(x),\\]\nlo que demuestra la primera propiedad.\nPor otro lado, que \\(X\\) se encuentre entre dos valores \\(a\\) y \\(b\\) es \\(\\left\\{a&lt; X \\leq b\\right\\}= \\left\\{X\\leq b\\right\\}-\\left\\{X\\leq a\\right\\}\\). Ahora podemos hacer\n\\[\\begin{eqnarray*}\nP(a&lt;X\\leq b)&=&P(\\left\\{X\\leq b\\right\\}-\\left\\{X\\leq a\\right\\})\\\\\n&=& P(\\left\\{X\\leq b\\right\\})-P(\\left\\{X\\leq a\\right\\})\\\\\n&=& F_{X}(b)-F_{X}(a).\n\\end{eqnarray*}\\]\n\n Más propiedades de la Función de Distribución\nSea \\(F_{X}\\) la función de distribución de una v.a. \\(X\\) entonces:\n\n\\(0\\leq F_{X}(x)\\leq 1\\).\nLa función \\(F_{X}\\) es no decreciente.\nLa función \\(F_{X}\\) es continua por la derecha.\nSi denotamos por \\(F_X(x_0^{-})=\\displaystyle \\lim_{x\\to x_0^{-}} F(x)\\), entonces se cumple que \\(P(X&lt; x_0)=F_X(x_0^{-})\\) y que \\(P(X=x_0)=F_X(x_0)-F_X(x_0^{-})\\).\nSe cumple que \\(\\displaystyle \\lim_{x\\to\\infty} F_{X}(x)=1\\); \\(\\displaystyle \\lim_{x\\to-\\infty}F_{X}(x)=0\\).\nToda función \\(F\\) verificando las propiedades anteriores es función de distribución de alguna v.a. \\(X\\).\n\\(P(X&gt;x)=1-F_{X}(x)\\).\nDados \\(a,b\\in \\mathbb{R}\\) con \\(a&lt;b\\), \\[P(a&lt;X\\leq b)=F_{X}(b)-F_{X}(a).\\]\n\nAdvertencia desigualdades estrictas\nEn las propiedades anteriores no se pueden cambiar, en general, las desigualdades de estrictas o no estrictas.\nVeamos qué propiedades tenemos cuando se cambian estas desigualdades.\nDada una \\(F_{X}\\) una función de distribución de la v.a. \\(X\\) y denotamos por \\[F_{X}(x_0^{-})=\\displaystyle \\lim_{x\\to x_0^{-}} F_{X}(x),\\] entonces se cumplen las siguientes igualdades:\n\n\\(P(X=x)=F_{X}(x)-F_{X}(x^{-})\\).\n\\(P(a&lt; X&lt; b)=F_{X}(b^{-})-F_{X}(a)\\).\n\\(P(a\\leq X&lt; b)=F_{X}(b^{-})-F_{X}(a^{-})\\).\n\\(P(X&lt;a)=F_{X}(a^{-})\\).\n\\(P(a\\leq X\\leq b)=F_{X}(b)-F_{X}(a^{-})\\).\n\\(P(X\\geq a)=1-F_{X}(a^{-})\\).\nSi \\(F_X\\) es continua en \\(x\\) se tiene que \\(P(X=x)=0\\). Así que si la v.a. es continua \\(P(X\\leq a)=P(X&lt; a)+P(X=a)=P(X&lt;a)\\) y propiedades similares.\nSea \\(X\\) una variable aleatoria discreta que con dominio \\(D_X\\) y que tiene por función de probabilidad \\(P_{X}(x)\\) entonces su función de distribución \\(F_{X}(x_0)\\) es \\[F_{X}(x_0)=\\sum_{x\\leq x_0} P_{X}(x),\\] donde \\(\\sum\\limits_{x\\leq x_0}\\) indica que sumamos todos los \\(x \\in D_X\\) tales que \\(x\\leq x_0\\).\n\n\nDemostración:\nSi \\(X\\) es continua \\[P(X=a)=F(a)-F(a^{-})=F(a)-F(a)=0,\\] por lo tanto \\[P(X\\leq a)=P(X&lt;a)+P(X=a)= P(X&lt;a)+0= P(X&lt;a),\\] lo que demuestra la primera propiedad.\nPara demostrar la segunda basta hacer \\[\nF_{X}(x_0)= P(X\\leq x_0)=P\\left(\\bigcup_{x\\leq\nx_0; x\\in D_X} \\{x\\}\\right)= \\sum_{x\\leq x_0}P(X=x)= \\sum_{x\\leq x_0}P_{X}(x).\n\\]\n\n\nEjemplo: dado (continuación)\nEn el experimento del dado se tiene que:\n\\[P_{X}(x)=\\left\\{\\begin{array}{ll} \\frac16, & \\mbox{si } x=1,2,3,4,5,6,\\\\ 0, & \\mbox{en el resto de casos.}\\end{array}\\right.,\\]\npor lo tanto, \\[F_{X}(x)=P(X\\leq x)=\\left\\{\\begin{array}{ll}\n   0, & \\mbox{si } x&lt;1,\\\\[1ex]\n   \\frac16, &\\mbox{si } 1\\leq x&lt;2,\\\\[1ex]\n   \\frac26, &\\mbox{si } 2\\leq x&lt;3,\\\\[1ex]\n   \\frac36, &\\mbox{si } 3\\leq x&lt;4,\\\\[1ex]\n   \\frac46, &\\mbox{si } 4\\leq x&lt;5,\\\\[1ex]\n   \\frac56, &\\mbox{si } 5\\leq x&lt;6,\\\\[1ex]\n   1, &\\mbox{si } 6\\leq x.\\end{array}\\right.\\]\nCalculemos más detalladamente algún valor de \\(F_{X}\\), por ejemplo:\n\\[\\begin{eqnarray*}\nF_{X}(3.5) & = & P(X\\leq 3.5)=  P(\\{X=1\\}\\cup\\{X=2\\}\\cup \\{X=3\\})\\\\\n&=& P(\\{X=1\\})+P(\\{X=2\\})+P(\\{X=3\\})\\\\\n&=& \\frac16+\\frac16+\\frac16=\\frac36 =\\frac12,\n\\end{eqnarray*}\\] o de otra forma, \\[\\begin{eqnarray*}\nF_{X}(3.5)&=&\\sum_{x\\leq 3.5} P_X(x)=\\sum_{x=1}^3 P(X=x)\\\\&=&\\sum_{x=1}^3 \\frac16= 3 \\cdot\n   \\frac16=\\frac12.\n\\end{eqnarray*}\\]\n\nPropiedades de la función de distribución\nSea \\(X\\) una variable con función de distribución \\(F_{X}\\) entonces:\n\n\\(0\\leq F_{X}(x)\\leq 1\\), para todo \\(x\\).\nSi \\(x&lt;x'\\), entonces \\(F_{X}(x)\\leq F_{X}(x'),\\) es decir, es una función creciente, no necesariamente estrictamente creciente.\n\\(\\displaystyle \\lim_{x\\to -\\infty}F_{X}(x)=0\\) y \\(\\displaystyle \\lim_{x\\to +\\infty}F_{X}(x)=1\\).\nEs continua por la derecha: \\(\\displaystyle \\lim_{x\\to x_0^{+}}F_{X}(x)=F_{X}(x_0)\\).\n\n\n\n3.2.2 Valores esperados o esperanza\nAl igual que en la estadística descriptiva se utilizan distintas medidas para resumir los valores centrales y para medir la dispersión de una muestra, podemos definir las correspondientes medidas para variables aleatorias.\nA estas medidas se les suele añadir el adjetivo poblacionales mientras que a las que provienen de la muestra se las adjetiva como muestrales.\nPor ejemplo, podemos buscar un valor que resuma toda la variable. Este valor es el que “esperamos” que se resuma la v.a. o esperamos que las realizaciones de la v.a. queden cerca de él. Veamos su definición formal.\nEsperanza de una variable aleatoria discreta \nEl valor esperado o esperanza (expected value en inglés) \\(E(X)\\) de una v.a. discreta \\(X\\), se define como\n\\[\nE(X)=\\sum_{x\\in X(\\Omega)} x P_{X}(x).\n\\]\nEn ocasiones se denomina media (mean en inglés) poblacional o simplemente media y muy frecuentemente se la denota \\(\\mu_{X}=E(X)\\) o simplemente \\(\\mu=E(X)\\).\n\nEjemplo: lanzamiento de un dado \\(n\\) veces\nSupongamos que lanzamos un dado \\(n\\) veces y obtenemos unas frecuencias absolutas \\(n_{i}\\) para el resultado \\(i\\) con \\(i=1,\\ldots,6\\). Sea \\(X\\) la v.a. que nos representa el valor de una tirada del dado.\nCalculemos la media aritmética (o media muestral) de los datos\n\\[\n\\overline{x}=\\frac{1\\cdot n_1+2\\cdot  n_2+3\\cdot  n_3+4\\cdot  n_4+5\\cdot  n_5+6 \\cdot\nn_6}{n}=\\sum_{x=1}^6 x\\cdot \\frac{n_{x}}{n}.\n\\] Si \\(n\\to \\infty\\) se tiene que \\(\\displaystyle\\lim_{n\\to \\infty} \\frac{n_{x}}{n}=P_{X}(x).\\)\nPor lo tanto \\(E(X)=\\displaystyle \\lim_{n\\to\\infty}\\sum_{x=1}^6 x\\cdot \\frac{n_{x}}{n}.\\)\nEntonces el valor esperado en una v.a. discreta puede entenderse como el valor promedio que tomaría una v.a. en un número grande de repeticiones.\n\n\nEjemplo: Erratas en un texto\nSea \\(X\\) el número de erratas en una página de un texto, con dominio \\(D_X=\\{0,1,2\\}\\).\nResulta que\n\\[\nP(X=0)=0.42,\\ P(X=1)=0.4,\\ P(X=2)=0.18.\n\\]\nentonces\n\\[\nE(X)=0\\cdot 0.42+ 1\\cdot 0.4 + 2 \\cdot 0.18=0.76.\n\\]\nElegida una página del texto al azar esperamos encontrar \\(0.76\\) errores por página.\nSupongamos que el editor nos paga \\(2\\) euros por cada página que encontremos con \\(1\\) error y \\(3\\) euros por cada página con dos errores (y nada por las páginas correctas) ¿Cuánto esperamos cobrar si analizamos una página?\n\\[0\\cdot 0.42 + 2\\cdot 0.4 + 3\\cdot 0.18=1.34.\\]\n\n Esperanzas de funciones de variables aleatorias discretas \nSea \\(X\\) una v.a. discreta con función de probabilidad \\(P_{X}\\) y de distribución \\(F_{X}\\). Entonces el valor esperado de una función \\(g(x)\\) es :\n\\[E(g(X))=\\sum_{x}g(x)\\cdot P_{X}(x).\\]\nPropiedades de los valores esperados\n\n\\(E(k)=k\\) para cualquier constante \\(k\\).\nSi \\(a\\leq X\\leq b\\) entonces \\(a\\leq E(X)\\leq b\\).\nSi \\(X\\) es una v.a. discreta que toma valores enteros no negativos entonces \\(E(X)=\\sum_{x=0}^{+\\infty}(1- F_X(x)).\\)\n\n\nEjercicio\nLa demostración de las propiedades anteriores se deja como ejercicio.\n\n\nEjemplo: paleta de colores aleatoria\nSupongamos que estamos sentados delante de nuestro ordenador con un amigo y le decimos que en dos minutos podemos programar una paleta para poner colores a unos gráficos.\nQueremos que la paleta tenga dos botones con las opciones color rojo y color azul. Como hemos programado a gran velocidad resulta que el programa tiene un error; cada vez que se abre la paleta los colores se colocan al azar (con igual probabilidad) en cada botón, así que no sabemos en qué color hemos de pinchar.\nAdemás, como nos sobraron \\(15\\) segundos para hacer el programa y pensando en la comodidad del usuario, la paleta se cierra después de haber seleccionado un color y hay que volverla a abrir de nuevo.\nLa pregunta es: ¿cuál es el valor esperado del número de veces que hemos pinchar el botón de color azul antes de obtener este color?\n\nLlamemos \\(X\\) al número de veces que pinchamos en el botón azul (y nos sale rojo) hasta obtener el primer azul. La variable \\(X\\) toma valores en los enteros no negativos. Su función de probabilidad queda determinada por\n\\[\nP_X(x)=P(X=x)=P(\\stackrel{x \\mbox{ veces}}{\\overbrace{rojo, rojo,\\ldots,rojo},azul})\n=\\left(\\frac12\\right)^{x+1}.\n\\]\n\n\nSeries geométricas\nUna progresión geométrica de razón \\(r\\) es una sucesión de la forma\n\\[\nr^0, r^1,\\ldots,r^n,\\ldots.\n\\] La serie geométrica es la suma de todos los valores de la progresión geométrica \\(\\displaystyle\\sum_{k=0}^{+\\infty} r^k\\).\nPropiedades\n\nLas sumas parciales desde el término \\(n_0\\) al \\(n\\) de una progresión geométrica valen \\[\n\\sum_{k=n_0}^n r^k=\\frac{r^{n_0}- r^n r}{1-r}.\n\\]\nSi \\(|r|&lt;1\\) la serie geométrica es convergente y \\[\\sum_{k=0}^{+\\infty }\nr^k=\\frac1{1-r}\\].\nEn el caso en que se comience en \\(n_0\\) se tiene que \\[\\sum_{k=n_0}^{+\\infty} r^k=\\frac{r^{n_0}}{1-r}.\\]\nSi \\(|r|&lt;1\\) también son convergentes las derivadas, respecto de \\(r\\), de la serie geométrica y convergen a la derivada correspondiente. Así tenemos que\n\n\\[\\begin{eqnarray*}\n\\left(\\sum_{k=0}^{+\\infty} r^k\\right)'= & \\sum_{k=1}^{+\\infty}k\nr^{k-1} =  \\left(\\frac1{1-r}\\right)'=\\frac1{(1-r)^2}\\\\\n\\left(\\sum_{k=0}^{+\\infty} r^k\\right)^{''}=& \\sum_{k=2}^{+\\infty}k (k-1)\nr^{k-2} = \\left(\\frac1{1-r}\\right)^{''}=\\frac2{(1-r)^3}.\n\\end{eqnarray*}\\]\n\nEjemplo: paleta de colores (continuación)\nSi seguimos con el ejemplo de la paleta de colores, su esperanza es:\n\\[\\begin{eqnarray*}\nE(X)&=&\\sum_{x=0}^{+\\infty} x\\cdot P(X=x)=\\sum_{x=0}^{+\\infty} x\\cdot\n\\left(\\frac12\\right)^{x+1}\\\\\n&= & \\left(\\frac12\\right)^2\\sum_{x=1}^{+\\infty} x\\cdot\n\\left(\\frac12\\right)^{x-1}=\\left(\\frac12\\right)^2\n\\frac1{\\left(1-\\frac12\\right)^2}=1.\n\\end{eqnarray*}\\]\nAhora calculemos su función de distribución\n\\[\\begin{eqnarray*}\nF_X(x)&=& P(X\\leq x)=\\sum_{k=0}^x P(X=k)=\\sum_{k=0}^x\n\\left(\\frac12\\right)^{k+1}\\\\\n&=& \\frac{\\frac12-\\frac12^{x+1}\\cdot\n\\frac12}{1-\\frac12}=1-\\left(\\frac12\\right)^{x+1}.\n\\end{eqnarray*}\\]\nComo la variable toma valores enteros positivos, podemos calcular su valor esperado de esta otra manera\n\\[E(X)=\\sum_{x=0}^{+\\infty} (1-F_X(x))=\\sum_{x=0}^{+\\infty}\\left(\\frac12\\right)^{x+1}=\\frac12\\cdot\n\\frac1{1-\\frac12}=1.\\]\n\n\nEjercicio\nCalculad el valor esperado de la variable\n\\[\nY=\\mbox{número de intentos para conseguir el color azul.}\n\\]\n\n Momentos de orden \\(m\\)\nLlamaremos momento de orden \\(m\\) respecto al punto \\(C\\) a \\[E\\left((X-C)^m\\right).\\]\n\nCuando \\(C=0\\) los momentos reciben el nombre de momentos respecto al origen.\nCuando \\(C=E(X)\\) reciben el nombre de momentos centrales o respecto de la media. Luego la esperanza es el momento de orden \\(1\\) respecto al origen. Estos momentos son la versión poblacional de los momentos que vimos en el curso de estadística descriptiva, recibiendo estos último el nombre de momentos muestrales.\n\nResumen de conceptos\n\nHemos descrito el comportamiento aleatorio de una v.a. discreta mediante sus funciones de probabilidad \\(P_{X}\\) y de distribución \\(F_{X}\\).\nTambién tenemos un valor central; el valor esperado \\(E(X)\\).\nComo medida básica nos queda definir una medida de lo lejos que están los datos del valor central, \\(E(X)\\). Una de estas medidas es la varianza de \\(X\\).\n\n\n\n3.2.3 Medidas de la variabilidad\n Varianza \nSea \\(X\\) una v.a. Llamaremos varianza de \\(X\\) a\n\\[Var(X)=E((X-E(X))^2).\\]\nPor lo tanto, la varianza es el momento central de orden \\(2\\).\nDe forma frecuente se utiliza la notación \\[\\sigma_{X}^2=Var(X).\\]\nA la raíz cuadrada positiva de la varianza \\[\\sigma_{X}=+\\sqrt{Var(X)},\\]\nse la denomina desviación típica o estándar de \\(X\\).\n Propiedad \n\nSi \\(X\\) es una v.a. discreta con función de probabilidad \\(P_X\\) su varianza es \\[\\sigma_{X}^2=Var(X)=E((X-E(X))^2)=\\sum_{x}(x-E(X))^2 \\cdot P_{X}(x).\\]\nSea \\(X\\) una v.a. \\[Var(X)=E(X^2)-(E(X))^2=\\sum_{x} x^2\\cdot  P_{X}(X)-(E(X))^2\\]\n\n\nDemostración de b)\n\\[\\begin{eqnarray*}\nVar(X)&= & \\sum_{x}(x-E(X))^2 \\cdot P_{X}(x) = \\sum_{x}(x^2 -2 x E(X)+(E(X)^2)\\cdot  P_{X}(x)\\\\\n&=& \\sum_{x}x^2 \\cdot  P_{X}(x) -  E(X)\\sum_{x}2 x\\cdot  P_{X}(x) + (E(X)^2)\\sum_{x} P_{X}(x)\\\\\n&=& E(X^2)- 2 E(X)\\cdot  E(X) + (E(X))^2=E(X^2)-(E(X))^2.\n\\end{eqnarray*}\\]\n\n\nEjemplo: número de errores (continuación)\nCalculemos en el ejemplo anterior la varianza del número de errores.\n\nRecordemos que:\n\\[\nP(X=0)=0.42,\\  P(X=1)=0.4, \\  P(X=2)=0.18,\n\\] y que \\[\nE(X)=0.76.\n\\] Entonces:\n\\[\nVar(X)=E(X^2)-(E(X))^2 = E(X^2)-(0.76)^2.\n\\] Ahora necesitamos calcular\n\\[E(X^2)= 0^2 (0.41)+ 1^2 (0.4)+ 2^2 (0.18)=0.4+0.72=1.12,\\] y por lo tanto\n\\[Var(X)= E(X^2)-(0.76)^2=1.12-0.5776=0.542,\\] y \\[\\sqrt{Var(X)}=\\sqrt{0.542}.\\]\nEn resumen \\(\\sigma_{X}^2=0.542\\) y \\(\\sigma_{X}=\\sqrt{0.542}.\\)\n\n\nMás propiedades de la varianza\n\n\\(Var(X)\\geq 0\\).\n\\(Var(cte)=E(cte^2)-(E(cte))^2= cte^2 - cte^2=0\\).\nEl mínimo de \\(E((X-C)^2)\\) se alcanza cuando \\(C=E(X)\\) y es \\(Var(X)\\). Esta propiedad es una de las que hace útil a la varianza como medida de dispersión.\n\n\nEjercicio\nSe deja como ejercicio la demostración de estas propiedades.\n\n\n\n3.2.4 Transformaciones lineales.\n Transformación lineal \nUn cambio de variable lineal o transformación lineal de una v.a. \\(X\\) es otra v.a. \\(Y= a+ b X\\) donde \\(a,b\\in\\mathbb{R}\\).\n Esperanza de una transformación lineal\nSea \\(X\\) una v.a. con \\(E(X)=\\mu_{X}\\) y \\(Var(X)=\\sigma_{X}^2\\) y \\(a,b\\in\\mathbb{R}\\). Entonces si \\(Y=a+b X\\):\n\n\\(E(Y)=E(a + b X)=a+ b\\cdot E(X)= a + b\\cdot \\mu_{X}\\).\n\\(Var(Y)=Var(a+bX)=b^2\\cdot Var(X)= b^2 \\cdot \\sigma_{X}^2\\).\n\\(\\sigma_{Y}=\\sqrt{Var(Y)}=\\sqrt{b^2\\cdot Var(X)}=|b|\\cdot \\sigma_{X}\\).\n\n\nDemostración:\n\\[\\begin{eqnarray*}\nE(Y)&=& E(a+bX)=\\sum_{x}(a+b\\cdot X)\\cdot P_{X}(x)\\\\\n&=& a \\sum_{x} P_{X}(x) + b \\sum_{x} x\\cdot P_{X}(x)\\\\\n&=& a + b\\cdot E(X)=a + b \\mu_{X}.\n\\end{eqnarray*}\\]\n\n\nEjercicio\nLas demostración de las demás propiedades se dejan como ejercicio."
  },
  {
    "objectID": "2.html#variables-aleatorias-continuas",
    "href": "2.html#variables-aleatorias-continuas",
    "title": "3  Variables Aleatorias",
    "section": "3.3 Variables aleatorias continuas",
    "text": "3.3 Variables aleatorias continuas\nComo ya hemos dicho las variables aleatorias continuas toman valores en intervalos o áreas.\nLo más habitual es que estas variables tengan función de distribución continua y derivable (salvo a los mejor en una cantidad finita o numerable de puntos).\nEn lo que sigue supondremos que la función de distribución de variables aleatorias continuas cumplen estas propiedades.\nNotemos que si \\(X\\) es una v.a. con función de distribución continua se tiene que \\(P(X=x_0)=F_X(x_0)-F(x_0^{-})=0\\). Por lo que no tiene sentido definir función de probabilidad.\nEn general tendremos que \\(P(X&lt;x_0)=P(X\\leq x_0)\\).\nPor otra parte podemos utilizar una regla parecida del cociente entre casos favorables y casos posibles de Laplace pero en este caso el conteo se hace por la medida de los casos posibles partida por la medida de los casos favorables.\nVeamos un ejemplo de v.a. continua, que ampliaremos en el tema siguiente, en el que se utilizan todos estos conceptos.\n\nEjemplo: distancia de un dardo al centro de la diana\nSupongamos que lanzamos un dardo a una diana de radio \\(1\\), de forma que sea equiprobable cualquier distancia al centro (¡Cuidado! esto no es equivalente a que cualquier punto de la diana sea equiprobable).\nConsideremos la v.a. continua \\(X=\\) distancia del dardo al centro de la diana.\n\nSu función de distribución es\n\\[\nF_{X}(x)=\n\\left\\{\n\\begin{array}{ll}\n0, & \\mbox{si } x\\leq 0,\\\\\nx, & \\mbox{si } 0&lt;x&lt;1,\\\\\n1, & \\mbox{si } x\\geq 1.\n\\end{array}\n\\right.\n\\]\n\nC.F. longitud favorable es \\(x-0\\).\nC.P. longitud posible es \\(1-0\\).\nLuego \\[P(X\\leq x)=\\frac{C.F.}{C.P.}=\\frac{x-0}{1-0}=x.\\]\n\n\n\n\ncurve(punif(x,0,1),xlim=c(-1,2),col=\"blue\",\n      main=\"Función de distribución de una v.a. uniforme en el intervalo unidad.\")\n\n\n\n\n\n\n\n\n\n3.3.1 Propiedades\nEn las variables continuas los sucesos del tipo \\(\\{X\\leq x \\}\\) y \\(\\{X&lt; x \\}\\) tendrán la misma probabilidad, y otros tipos de sucesos similares también. Algunas de estas propiedades se explicitan en la siguiente proposición.\nPropiedades\nDada una v.a. continua \\(X\\) se tiene que:\n\n\\(P(X\\leq b)=P(X&lt;b)\\).\n\\(P(X&lt;b)=P(X&lt;a)+P(a&lt;X&lt;b)\\).\n\\(P(a&lt;X&lt;b)=P(X&lt;b)-P(X&lt;a)\\).\n\n\nDemostración:\nLa primera es evidente \\(P(X\\leq b)=P(X&lt;b)+P(X=b)=P(X&lt;b)\\).\nPara demostrar la segunda, tenemos\n\\[\\{X\\leq a\\}\\cap \\{a&lt;X&lt;b\\}=\\emptyset,\\] \\[\\{X\\leq a\\}\\cup \\{a&lt;X&lt;b\\}=\\{X&lt;b\\},\\] entonces \\[\\begin{eqnarray*}\nP(X&lt; b) & = & P(\\{X\\leq a\\}\\cup \\{a&lt;X&lt;b\\})\\\\\n& = & P(X\\leq a)+P(a&lt;X&lt;b) \\\\\n& = & P(X&lt; a)+P(a&lt;X&lt;b).\n\\end{eqnarray*}\\]\n\n\nEjercicio\nLa demostración de la tercera propiedad es similar a la segunda pero aplicando la primera. La dejamos como ejercicio.\n\nPropiedades de la función de distribución\nLas propiedades anteriores y combinaciones de ellas se pueden escribir utilizando la función de distribución de \\(X\\):\n Propiedades de la Función de Distribución \nDada una variable aleatoria continua se tiene que:\n\n\\(F_{X}(b)=F_{X}(a)+P(a&lt;X&lt;b)\\).\n\\(P(a&lt;X&lt;b)=F_{X}(b)-F_{X}(a)\\).\n\\(P(a\\leq X\\leq b)=F_{X}(b)-F_{X}(a)\\).\n\n\nEjercicio Se deja la demostración como ejercicio.\n\n\nEjemplo: diana (continuación)\nEn el ejemplo de la diana:\n\\[P(0.25&lt;X&lt;0.3)=F_{X}(0.3)-F_{X}(0.25)=0.3-0.25=0.05.\\]\n\n\n\n3.3.2 Función de densidad\n Función de densidad \nUna función \\(f:\\mathbb{R}\\to\\mathbb{R}\\) es una función de densidad sobre \\(\\mathbb{R}\\) si cumple que\n\n\\(f_{X}(x)\\geq 0\\) para todo \\(x \\in\\mathbb{R}.\\)\n\\(f\\) es continua salvo a lo sumo en una cantidad finita de puntos sobre cada intervalo acotado de \\(\\mathbb{R}\\).\n\\(\\displaystyle\\int\\limits_{-\\infty}^{+\\infty} f_{X}(x) dx=1.\\)\n\n Función de distribución de una variable aleatoria \nSea \\(X\\) una v.a. con función de distribución \\(F_X\\). Sea \\(f:\\mathbb{R}\\to\\mathbb{R}\\) una función de densidad tal que\n\\[F_X(x)=\\displaystyle\\int_{-\\infty}^{x} f_X(t)\\,dt,\\mbox{ para todo } x\\in\\mathbb{R}.\\]\nEntonces \\(X\\) es una variable aleatoria continua y \\(f_X\\) es la densidad de la v.a. \\(X\\).\nEl conjunto \\(D_X=\\{x\\in\\mathbb{R}| f_x(x)&gt;0\\}\\) recibe el nombre de  soporte o dominio de la variable aleatoria continua y se interpreta como su conjunto de resultados posibles.\n\nEjemplo: diana (continuación)\nEn nuestro ejemplo, la función \\(f\\) es una densidad\n\\[\nf_{X}(x)=\\left\\{\n\\begin{array}{ll}\n0, & \\mbox{si } x\\leq 0,\\\\\n1, & \\mbox{si } 0 &lt; x &lt; 1,\\\\\n0, & \\mbox{si } 1\\leq x,\n\\end{array}\\right.\n\\] que es la densidad de \\(X\\). En efecto:\n\nSi \\(x \\leq 0\\), entonces \\(\\displaystyle\\int_{-\\infty}^x f_X(t) dt = 0.\\)\nSi \\(0\\leq x\\leq 1\\), entonces \\(\\displaystyle\\int_{-\\infty}^x f_X(t) dt = \\int_0^x 1\\, dt = x.\\)\nSi \\(x\\geq 1\\), entonces \\(\\displaystyle\\int_{-\\infty}^x f_X(t) dt = \\int_0^1 1\\, dt = 1.\\)\n\nPor lo tanto, \\(F_X(x)=\\displaystyle\\int_{-\\infty}^x f_X(t) dt\\) para todo \\(x\\in\\mathbb{R}.\\)\n\ncurve(dunif(x,0,1),xlim=c(-0.5,1.5),col=\"blue\",\n      main=\"Densidad de la distribución uniforme en [0,1]\")\n\n\n\n\n\n\n\n\n\n\n\n3.3.3 Utilidad de la función de densidad\nLa función de densidad nos permite calcular diversas probabilidades.\nPropiedades de la función de densidad \n\nSea \\(X\\) una v.a. continua con función de distribución \\(F_X\\) y de densidad \\(f_X\\), entonces \\[\\begin{eqnarray*}\nP(a&lt; X&lt; b) &=&  P(a&lt;X\\leq b)= P(a\\leq X&lt; b)=\\\\\n& & P(a\\leq X\\leq b)= \\displaystyle\\int_{a}^b f_X(x) dx.\n\\end{eqnarray*}\\]\nSi \\(A\\) es un subconjunto de \\(\\mathbb{R}\\) entonces\n\n\\[\nP(X\\in A)=\\displaystyle\\int_{A} f(x) dx=\\displaystyle\\int_{A\\cap D_X} f(x) dx.\n\\]\nPropiedades de la función de densidad \nSea \\(X\\) una v.a. continua con función de distribución \\(F_X\\) y de densidad \\(f_X\\), entonces:\n\nSi \\(f_x\\) es continua en un punto \\(x\\), \\(F_X\\) es derivable en ese punto y \\(F_X'(x)=f_X(x).\\)\n\\(P(X=x)=0\\) para todo \\(x\\in\\mathbb{R}.\\)\n\n\nEjercicio\nComprobar estas propiedades en el ejemplo de la diana.\n\n\nEjemplo: tiempo ejecución de un proceso.\nSea \\(X=\\) tiempo de ejecución de un proceso. Se supone que \\(X\\) sigue una distribución uniforme en dos unidades de tiempo, si tarda más el proceso se cancela.\nCalculemos la función de densidad y de distribución de la v.a \\(X\\).\n\nEntonces\n\\[\nF_{X}(x)=P(X\\leq x)=\\frac{\\mbox{Casos Favorables}}{\\mbox{Casos Posibles}}=\\frac{x}2.\n\\]\nLuego su función de distribución es:\n\\[\nF_{X}(x)=\\left\\{\\begin{array}{ll}\n0, & \\mbox{si } x\\leq 0,\\\\[1ex]\n\\frac{x}2, & \\mbox{si } 0&lt;x&lt;2,\\\\[1ex]\n1, & \\mbox{si } 2\\leq x.\n\\end{array}\\right.\n\\]\nSu función de densidad por su lado es: \\[\nf_{X}(x)=F_{X}'(x)=\\left\\{\\begin{array}{ll}\n0, & \\mbox{si } x\\leq 0,\\\\[1ex]\n\\frac12, & \\mbox{si } 0&lt;x\\leq 2,\\\\[1ex]\n0, & \\mbox{si } 2\\leq x.\n\\end{array}\\right.\n\\]\nEfectivamente\n\n\\(f_{X}(x)\\geq 0,\\) y tiene un conjunto finito de discontinuidades: \\(\\{0,2\\}\\).\n\\(F_X(x)=\\int_{-\\infty}^x f_X(t) dt,\\) para todo \\(x\\in \\mathbb{R}\\). (Ejercicio: resolverlo gráficamente.)\n\\(\\displaystyle\\int\\limits_{-\\infty}^{+\\infty}f_{X}(x)dx=\\int\\limits_0^2\\frac12dx=\\left[\\frac{x}2\\right]_0^2=\\frac22-\\frac02=1.\\)\n\n\n\n\nEjercicio: tiempo de un proceso\nCalcular la probabilidad de que uno de nuestros procesos tarde más de una unidad de tiempo en ser procesado. Calcular también la probabilidad de que dure entre \\(0.5\\) y \\(1.5\\) unidades de tiempo.\n\n\n\n3.3.4 Esperanza y varianza para variables aleatorias continuas\nLos mismos comentarios y definiciones que se dieron en la sección correspondiente del tema de estadística descriptiva son aplicables aquí.\nAsí que sólo daremos las definiciones, la forma de cálculo y algunos ejemplos.\nEn lo que sigue, salvo que diagamos lo contrario, \\(X\\) es una v.a. continua con función de densidad \\(f_{X}(x)\\)\n Esperanza v.a. continuas \n\nSu esperanza es: \\[E(X)=\\displaystyle\\int\\limits_{-\\infty}^{+\\infty} x\\cdot f_{X}(x)dx.\\]\nSi \\(g(x)\\) es una función de la variable \\(X\\) entonces: \\[E(g(X))=\\displaystyle\\int\\limits_{-\\infty}^{+\\infty} g(x)\\cdot f_{X}(x)dx.\\]\n\n Varianza v.a. continuas \n\nSu varianza es: \\[\nVar(X)=\\sigma_{X}^2=E((X-\\mu_{X})^2)=\n\\displaystyle\\int\\limits_{-\\infty}^{+\\infty} (x-\\mu_{X})^2 f_{X}(x)dx.\n\\]\nSu desviación típica es: \\[\\sigma_{X}=+\\sqrt{\\sigma_{X}^2}.\\]\n\n Propiedades \n\n\\(\\sigma_{X}^2\\geq 0\\).\n\\(Var(cte)=E(cte^2)-(E(cte))^2= cte^2 - cte^2=0\\).\n\\(\\displaystyle Var(x)=E(X^2)-\\mu_{X}^2=\\int\\limits_{-\\infty}^{+\\infty}x^2 f_{X}(x)dx - \\mu_{X}^2.\\)\nEl mínimo de \\(E((X-C)^2)\\) se alcanza cuando \\(C=E(X)\\) y es \\(Var(X)\\).\n\n\nEjemplo: diana (continuación)\nCalcular \\(\\mu_{X}\\) y \\(\\sigma_{X}^2\\) en el ejemplo de la diana.\n\nResultado \\[\\mu_{X}=\\frac12,\\ E(X^2)=\\frac13,\\ Var(X)=\\frac1{12}.\\]\n\n\nProposición\nSea \\(X\\) una v.a. continua con \\(E(X)=\\mu_{X}\\) y \\(Var(X)=\\sigma_{X}^2\\) sea \\(Y=a+b\\cdot X\\), donde \\(a,b\\in\\mathbb{R}\\), es una nueva v.a. continua obtenida mediante una transformación lineal de \\(X\\). Se verifican las mismas propiedades que en el caso discreto:\n\n\\(E(Y)=E(a+b\\cdot X)=a+b\\cdot E(X)\\).\n\\(Var(Y)=Var(a+b\\cdot X)=b^2\\cdot Var(X)\\).\n\\(\\sigma_{Y}=|b|\\cdot \\sigma_{X}\\).\n\\(Z=\\frac{X-\\mu_{X}}{\\sigma_{X}}\\) es una transformación lineal de \\(X\\) de forma que \\[E(Z)=0 \\mbox{ y } Var(Z)=1.\\]\n\n\nEjemplo: venta de vinos\nEn una empresa de venta de vinos por internet, sea \\(X\\) el número de litros de vino del país vendidos en un año. Supongamos que sabemos que \\(E(X)=10000\\) y que \\(Var(X)=100\\). Supongamos que los gastos fijos de distribución son 50.000 € y el beneficio por litro es de 10 € por botella. Definimos \\(T=10 X-50000\\) que será el beneficio después de gastos.\n\nEntonces la esperanza del beneficio es \\[E(T)=10 E(X)-50000 = 50000,\\] y \\[Var(T)=10^2 Var(X)= 10000.\\]"
  },
  {
    "objectID": "2.html#transformaciones-de-variables-aleatorias",
    "href": "2.html#transformaciones-de-variables-aleatorias",
    "title": "3  Variables Aleatorias",
    "section": "3.4 Transformaciones de variables aleatorias",
    "text": "3.4 Transformaciones de variables aleatorias\nMuchas variables aleatorias son funciones de otras v.a. En lo que sigue resumiremos diversas técnicas para dada una v.a. \\(X\\) y una transformación \\(Y=h(X)\\) encontrar \\(F_{Y}\\) a partir de \\(F_{X}\\).\nTranformaciones de v.a. discretas \nSea \\(X\\) una v.a. discreta con \\(X(\\Omega)=\\{x_1,x_2,\\ldots,x_{n},..\\}\\) y sea \\(h:\\mathbb{R}\\to\\mathbb{R}\\) una aplicación. Entonces \\(Y=h(X)\\) es también una v.a. discreta. Además si \\(P_X\\) y \\(F_{X}\\) son las funciones de probabilidad y de distribución de \\(X\\) entonces\n\n\\(\\displaystyle P_{Y}(y)=\\sum_{x_{i}|h(x_{i})=y}P_X(x_{i}).\\)\n\\(\\displaystyle F_{Y}(y)=\\sum_{x_{i}|h(x_{i})\\leq y} P_X(x_{i}).\\)\n\nDesafortunadamente para variables no discretas, el resultado no es tan sencillo como la expresión anterior, pues la transformación de, por ejemplo, una v.a. continua puede ser continua, discreta, mixta, \\(\\ldots\\)\nTransformación de v.a. continuas en continuas\nSea \\(X\\) una v.a. continua cuya función de densidad es \\(f_{X}\\). Sea \\(h:\\mathbb{R}\\to\\mathbb{R}\\) una aplicación estrictamente monótona y derivable; por lo tanto, \\(h'(x)\\not=0\\) para todo \\(x\\in\\mathbb{R}\\). Sea \\(Y=h(X)\\) la transformación de \\(X\\) por \\(h\\). Entonces \\(Y\\) es una v.a. continua con función de densidad\n\\[f_{Y}(y)=\\left.\\frac{f_{X}(x)}\n{\\left|h'(x)\\right|}\\right|_{x=h^{-1}(y)}.\\]\nDensidad de una transformación de una v.a. continua\nSea \\(X\\) una v.a. continua cuya función de densidad es \\(f_{X}\\). Sea \\[h:\\mathbb{R}\\to\\mathbb{R},\\] una aplicación, no necesariamente monótona tal que :\n\nsea derivable con derivada no nula,\nla ecuación \\(h(x)=y\\) tiene un número finito de soluciones \\(x_1,x_2,..,x_{n}\\),\n\nentonces:\n\\[\n\\displaystyle f_{Y}(y)=\\left.\\sum_{k=1}^{n} \\frac{f_{X}(x)}\n{\\left|h'(x)\\right|}\\right|_{x=x_{k}}.\n\\]\nMétodo general de transformación de v.a.\nCuando no podamos aplicar las propiedades anteriores intentaremos calcular primero la función de distribución de la transformación y luego su densidad.\nNotemos que en general si \\(Y=g(X)\\) es una v.a. transformación de la v.a. \\(X\\) entonces\n\\[\nF_{Y}(y)=P(Y\\leq y)=P(g(X)\\leq y).\n\\]\nPor ejemplo, si \\(g\\) es estrictamente creciente y continua,\n\\[\nF_{Y}(y)=P(g(X)\\leq y)=P(X\\leq g^{-1}(y))=F_{X}(g^{-1}(y)),\n\\] y si \\(g\\) es estrictamente decreciente y continua, \\[\nF_{Y}(y)=P(g(X)\\leq y)=P(X\\geq g^{-1}(y))=1-F_{X}(g^{-1}(y)).\n\\]"
  },
  {
    "objectID": "2.html#desigualdades-de-markov-y-de-chebychev",
    "href": "2.html#desigualdades-de-markov-y-de-chebychev",
    "title": "3  Variables Aleatorias",
    "section": "3.5 Desigualdades de Markov y de Chebychev",
    "text": "3.5 Desigualdades de Markov y de Chebychev\nEn esta sección veremos distintas desigualdades que acotan determinadas probabilidades de una variable aleatoria.\nEstas desigualdades sirven en algunos casos para acotar probabilidades de determinados sucesos.\nTambién son útiles desde el punto de vista teórico, por ejemplo para justificar que la varianza es una medida de la dispersión de los datos.\n\n3.5.1 Desigualdad de Markov\nDesigualdad de Markov\nSea \\(X\\) una v.a. positiva con \\(E(X)\\) finita. Entonces\n\\[P(X\\geq a)\\leq \\frac{E(X)}{a},\\mbox{ para todo }a&gt;0.\\]\n\nDemostración:\nSi \\(X\\) es continua y solo toma valores positivos\n\\[\\begin{eqnarray*}\nE(X) &=& \\int_{-\\infty}^{+\\infty} x\\cdot  f_{X}(x) dx=  \\int_0^{+\\infty} x\\cdot f_{X}(x) dx=  \\int_0^{a} x\\cdot  f_{X}(x) dx +\\int_{a}^{+\\infty} x\\cdot f_{X}(x) dx \\\\\n& &\\geq   \\int_{a}^{+\\infty} x\\cdot\nf_{X}(x) dx \\geq a \\int_{a}^{+\\infty}\nf_{X}(x) dx = a \\cdot  P(X\\geq a),\n\\end{eqnarray*}\\] de donde se sigue que \\[P(X\\geq a)\\leq \\frac{E(X)}{a}.\\]\n\n Corolario\nSea \\(X\\) una v.a. con \\(E(X)\\) finita entonces para todo \\(a&gt;0\\)\n\\[P(|X|\\geq a )\\leq \\frac{E(|X|)}{a}.\\]\n\nEjercicio\nDemuestra el corolario anterior a partir de la desigualdad de Markov.\n\n\n\n3.5.2 Desigualdad de Chebychev\nDesigualdad de Chebychev\nLa desigualdad de Chebychev también se denomina de Chebyshov y en inglés Chebyshev.\nSea \\(X\\) una v.a. con \\(E(X)=\\mu\\) y \\(Var(X)=\\sigma^2\\) entonces para todo \\(a&gt;0\\),\n\\[P(|X-\\mu|\\geq a)\\leq \\frac{\\sigma^2}{a^2}.\\]\n\nDemostración\nApliquemos la consecuencia de la desigualdad de Markov a la v.a. no negativa \\(Y^2=(X-\\mu)^2\\). Entonces\n\\[\nP(Y^2\\geq a^2) \\leq\n\\frac{E(Y^2)}{a^2}=\\frac{E((X-\\mu)^2)}{a^2}\n= \\frac{Var(X)}{a^2}=\\frac{\\sigma^2}{a^2}.\n\\] Por otra parte\n\\[\nP(Y^2\\geq a^2)=P(|Y|\\geq a)= P(|X-\\mu|\\geq a),\n\\] hecho que, junto con la desigualdad anterior, demuestra el resultado.\n\n Utilidad básica de la desigualdad de Chebychev\nSupongamos que \\(X\\) es una v.a. con \\(Var(X)=0\\). Entonces, aplicando la desigualdad anterior, \\[P(|X-E(X)|\\geq a )=0,\\mbox{ para todo }a&gt;0,\\] lo que implica que \\[P(X=E(X))=1,\\] por lo que probabilidad de que \\(X\\) sea constantemente \\(E(X)\\) es 1, hecho que nos confirma la utilidad de la varianza como una medida de la dispersión de los datos.\n\nEjemplo: tiempo de respuesta\nSe sabe que el tiempo de respuesta medio y la desviación típica de un sistema multiusuario son 15 y 3 unidades de tiempo, respectivamente. Entonces: \\[\nP(|X-15|\\geq 5)\\leq \\frac9{25}=0.36.\n\\]\n\nSi substituimos \\(a\\) por \\(a\\cdot \\sigma\\) en la desigualdad de Chebychev, nos queda:\n\\[\nP(|X-\\mu|\\geq a \\sigma)\\leq\n\\frac{\\sigma^2}{(a\\sigma)^2}=\\frac1{a^2},\n\\] que es otra manera de expresar la desigualdad de Chebychev.\nMás formas de la desgualdad de Chebychev\nLa desigualdad de Chebychev también se puede escribir de al menos dos maneras más:\n\\[\nP(\\mu-a\\leq X\\leq \\mu+a)\\geq 1-\\frac{\\sigma^2}{a^2},\n\\] y tomado como \\(a=k\\cdot \\sigma\\), \\[\nP(\\mu-k\\cdot \\sigma\\leq X\\leq \\mu+ k \\cdot \\sigma)\\geq 1-\\frac1{k^2}.\n\\]\nTomando la segunda expresión que hemos visto para la desigualdad de Chebychev para distintos valores de \\(k&gt;0\\), tenemos la siguiente tabla:\n\n\n\nk\n\\(P(|X-E(X)|\\geq k \\cdot \\sigma)\\)\n\n\n\n\n1\n\\(\\leq 1\\)\n\n\n2\n\\(\\leq 0.25\\)\n\n\n3\n\\(\\leq 0.111\\)\n\n\n4\n\\(\\leq 0.0025\\)\n\n\n\nPor ejemplo para \\(k=2\\), esta desigualdad se puede interpretar como que, dada una v.a. \\(X\\) con cualquier distribución que tenga \\(E(X)\\) y \\(Var(X)\\) finitos, la probabilidad de que un valor se aleje de la media \\(\\mu\\) más de \\(a=2\\) desviaciones típicas es menor o igual que \\(0.25\\).\nEs decir sólo el 25% de los valores estarán alejados de la media más de \\(2\\sigma\\), ¡sea cual sea la distribución de la v.a.!"
  },
  {
    "objectID": "2.html#cuantiles-de-variables-aleatorias",
    "href": "2.html#cuantiles-de-variables-aleatorias",
    "title": "3  Variables Aleatorias",
    "section": "3.6 Cuantiles de variables aleatorias",
    "text": "3.6 Cuantiles de variables aleatorias\nSi \\(X\\) es una v.a. con dominio \\(D_X\\) y \\(0&lt;p&lt;1\\), llamaremos cuantil de orden \\(p\\) al menor valor perteneciente al dominio \\(x_p\\in D_X\\) tal que: \\[P(X\\leq x_p)\\geq p.\\]\nEn R, cada distribución \\(X\\) tiene la función qX(p,...) que devuelve precisamente el cuantil \\(x_p\\) tal que \\(P(X\\leq x_p)\\geq p.\\)\nDada una variable aleatoria \\(X\\), si existe la inversa de la función de distribución de \\(X\\), \\(F_X^{-1}\\), el cuantil de orden \\(p\\) sería el valor que tiene la función \\(F_X^{-1}\\) en \\(p\\): \\(x_p=F^{-1}(p)\\).\nEn caso de no existir la inversa, dado \\(p\\), definimos el conjunto \\(A_p\\) como:\n\\[\nA_p =\\{x\\in\\mathbb{R},\\ |\\ F_X(x)\\geq p\\}.\n\\]\nEntonces el cuantil \\(p\\) es el mínimo del conjunto \\(A_p\\) considerando sólo valores del dominio de la variable: \\(x_p =\\displaystyle\\min_{x\\in D_X}(A_p)\\). Este mínimo siempre existirá y nos da una fórmula explícita para calcular los cuantiles de cualquier variable aleatoria.\n\nEjemplo: variable aleatoria que nos da el resultado del lanzamiento de un dado\nSea \\(X\\) la variable aleatoria uniforme discreta que nos da el número de puntos obtenidos en el lanzamiento de un dado (seis caras numeradas del 1 al 6).\nSu dominio es \\(D_X=\\{1,2,3,4,5,6\\}\\) y su función de probabilidad es \\[\nP_X(x)=P(X=x)=\n\\left\\{\n\\begin{array}{ll}\n\\frac{1}{6}, & \\mbox{ si } x=1,2,3,4,5,6, \\\\\n0, & \\mbox{ en otro caso. }.\n\\end{array}\n\\right.\n\\] Su función de distribución es: \\[\nF_X(x)= P(X\\leq x)=\n\\left\\{\n\\begin{array}{ll}\n0, & \\mbox{ si } x&lt;1, \\\\\n\\frac{k}{6} & \\mbox{ si } k\\leq x&lt; k+1 \\mbox{ para } x= 1,2,3,4,5, \\\\\n1, & \\mbox{si  } x \\geq 6.\n\\end{array}\n\\right.\n\\]\nLa función siguiente llamada ddado nos define la función de probabilidad de \\(X\\) para un dado de \\(n\\) caras:\n\nddado=function(x,n=6) {\n  sapply(x,FUN=function(x) {\n    if( x %in% c(1:n)){return(1/n)} else {return(0)}})\n  }\n\nPor ejemplo, el valor de \\(P_X(0.5)\\) sería:\n\nddado(0.5,n=6)\n\n[1] 0\n\n\ny los valores de \\(P_X(i)\\) para \\(i=1,\\ldots 10\\) sería:\n\nddado(1:10,n=6)\n\n [1] 0.1666667 0.1666667 0.1666667 0.1666667 0.1666667 0.1666667 0.0000000\n [8] 0.0000000 0.0000000 0.0000000\n\n\nLa función pdado nos da la función de distribución de \\(X\\):\n\npdado=function(x,n=6) \n  {\n  sapply(x,FUN=function(y){ if (y&lt;1){ return(0)}else{if(y&gt;=n){return(1)} else\n  {return(sum(ddado(c(1:(floor(y))),n=n)))}}})\n  }\n\nLos valores de \\(F_X(i)\\) para \\(i=0,\\ldots, 11\\) serían:\n\npdado(0:11,6)\n\n [1] 0.0000000 0.1666667 0.3333333 0.5000000 0.6666667 0.8333333 1.0000000\n [8] 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000\n\n\nA continuación, construímos la función qdado que nos calcula el cuantil \\(p\\), para \\(0\\leq p\\leq 1\\), de la variable \\(X\\) como el mínimo del conjunto \\(A_p\\):\n\nqdado=function(p,n=6){\nsapply(p,FUN=function(pp=p,nn=n) \n  {\n  if(pp&lt;0 | pp&gt;1) {return(NA)}\n  else {\n  aux=pp&gt;=pdado(1:n,nn)\n  aux\n  ifelse(all(!aux),return(1),return(max(which(pp&gt;=pdado(1:n,nn)))))}}\n)\n}\n\nSi \\(p=1.5\\) o \\(p=-1\\), la función anterior nos devuelve NA ya que ni 1.5 ni -1 están entre 0 y 1:\n\nqdado(1.5)\n\n[1] NA\n\nqdado(-1)\n\n[1] NA\n\n\nLos cuantiles \\(x_{0.1}\\), \\(x_{0.5}\\) \\(x_{0.6}\\) y \\(x_1\\) son:\n\nqdado(c(0.1,0.5,0.6,1))\n\n[1] 1 3 3 6"
  },
  {
    "objectID": "3.html#distribuciones-discretas",
    "href": "3.html#distribuciones-discretas",
    "title": "4  Distribuciones Notables",
    "section": "4.1 Distribuciones discretas",
    "text": "4.1 Distribuciones discretas\n\n4.1.1 Distribución de Bernoulli\nConsideremos un experimento con dos resultados posibles: éxito (E) y fracaso (F). El espacio de sucesos será \\(\\Omega=\\{E,F\\}\\).\nSupongamos que la probabilidad de éxito es \\(P(E)=p\\), y naturalmente \\(P(F)=1-p=q\\), con \\(0&lt;p&lt;1\\).\nConsideremos la aplicación\n\\[\nX:\\Omega=\\{E,F\\}\\to \\mathbb{R},\n\\] definida por \\(X(E)=1,\\ X(F)=0.\\)\nSu función de probabilidad es: \\[\nP_{X}(x)=\n\\left\\{\n\\begin{array}{ll} 1-p=q, & \\mbox{si } x=0,\\\\\np, & \\mbox{si } x=1,\\\\\n0, & \\mbox{en cualquier otro caso.}\n\\end{array}\n\\right.\n\\]\nSu función de distribución es: \\[\nF_{X}(x)=P(X\\leq x)=\n\\left\\{\n\\begin{array}{ll}\n0, & \\mbox{si } x&lt;0,\\\\\n1-p=q, & \\mbox{si } 0\\leq x &lt;1,\\\\\n1, & \\mbox{si } 1\\leq x. \\\\\n\\end{array}\n\\right.\n\\] Bajo estas condiciones, diremos que \\(X\\) es una v.a. Bernoulli o que sigue una ley de distribución de probabilidad Bernoulli de parámetro \\(p\\).\nLo denotaremos por \\(X\\equiv Ber(p)\\) o también \\(X\\equiv B(1,p).\\)\nA este tipo de experimentos (éxito/fracaso) se les denomina experimentos Bernoulli.\nFue su descubridor un científico suizo Jacob Bernoulli, uno más de la de la conocida familia de científicos suizos Bernoulli.\nEsperanza de una v.a. \\(X\\) \\(Ber(p)\\)\nSu valor esperado es: \\[E(X)=\\displaystyle\\sum_{x=0}^1 x\\cdot P(X=x)= 0\\cdot(1-p)+1\\cdot p=p.\\] Calculemos también \\(E(X^2)\\):\n\\[E(X^2)=\\displaystyle\\sum_{x=0}^1 x^2\\cdot P(X=x)= 0^2\\cdot(1-p)+1^2\\cdot p=p.\\] Varianza de una v.a. \\(X\\) \\(Ber(p)\\)\nSu varianza es:\n\\[Var(X)=E(X^2)-\\left(E(X)\\right)^2=p-p^2=p\\cdot (1-p)=p\\cdot q.\\] Su desviación típica es: \\[\n\\sqrt{Var(X)}=\\sqrt{p \\cdot (1-p)}.\n\\]\nResumen v.a con distribución Bernoulli\n\n\n\n\n\n\n\n\\(X\\) Bernoulli\n\\(Ber(p)\\)\n\n\n\n\n\\(D_X=\\)\n\\(\\{0,1\\}\\)\n\n\n\\(P_X(x)=P(X=x)=\\)\n\\(\\left\\{\\begin{array}{ll} q & \\mbox{si } x=0\\\\ p & \\mbox{si } x=1\\\\0 & \\mbox{en otro caso}\\end{array}\\right.\\)\n\n\n\\(F_X(x)=P(X\\leq x)=\\)\n\\(\\left\\{\\begin{array}{ll} 0 & \\mbox{ si } x&lt;0\\\\q & \\mbox{ si } 0\\leq x&lt;1\\\\1 & \\mbox{ si } 1\\leq x \\end{array}\\right.\\)\n\n\n\\(E(X)=p\\)\n\\(Var(X)=p\\cdot q\\)\n\n\n\n\nEjemplo de Distribución Bernoulli\nVeamos los cálculos básicos usando la distribución \\(Ber(p=0.25)\\) en R.\n\ndbinom(0,size=1,prob=0.25)\n\n[1] 0.75\n\ndbinom(1,size=1,prob=0.25)\n\n[1] 0.25\n\nrbinom(n=20,size = 1,prob=0.25)\n\n [1] 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n\n\nEl siguiente código dibuja las función de probabilidad y la de distribución de una \\(Ber(p=0.25)\\)\n\npar(mfrow=c(1,2))\nplot(x=c(0,1),y=dbinom(c(0,1),size=1,prob=0.25),\n     ylim=c(0,1),xlim=c(-1,2),xlab=\"x\",\n     main=\"Función de probabilidad\\n Ber(p=0.25)\")\nlines(x=c(0,0,1,1),y=c(0,0.75,0,0.25), type = \"h\", lty = 2,col=\"blue\")\ncurve(pbinom(x,size=1,prob=0.25),\n      xlim=c(-1,2),col=\"blue\",\n      main=\"Función de distribución\\n Ber(p=0.25)\")\npar(mfrow=c(1,1))\n\n\n\n\n\n\n\n\n\n\nGráficas interactivas \\(Ber(p)\\)\nPara ejecutar el siguiente gráfico interactivo, solamente tienes que cargar el paquete shiny en tu ordenador y luego copiar/pegar las siguientes instrucciones. De este modo podrás observar los cambios en las distribuciones variando los parámetros.\n\nsliderInput(\"p_ber\", label = \"Probabilidad éxito p:\",\n              min = 0.01, max = 0.99, value = 0.25, step = 0.01)\n\nrenderPlot({\npar(mfrow=c(1,2))\n  p=input$p_ber\nplot(x=c(0,1),y=dbinom(c(0,1),size=1,prob=p),\n     ylim=c(0,1),xlim=c(-0.5,2),xlab=\"x\",pch=21,\n     main=paste0(c(\"Función de probabilidad\\n\n                   Ber(p=\",p,\")\"),collapse=\"\"),bg=\"black\")\nsegments(x0=0,y0=0,x1=0,y1=1-p, col = \"blue\", lty =2)\nsegments(x0=1,y0=0,x1=1,y1=p, col = \"blue\", lty =2)\nsegments(x0=-1,y0=1-p,x1=0,y1=1-p, col = \"blue\", lty =2)\nsegments(x0=-1,y0=p,x1=1,y1=p, col = \"blue\", lty =2)\nx=0:1\ny=pbinom(x,size=1,prob=p)\ncurve(pbinom(x,size=1,prob=p),\n      xlim=c(-1,2),col=\"blue\",\n      main=paste0(c(\"Función de distribución\\n Ber(p=\",p,\")\"),collapse=\"\")\n      )\n\npar(mfrow=c(1,1))\n})\n\n\n\n\n\n4.1.2 Distribución binomial\nSi repetimos \\(n\\) veces de forma independiente un experimento Bernoulli de parámetro \\(p\\), el espacio muestral \\(\\Omega\\) estará formado por cadenas de \\(E\\)’s y \\(F\\)’s de longitud \\(n\\). Consideremos la v.a.: \\[X(\\overbrace{EFFF\\ldots EEF}^{n})=\\mbox{número de éxitos en la cadena}.\\] A la variable aleatoria anterior se la conoce como distribución binomial de parámetros \\(n\\) y \\(p\\), y lo denotaremos por \\(X\\equiv B(n,p).\\)\nFunción de probabilidad de una binomial\nSu función de probabilidad es: \\[\nP_{X}(x)=\\left\\{\n\\begin{array}{ll}\n{n\\choose x}\\cdot  p^x \\cdot(1-p)^{n-x}, &\\mbox{ si } x=0,1,\\ldots,n,\\\\\n0,  & \\mbox{ en otro caso.}\n\\end{array}\\right.\n\\] Función de distribución de una binomial\nSu función de distribución no tiene una fórmula cerrada. Hay que acumular la función de probabilidad: \\[\n\\begin{array}{ll}\nF_{X}(x)=P(X\\leq x) & =  \\sum_{i=0}^x P_X(i)\\\\\n& =\n\\left\\{\n\\begin{array}{ll}\n0, & \\mbox{ si } x\\leq 0,\\\\\\displaystyle\n\\sum_{i=0}^k {n\\choose i}\\cdot  p^i \\cdot (1-p)^{n-i} & \\mbox{ si }\n\\left\\{\n  \\begin{array}{l}\n  k\\leq x&lt; k+1,\\\\\n  k=0,1,\\ldots,n,\n  \\end{array}\n\\right.\\\\\n1, & \\mbox{ si } n\\leq x.\n\\end{array}\n\\right.\n\\end{array}\n\\]\nNúmeros binomiales con R\nLos números binomiales calculan el número de equipos de baloncesto distintos que (\\(k=5\\) jugadores) se pueden hacer con 6 jugadores (\\(n=6\\)).\nEs decir, cuántas maneras distintas hay para elegir (choose) 5 jugadores en un conjunto de 6 jugadores. Todo el mundo diría ¡¡¡6!!! Efectivamente con R es\n\nchoose(6,5)\n\n[1] 6\n\n\nCon 10 jugadores, el número de equipos de 5 distintos es bastante más grande\n\nchoose(10,5)\n\n[1] 252\n\n\nY, por ejemplo, con un equipo de fútbol profesional que tiene en plantilla 22 jugadores (quitando los guardametas) se pueden formar ¡¡nada menos que!!\n\nchoose(22,10)\n\n[1] 646646\n\n\nun bonito número capicúa que nos da el número de equipos distintos que se pueden formar.\nObviamente se tiene que una v.a. Bernoulli es una binomial con \\(n=1\\): \\(B(1,p)=Ber(p).\\)\n\nEjercicio\nCalculad las funciones de distribución de una binomial \\(B(n=1,p=0.3)\\) y comprobad que coinciden con las distribuciones de una \\(Ber(p=0.3)\\).\n\nObservaciones sobre la distribución binomial\n\nLa probabilidad de fracaso se suele denotar con \\(q=1-p\\), sin ningún aviso adicional, con el fin de acortar y agilizar la escritura de las fórmulas.\nSu función de distribución no tienen una formula general, hay que calcularla con una función de R o Python. En el siglo pasado se tabulaban en los libros de papel :-).\nEn el material adicional os pondremos unas tablas de esta distribución para distintos valores de \\(n\\) y \\(p\\) para que disfrutéis de tan ancestral método de cálculo.\nCualquier paquete estadístico u hoja de cálculo dispone de funciones para el cálculo de estas probabilidades, así que el uso de las tablas queda totalmente anticuado.\n\nEsperanza de una \\(B(n,p)\\)\nSu esperanza es: \\[E(X)=\\displaystyle\\sum_{k=0}^n k \\cdot  {n \\choose k }\\cdot p^k\\cdot q^{n-k} = n\\cdot p.\\] La esperanza de \\(X^2\\) es: \\[\nE(X^2)= \\displaystyle\\sum_{k=0}^n k^2 \\cdot  {n \\choose k }\\cdot p^k\\cdot q^{n-k}= n\\cdot p\\cdot q+(n\\cdot p)^2.\n\\]\nVarianza de una \\(B(n,p)\\)\nSu varianza es:\n\\[Var(X)=E(X^2)-\\left(E(X)\\right)^2=n\\cdot p \\cdot q=n\\cdot p\\cdot (1-p).\\]\nSu desviación típica es:\n\\[\\sqrt{n\\cdot p\\cdot q}=\\sqrt{n\\cdot p\\cdot (1-p)}.\\]\nEn temas posteriores veremos una forma sencilla del cálculo de la esperanza y varianza de una \\(B(n,p)\\) como las suma de \\(n\\) v.a. \\(Ber(p)\\) independientes.\n\nEjercicio\nJustificar de forma intuitiva que si \\(X_i\\) con \\(i=1,2,\\ldots, n\\) son v.a. \\(Ber(p)\\) independientes entonces \\(X=\\displaystyle\\sum_{i=1}^n X_i\\) sigue una distribución \\(B(n,p).\\)\n\nResumen v.a con distribución binomial \\(B(n,p)\\)\n\n\n\n\n\n\n\n\\(X\\) binomial\n\\(B(n,p)\\)\n\n\n\n\n\\(D_X=\\)\n\\(\\{0,1,\\ldots n\\}\\)\n\n\n\\(P_X(x)=P(X=x)=\\)\n\\(\\left\\{\\begin{array}{ll}{n\\choose x} p^x (1-p)^{n-x} & \\mbox{si } x=0,\\dots,n\\\\0 & \\mbox{ en otro caso.}\\end{array}\\right.\\)\n\n\n\\(F_X(x)=P(X\\leq x)=\\)\nno tiene fórmula (utilizad funciones de R o Python)\n\n\n\\(E(X)=n\\cdot p\\)\n\\(Var(X)=n\\cdot p \\cdot (1-p)\\)\n\n\n\nCálculos de la distribución Binomial con R\nVeamos los cálculos básicos con funciones de R para una v.a \\(X\\) con distribución binomial \\(B(n=10,p=0.25)\\).\nSi queremos calcular con R algún valor de la función de distribución, como por ejemplo \\(F_X(0)=P(X\\leq 0)\\), tenemos que hacer:\n\npbinom(0,size=10,prob=0.25)\n\n[1] 0.05631351\n\n\ny si queremos por ejemplo \\(F_X(4)=P(X\\leq 4)\\), tenemos que hacer:\n\npbinom(4,size=10,prob=0.25)\n\n[1] 0.9218731\n\n\nSin embargo, si queremos calcular algún valor de la función de probabilidad, como por ejemplo \\(P(X=0)\\), tenemos que hacer:\n\ndbinom(0,size=10,prob=0.25)\n\n[1] 0.05631351\n\n\no, por ejemplo para \\(P(X=4)\\):\n\ndbinom(4,size=10,prob=0.25)\n\n[1] 0.145998\n\n\nGeneración de muestras aleatorias con R\nGeneraremos una muestra aleatoria de 100 valores de una población con distribución \\(B(20,0.5)\\)\n\nset.seed(2019)\nrbinom(100,size = 20,prob=0.5)\n\n  [1] 12 11  9 11  6  6 12  5  7 11 12 11  8  8 11 11  7 11  9 10  9 10 14  8  8\n [26]  5 11 14 11 10 11  5 12  8  6  7  9 10  5 12 11  9 12 11 12 10 13 13  8  8\n [51]  9  7  6  9 10  9 16 13  6  6  8  8 11  9 12 15  9  7 12 11  9  8  9  8 11\n [76] 15  7 10  9 12  6 13 14  8 10  8 10 11 11  9 10 11 12  8 10 12  9 13  9 13\n\n\n\nEjemplo\nEl ejemplo anterior correspondería a repetir 100 veces el experimento de lanzar una moneda 20 veces y contar el número de caras.\n\nCálculos de la distribución Binomial con Python\nVeamos los cálculos básicos con funciones de Pyython para una v.a \\(X\\) con distribución binomial \\(B(n=10,p=0.25)\\).\nPrimero importamos la función binom de la librería scipy.stat:\n\n\nfrom scipy.stats import binom\n\nEn general, en el paquete scipy, la función de probabilidad se invocará con el método pmf, la de distribución con el método cdf, mientras que una muestra aleatoria que siga esta distribución, con el método rvs. En todos ellos aparecerá siempre el parámetro loc que se utiliza para desplazar el dominio de la variable aleatoria. Por ejemplo, en este caso:\n\nbinom.pmf(k, n, p, loc) =  binom.pmf(k - loc, n, p)\n\nPara calcular los valores de la función de distribución como por ejemplo \\(F_X(0)=P(X\\leq 0)\\) y \\(F_X(4)=P(X\\leq 4)\\) utilizamos la función cdf:\n\nbinom.cdf(0,n=10,p=0.25)\n\n0.056313514709472684\n\nbinom.cdf(4,n=10,p=0.25)\n\n0.9218730926513672\n\n\nNotemos que al no indicar el valor de loc, se le asume que toma el valor 0.\nPara calcular los valores de la función de probabilidad \\(P(X=0)\\) y \\(P(X=4)\\) utilizamos la función pmf:\n\nbinom.pmf(0,n=10,p=0.25)\n\n0.056313514709472656\n\nbinom.pmf(4,n=10,p=0.25)\n\n0.14599800109863284\n\n\nNotemos que al no indicar el valor de loc, se le asume que toma el valor 0.\nSi queremos generar una muestras aleatorias que siga una distribución binomial, podemos usar la función rvs. En este caso, generaremos una muestra aleatoria de 100 valores de una población \\(B(20,0.5)\\)\n\nbinom.rvs(n=20,p=0.25,size = 100)\n\narray([ 6,  5,  3,  7,  5,  3,  5,  6,  3,  4,  5,  6,  4,  4,  6,  8,  5,\n        6,  6,  5,  5,  5,  4,  6,  4,  6,  5,  6,  9,  4,  6,  7,  8,  6,\n        5,  7,  5,  7,  4,  6,  5,  9, 10,  5,  1,  7,  5,  8,  7,  7,  6,\n        7,  6,  8,  8,  4,  4,  7,  6,  6,  8,  3,  5,  9,  6,  4,  7,  6,\n        6,  8,  5,  6,  1,  3,  5,  7,  6,  6,  5,  4,  4,  7,  7,  4,  4,\n        4,  4,  2,  6,  7,  5,  5,  4,  6,  2,  6,  5,  4,  7,  6],\n      dtype=int64)\n\n\n Observación\nNotemos que la secuencia aleatoria generada no es la misma que con R. De hecho, si volvemos a ejecutar esta función obtendremos una muestra aleatoria distinta.\n\nbinom.rvs(n=20,p=0.25,size = 100)\n\narray([ 4,  6,  6,  5,  4,  7,  6,  6,  5,  7,  7,  4,  5,  4,  5,  4,  3,\n        6,  5,  6,  5,  1,  4,  3,  5,  5,  5,  5,  5,  8,  4,  3, 10,  1,\n        6,  7,  3,  3,  5,  3,  3,  5,  6,  3,  6,  7,  3,  6,  7,  6,  6,\n        6,  4,  7,  5,  8,  6,  7,  3,  7,  4,  6,  4,  5,  4,  4,  7,  5,\n        7,  6,  6,  6,  5,  9,  5,  4,  7,  3,  4,  5,  7,  4,  6,  2,  5,\n        7,  3,  7,  7,  6,  2,  5,  9,  5,  5,  7,  4,  7,  4,  8],\n      dtype=int64)"
  },
  {
    "objectID": "3.html#cuantiles-de-distribuciones-notables-discretas",
    "href": "3.html#cuantiles-de-distribuciones-notables-discretas",
    "title": "4  Distribuciones Notables",
    "section": "4.2 Cuantiles de distribuciones notables discretas",
    "text": "4.2 Cuantiles de distribuciones notables discretas\n\nEjemplo\nConsideremos una v.a. \\(X\\) de distribución \\(B(5,0.5)\\).\nLos cuantiles \\(x_{0.3}\\), \\(x_{0.6}\\) y \\(x_{0.8}\\) son los siguientes:\n\nqbinom(c(0.3,0.6,0.8),5,0.5)\n\n[1] 2 3 3\n\n\nCalculemos a mano, el valor \\(x_{0.3}\\) y verifiquemos que da el mismo resultado que nos ha dado R.\nLa función de distribución de \\(X\\) es: \\[\n\\small{\nF_x(x)=P(X\\leq x)=\n\\left\\{\n\\begin{array}{ll}\n0, & x&lt; 0, \\\\\n0.03125, & \\mbox{ si } 0 \\leq x&lt; 1, \\\\\n0.18750, & \\mbox{ si } 1 \\leq x&lt; 2, \\\\\n0.50000, & \\mbox{ si } 2 \\leq x&lt; 3, \\\\\n0.81250, & \\mbox{ si } 3 \\leq x&lt; 4, \\\\\n0.96875, & \\mbox{ si } 4 \\leq x&lt; 5, \\\\\n1.00000, & \\mbox{ si }  5\\leq x. \\\\\n\\end{array}\n\\right.}\n\\]\nEl cuantil \\(q=0.3\\) es el primer valor \\(x\\in D_X\\) tal que \\(F_X(x)=P(X\\leq x_{0.3})\\geq 0.3\\). Mirando la expresión anterior, comprobamos que \\(x_{0.3}=2\\) ya que \\(F_X(2)=P(X\\leq 2)=0.5 \\geq 0.3\\).\n\n\nEjercicio\nCalcular los cuantiles de \\(0.6\\) y \\(0.8\\) de una \\(B(5,0.5).\\)\n\nGráfico interactivo que muestra los cuantiles de las distribuciones \\(B(n,p)\\) y \\(Po(\\lambda)\\)\nPara ejecutar el siguiente gráfico interactivo, solamente tienes que cargar el paquete shiny en tu ordenador y luego copiar/pegar las siguientes instrucciones. De este modo podrás observar los cambios en las distribuciones variando los parámetros.\n\nfluidPage(\nfluidRow(\n  column(3,\n         sliderInput(\"nq\", label = \"Par. n B(n,p)\",\n              min = 1, max = 20, value =10 , step = 1)\n         ),\n  column(3,\n          sliderInput(\"pq\", label = \"Par. p B(n,p)\",\n                     min = 0.01, max = 0.99, value = 0.5, step = 0.1)\n         ),\n  column(3,\n         sliderInput(\"qq\", label=\" Cuantil q\", value=0.75, min = 0.01, max = 0.99, \n                     step = 0.01)\n         ),\n  column(3,\n         sliderInput(\"lq\", label=\"Par. lambda Po(lambda)\", value=5, min = 1, max = 20, \n                     step = 1)\n         )\n  )\n)\n\n  \nrenderPlot({\n  n=input$nq\n  p=input$pq\n  q=input$qq\n  lambda=input$lq\n  par(mfrow=c(1,2))\n  #n=10;p=0.5;q=0.75;lambda=5\n  #xx=c(seq(min(a,x),min(b,x),by=0.001))\n  probsB=pbinom(0:n,n,p)\n  curve(pbinom(x,n,p),xlim=c(0-0.25,n+0.25),ylim=c(0,max(probsB+0.05,0.1)),\n        col=\"blue\",main=paste0(\"Función distribución\\n B(n=\",n,\", p=\",p,\")\"),\n        ylab=paste0(\"dbinom(x,\",n,\", \",p,\")\"),yaxt=\"n\")\n  segments(x0 = qbinom(q,n,p),y0 = 0,x1 = qbinom(q,n,p),y1 = q,lty=2,col=\"red\")\n  segments(x0 = qbinom(q,n,p),y0 = q,x1 = -0.25,y1 = q,lty=2,col=\"red\")\n  ytick=c(0.0,q,1)\n  axis(side=2, at=ytick, labels = TRUE)\n  axis(side=1, at=qbinom(q,n,p), labels = TRUE)\n  curve(ppois(x,lambda),xlim=c(0-0.25,2.5*lambda),ylim=c(0,1+0.1),\n        col=\"blue\",main=paste0(\"Función distribución \\n Po(lambda=\",lambda,\")\"),\n        ylab=paste0(\"dpois(x, lambda\",lambda,\")\"),yaxt=\"n\")\n  segments(x0 = qpois(q,lambda),y0 = 0,x1 = qpois(q,lambda),y1 = q,lty=2,col=\"red\")\n  segments(x0 = qpois(q,lambda),y0 = q,x1 = -0.25,y1 = q,lty=2,col=\"red\")\n  ytick=c(0.0,q,1)\n  axis(side=2, at=ytick, labels = TRUE)\n  axis(side=1, at=qpois(q,lambda), labels = TRUE)\n  par(mfrow=c(1,1))\n})"
  },
  {
    "objectID": "3.html#distribuciones-continuas",
    "href": "3.html#distribuciones-continuas",
    "title": "4  Distribuciones Notables",
    "section": "4.3 Distribuciones continuas",
    "text": "4.3 Distribuciones continuas\n\n4.3.1 Distribución uniforme\nUna v.a. continua \\(X\\) tiene una distribución uniforme sobre el intervalo real \\((a,b)\\) ,con \\(a&lt;b\\), si su función de densidad es\n\\[\nf_X(x)=\\left\\{\\begin{array}{ll}\n\\frac1{b-a}, & \\mbox{si } a&lt;x&lt;b,\\\\ 0,  & \\mbox{en cualquier otro caso.}\n\\end{array}\n\\right.\n\\]\n\nEjercicio\nComprobar que el área comprendida entre \\(f_X\\) y la horizontal vale 1.\n\nEl área pedida vale: \\[\n\\int_{-\\infty}^{+\\infty} f_x(x)\\cdot dx=\\int_{a}^{b} \\frac{1}{b-a} \\cdot dx=\\left.\\frac{x}{b-a}\\right]_{x=a}^{x=b}=\\frac{b}{b-a}-\\frac{a}{b-a}=\n\\frac{b-a}{b-a}=1.\n\\]\n\n\nSu función de distribución es: \\[\nF_X(x)=\\left\\{\\begin{array}{ll} 0,  & \\mbox{si } x\\leq a,\\\\\n\\frac{x-a}{b-a}, & \\mbox{si } a&lt;x&lt;b,\\\\ 1,  & \\mbox{si } b\\leq x.\n\\end{array}\n\\right.\n\\] Efectivamente:\n\nSi \\(x\\leq a\\), entonces \\[F_X(x)=\\int_{-\\infty}^{x} f(t)\\cdot dt= \\int_{-\\infty}^{x} 0\\cdot dt.\\]\nSi \\(a&lt;x&lt;b\\) entonces , \\[\n\\begin{array}{rl}\nF_X(x)&=\\displaystyle\\int_{-\\infty}^{x} f(t)\\cdot dt= \\int_{-\\infty}^{a} 0\\cdot dt+\\int_{a}^{x} \\frac1{b-a} \\cdot dt\\\\\n&= \\displaystyle 0 +\\left.\\frac{t}{b-a}\\right]_{t=a}^{t=x}= \\frac{x}{b-a}-\\frac{a}{b-a}=\\frac{x-a}{b-a}.\n\\end{array}\n\\]\nPor último si \\(x\\geq b\\) entonces,\n\n\\[\n\\begin{array}{rl}\nF_X(x)&=\\displaystyle\\int_{-\\infty}^{x} f(t) dt=\\int_{a}^{b} \\frac{1}{b-a} dt=\n  \\left.  \\frac{t}{b-a} \\right]_{t=a}^{t=b}\n\\\\&=\\displaystyle \\frac{b}{b-a}-\\frac{a}{b-a}=\\frac{b-a}{b-a}=1.\n\\end{array}\n\\]\nDenotaremos a la v.a. \\(X\\) uniforme en el intervalo \\((a,b)\\) por \\(U(a,b)\\).\nEsperanza y varianza para una v.a. \\(X\\) \\(U(a,b)\\)\nCalculemos la esperanza de \\(X\\):\n\\[\n\\begin{array}{rl}\nE(X)&=\\displaystyle\\int_{-\\infty}^{+\\infty} x\\cdot f_X(x) dx =\\int_{a}^{b} x \\cdot \\frac{1}{b-a} dx =\n\\left.\\frac{x^2}{2\\cdot (b-a)}\\right]_{x=a}^{x=b}\\\\\n&=\\displaystyle \\frac{b^2}{2\\cdot (b-a)}-\\frac{a^2}{2\\cdot (b-a)}=\n\\frac{b^2-a^2}{2\\cdot (b-a)}=\\frac{(b+a)\\cdot (b-a)}{2\\cdot (b-a)}=\n\\frac{b+a}{2}.\n\\end{array}\n\\]\nDe cara a calcular su varianza, calculemos primero la esperanza de \\(X^2\\):\n\\[\n\\begin{array}{rl}\nE(X^2)&=\\displaystyle\\int_{-\\infty}^{+\\infty} x^2 f_X(x) dx=\\int_{a}^{b} x^2 \\frac1{b-a}\ndx =\\left.\\frac{x^3}{3\\cdot (b-a)}\\right]_{x=a}^{x=b} \\\\\n&=\\displaystyle\\frac{b^3-a^3}{3\\cdot (b-a)}=\\frac{b^2+ab+a^2}{3}.\n\\end{array}\n\\]\n\nEjercicio\n\nDemostrad que la igualdad \\(b^3-a^3=(b-a)\\cdot (b^2+ab+a^2)\\) es cierta.\nUtilizadla para el cálculo final del valor de \\(E(X^2)\\).\n\n\nCalculemos \\(Var(X)\\). \\[\n\\begin{array}{rl}\nVar(X)&=\\displaystyle E(X^2)-(E(X))^2=\\frac{b^2+ab+a^2}3-\\left(\\frac{b+a}2\\right)^2\\\\&=\\displaystyle\n\\frac{b^2+ab+a^2}{3}-\\frac{b^2+2ab+a^2}{4}\\\\\n&=\\displaystyle\n\\frac{4\\cdot (b^2+ab+a^2)-3\\cdot (b^2+2ab+a^2)}{4\\cdot 3}\n\\\\\n&=\\displaystyle\n\\frac{b^2-2ab+a^2}{12}=\n\\frac{(b-a)^2}{12}.\n\\end{array}\n\\]\nGráficas \\(U(0,1)\\)\nEl código en R para dibujar la función de densidad y la función de distribución de una distribución \\(U(0,1)\\) es el siguiente:\n\npar(mfrow=c(1,2))\na=0;b=1\ncurve(dunif(x,a,b),xlim=c(a-0.25,b+0.25),ylim=c(0,max(1/(b-a)+0.05,0.1)),\n      col=\"blue\",main=paste0(\"Función densidad  U(\",a,\",\",b,\")\"),\n      ylab=paste0(\"dunif(x,\",a,\", \",b,\")\")\n      )\ncurve(punif(x,a,b),xlim=c(a-1,b+1),ylim=c(0,1.1),\n      col=\"blue\",main=paste0(\"Función de distribución U(\",a,\",\",b,\")\"),\n      ylab=paste0(\"punif(x,\",a,\", \",b,\")\",cex.axis=0.8)\n      )\npar(mfrow=c(1,1))\n\n\n\n\n\n\n\n\n\n\nGráficas interactivas \\(U(a,b)\\)\nPara ejecutar el siguiente gráfico interactivo, solamente tienes que cargar el paquete shiny en tu ordenador y luego copiar/pegar las siguientes instrucciones. De este modo podrás observar los cambios en las distribuciones variando los parámetros.\n\nfluidPage(\nfluidRow(\n  column(4,\n         sliderInput(\"a1\", label = \"Parámetro a\",\n              min = -5, max = 9, value =0 , step = 0.1)\n         ),\n  column(4,\n          sliderInput(\"b1\", label = \"Parámetro b\",\n                     min = 10, max = 15, value = 5, step = 0.1)\n         ),\n  column(4,\n         sliderInput(\"x1\", label=\"x\", value=9, min = -5, max = 15, step = 0.1)\n         )\n  \n)\n)\n\nrenderPlot({\n  a=input$a1\n  b=input$b1\n  x=input$x1\n  par(mfrow=c(1,2))\n  #a=0;b=1;x=0.25\n  xx=c(seq(min(a,x),min(b,x),by=0.001))\n  curve(dunif(x,a,b),xlim=c(a-0.25,b+0.25),ylim=c(0,max(1/(b-a)+0.05,0.1)),\n        col=\"blue\",main=paste0(\"Función densidad U(\",a,\",\",b,\")\"),\n  ylab=paste0(\"dunif(x,\",a,\", \",b,\")\"),xaxt=\"n\")\n  axis(side=1, at=c(a,x,b), labels = TRUE)\n  polygon(x=c(a,xx,min(x,b)),y=c(0,dunif(xx,a,b),0),\n          density=20,col=\"skyblue\")\n  curve(punif(x,a,b),xlim=c(a-1,b+1),ylim=c(0,1.1),col=\"blue\",\n        main=paste0(\"Función de distribución U(\",a,\",\",b,\")\"),\n  ylab=paste0(\"punif(x,\",a,\", \",b,\")\"),xaxt=\"n\",yaxt=\"n\")\n  segments(x0=x,y0=0,x1=x,y1=punif(x,a,b),col=\"red\",lty=2)\n  segments(x0=a-1.01,y0=punif(x,a,b),x1=x,y1=punif(x,a,b),col=\"red\",lty=2)\n  axis(side=2, at=c(0,round(punif(x,a,b),1),2), labels = TRUE)\n  axis(side=1, at=c(a,x,b), labels = TRUE)\n  par(mfrow=c(1,1))\n})\n\n\nTransformación lineal de la v.a. uniforme\nSi \\(X\\) sigue una distribución \\(U(a,b)\\) entonces \\(Z=\\frac{X-a}{b-a}\\) sigue una distribución \\(U(0,1)\\).\n\nPropiedad: Transformación lineal de la v.a. uniforme\n\nSea \\(X\\) una v.a \\(U(a,b)\\).\nSi \\(scale\\not=0\\) y \\(loc\\) son dos constantes reales, entonces\n\nsi \\(scale&gt;0\\), \\(T=scale\\cdot X+loc\\) sigue una ley \\(U(scale\\cdot a +loc,scale\\cdot b +loc)\\)\n\nsi \\(scale&lt;0\\), \\(T=scale\\cdot X+loc\\) sigue una ley \\(U(scale\\cdot b +loc,scale\\cdot a +loc)\\)\n\n\nDemostración\nSupongamos que \\(X\\) sigue una ley \\(U(a,b)\\), que \\(scale&gt;0\\) y que \\(T=scale\\cdot X+loc\\). Dejamos el caso \\(scale&lt;0\\) como ejercicio.\nLa función de distribución de \\(X\\) es: \\[\nF_X(x)=P(X\\leq x)=\\left\\{\\begin{array}{ll} 0, & \\mbox{ si } x\\leq a,\\\\\\frac{x-a}{b-a}, & \\mbox{ si } a\\leq x\\leq b, \\\\1, & \\mbox{ si } b\\leq x.\\end{array}\\right.\n\\]\nSi \\(T\\) vale \\(T=scale\\cdot X+loc\\), su función de distribución será: \\[\n\\begin{array}{rl}\nF_T(t)&=P(T\\leq t)= P(scale\\cdot X+ loc\\leq t)= P\\left(X\\leq \\frac{t-loc}{scale}\\right)=F_X\\left(\\frac{t-loc}{scale}\\right)\\\\\n&=\n\\left\\{\\begin{array}{ll} 0, & \\mbox{ si } \\frac{t-loc}{scale}\\leq a\\\\\\frac{\\frac{t-loc}{scale}-a}{b-a}, & \\mbox{ si } a\\leq \\frac{t-loc}{scale}\\leq b,\\\\1, & \\mbox{ si } b\\leq \\frac{t-loc}{scale},\\end{array}\\right. \\\\ & =\n\\left\\{\\begin{array}{ll} 0, & \\mbox{ si }  t\\leq scale\\cdot a +loc, \\\\\n\\frac{t-(scale\\cdot a+loc)}{scale\\cdot (b-a)}, & \\mbox{ si } scale\\cdot a+loc \\leq t\\leq scale\\cdot b+loc, \\\\\n1, & \\mbox{ si } scale\\cdot b+loc\\leq t, \\end{array}\\right.\\\\\n& =\n\\left\\{\\begin{array}{ll} 0, & \\mbox{ si }  t\\leq scale\\cdot a +loc, \\\\\n\\frac{t-(scale\\cdot a+loc)}{scale\\cdot b+loc-(scale\\cdot a+loc)}, & \\mbox{ si } scale\\cdot a+loc \\leq t\\leq scale\\cdot b+loc, \\\\\n1, & \\mbox{ si } scale\\cdot b+loc\\leq t,\\end{array}\\right.\n\\end{array}\n\\] función que corresponde a la función de distribución de una v.a. \\(U(scale\\cdot a+loc,scale\\cdot b+loc)\\), como queríamos demostrar.\n\n\nEjercicio\nSea \\(X\\) una variable \\(U(0,1)\\) y sea \\(T=scale\\cdot X+loc\\):\n\nSi \\(T\\) es \\(U(-5,5)\\) ¿qué valores toman \\(scale\\) y \\(loc\\)?\nSi \\(loc=-10\\) y \\(scale=10\\) ¿qué distribución de probabilidad sigue \\(T\\)?\nSi \\(loc=0\\) y \\(scale=-1\\) ¿qué distribución probabilidad sigue \\(T\\)?\n\n\nResumen v.a con distribución uniforme, \\(U(a,b)\\)\n\n\n\n\n\n\n\nDistribución uniforme\n\\(U(a,b)\\)\n\n\n\n\nDominio\n\\(D_X=(a,b)\\)\n\n\n\\(f_{X}(x)\\)\n\\(\\left\\{\\begin{array}{ll}\\frac1{b-a}, & \\mbox{si } a&lt;x&lt;b,\\\\ 0, & \\mbox{en cualquier otro caso.}\\end{array} \\right.\\)\n\n\n\\(F_X(x)=P(X\\leq X)=\\)\n\\(\\left\\{\\begin{array}{ll} 0, & \\mbox{ si } x\\leq a\\\\\\frac{x-a}{b-a}, & \\mbox{ si } a\\leq x\\leq b,\\\\1, & \\mbox{ si } b\\leq x.\\end{array}\\right.\\)\n\n\n\\(E(X)= \\frac{a+b}2\\)\n\\(Var(X)=\\frac{(b-a)^2}{12}\\)\n\n\n\nCálculos con R\nSea \\(X\\) una \\(v.a.\\) \\(U(a,b)\\). Las funciones dunif(x,a,b) y punif(x,a,b) calculan la función de densidad y de distribución de \\(X\\) en el valor \\(X\\). Por ejemplo, para \\(a=-1\\), \\(b=1\\) y \\(x=0.5\\), los valores \\(f_X(x)\\) y \\(F_X(x)\\) valen:\n\ndunif(x=0.5, min=-1,max=1)\n\n[1] 0.5\n\npunif(q=0.5,min=-1,max=1)\n\n[1] 0.75\n\n\nLa función runif(n,a,b) calcula una muestra de observaciones de tamaño \\(n\\) que sigan la distribución \\(U(a,b)\\):\n\nrunif(n=5,min=-1,max=1)\n\n[1] -0.5502232 -0.4271506 -0.0515360  0.8052209  0.8983523\n\n\nPor defecto, el valor de los parámetros a y b son 0 y 1, respectivamente:\n\ndunif(x=0.5)\n\n[1] 1\n\npunif(q=0.5)\n\n[1] 0.5\n\nrunif(n=5)\n\n[1] 0.382302539 0.009313886 0.351767001 0.294007361 0.071581515\n\n\nCálculos con Python\nSea \\(X\\) una \\(v.a.\\) \\(U(-1,1)\\). Tomando como “base” la v.a. \\(U(0,1)\\), los parámetros loc y scale valen: loc\\(=-1\\) y scale\\(=2\\), ya que como hemos visto \\(X=2*U(0,1)-1=U(-1,1)\\).\nEn Python, hay que usar dichos parámetros para calcular la función de densidad y de distribución:\n\nfrom scipy.stats import uniform\nuniform.pdf(0.5,loc=-1,scale=2)\n\n0.5\n\nuniform.ppf(0.5,loc=-1,scale=2)\n\n0.0\n\n\nPara generar una muestra de valores aleatorios, hay que usar la función uniform.rvs:\n\nuniform.rvs(size=30,loc=-1,scale=2)\n\narray([-0.46029716, -0.34498285, -0.2767083 , -0.12818905, -0.96811061,\n       -0.31136321, -0.69897762, -0.77363762,  0.29742925,  0.73818078,\n       -0.92204654, -0.27979008, -0.73919678, -0.26917781,  0.21037103,\n       -0.89429298, -0.18190193,  0.30496477,  0.98543466,  0.76324483,\n        0.67251934, -0.82561707, -0.21232468,  0.29661697,  0.80642791,\n        0.4196325 ,  0.46232967, -0.32752918, -0.3494945 , -0.92258221])\n\n\nLos valores de los parámetros por defecto son loc=0, scale=1:\n\nuniform.pdf(0.5)\n\n1.0\n\nuniform.ppf(0.5)\n\n0.5\n\nuniform.rvs(size=5)\n\narray([0.79948668, 0.29038236, 0.25591743, 0.28172549, 0.22280437])\n\n\n\n\n4.3.2 Distribución exponencial\nDistribución del tiempo entre dos eventos Poisson\nSupongamos que tenemos un proceso Poisson con parámetro \\(\\lambda\\) en una unidad de tiempo.\nDado un tiempo \\(t\\), definimos \\(N_{t}\\) como el número de eventos en el intervalo de tiempo \\((0,t]\\). La distribución de \\(N(t)\\) es una \\(Po(\\lambda\\cdot t)\\). Consideremos la v.a. \\(T\\) como el tiempo transcurrido entre dos eventos Poisson consecutivos.\nSea \\(t&gt;0\\), entonces\n\\[\n\\begin{array}{rl}\nP(T&gt;t)&=P(\\mbox{Cero eventos en el intervalo}(0,t])\\\\\n&=P(N_{t}=0)=\n         \\frac{(\\lambda t)^0}{0!} e^{-\\lambda\n         t}=e^{-\\lambda t}.\n\\end{array}\n\\]\nTomando complementarios, la función de distribución de \\(T\\) será: \\[\nF_{T}(t)= P(T\\leq t)=1-P(T&gt;t)=\\left\\{\\begin{array}{ll} 0, &\\mbox{ si } t\\leq 0,\\\\\n  1-e^{-\\lambda t},& \\mbox{ si } t&gt;0,\\end{array}\\right.\n\\]\nPara hallar la función de densidad de \\(T\\), basta derivar la expresión anterior:\n\\[\nf_{T}(t)=\\left\\{\\begin{array}{ll}\\lambda \\cdot e^{-\\lambda t}, & \\mbox{ si }  t&gt;0,\\\\\n0, & \\mbox{ si } t\\leq 0. \\end{array}\\right.\n\\]\nLlamaremos a la variable \\(T\\) exponencial de parámetro \\(\\lambda\\) y la denotaremos por \\(Exp(\\lambda)\\).\nPropiedad de la falta de memoria\nSea \\(X\\) una v.a. \\(Exp(\\lambda)\\), entonces\n\\[P(X&gt;s+t\\big|X&gt;s)=P(X&gt;t)\\mbox{  para todo } s,t\\in \\mathbb{R}\\]\n\nDemostración\nSi \\(X\\) es una v.a. \\(Exp(\\lambda)\\) tenemos que \\(P(X&gt;x)=1-P(X\\leq x)=1-(1-e^{-\\lambda\\cdot x})=e^{-\\lambda\\cdot x}\\) para todo \\(x&gt;0\\).\nPor tanto, \\[\n\\begin{array}{rl}\nP(X&gt;s+t\\big|X&gt;s) & =\\frac{P(\\{X&gt;s+t\\}\\cap \\{X&gt;s\\})}{P(X&gt;s)}=\\frac{P(X&gt;s+t)}{P(X&gt;s)}=\\frac{e^{-\\lambda\\cdot (s+t)}}{e^{-\\lambda\\cdot s}}=\n\\frac{e^{-\\lambda\\cdot s}\\cdot e^{-\\lambda\\cdot t} }{e^{-\\lambda\\cdot s}}\\\\ & =e^{-\\lambda\\cdot t}=P(X&gt;t).\n\\end{array}\n\\]\n\n\nEjemplo: el clásico problema del peluquero.\nUna pequeña peluquería es regentada por un único peluquero. El peluquero está esperando al próximo cliente mientras lee el periódico.\nSupongamos que la v.a. \\(N_T\\), que representa el número de clientes que llegan en el intervalo \\([0,t)\\), es una \\(Po(\\lambda\\cdot t)\\) entonces la variable \\(T\\), tiempo entre dos clientes consecutivos, sigue una ley \\(Exp(\\lambda)\\).\nSupongamos que \\(t\\) se mide en horas y que \\(\\lambda=4\\) es el promedio de clientes por hora.\nSe pide:\n\nEl tiempo esperado (en horas) y la varianza hasta el siguiente cliente.\n¿Cuál es la probabilidad de que nuestro peluquero esté sin clientes (leyendo el periódico) más de 30 minutos (0.5 horas)?\n\n\nEn este ejemplo, la propiedad de la pérdida de memoria significa que por ejemplo, si el peluquero lleva ya esperando más de \\(s&gt;0.25\\) (un cuarto de hora), la probabilidad de que espere \\(t=1/6\\) de hora más (10 minutos) no cambia, sigue siendo \\(P(T&gt;0.25+1/6|T&gt;0.25)=P(T&gt;1/6).\\)\nSolución de 1. El tiempo esperado (en horas) y la varianza hasta el siguiente cliente.\nEl tiempo esperado (en horas) hasta el siguiente cliente es\n\\[\nE(X)=\\frac{1}{\\lambda}=\\frac{1}{4}=0.25,\n\\] y la varianza es \\[\nVar(X)=\\frac{1}{\\lambda^2}=\\frac{1}{4^2}=0.0625.\n\\] Solución de 2. ¿Cuál es la probabilidad de que nuestro peluquero esté sin clientes (leyendo el periódico) más de 30 minutos (0.5 horas)?\nLa probabilidad pedida vale: \\[\nP(X&gt;0.5)=1-P(X\\leq 0.5)=1-(1-e^{-4\\cdot 0.5 })=e^{-2}=0.1353353.\n\\]\nUsando R, la probabilidad anterior puede ser calculada de la forma siguiente:\n\npexp(0.5,rate=3)\n\n[1] 0.7768698\n\n1-pexp(0.5,rate=3)\n\n[1] 0.2231302\n\npexp(0.5,rate=3,lower.tail = FALSE)\n\n[1] 0.2231302\n\n\n\n\nCálculos con R\nLas funciones de densidad y de distribución de una variable exponencial de parámetro \\(\\lambda\\) en un valor x se pueden obtener en R usando las funciones dexp(x,lambda) y pexp(x,lambda), respectivamente. Para generar n valores aleatorios de una variable exponencial de parámetro \\(\\lambda\\), hay que usar la función rexp(n,lambda). Veamos un ejemplo de aplicación de las tres funciones anteriores:\n\ndexp(0.001,rate=3) # Alerta! No es una probabilidad, es una densidad y puede ser &gt; 1\n\n[1] 2.991013\n\npexp(0.5,rate=3) # P(X&lt;0.5)\n\n[1] 0.7768698\n\nrexp(10,3) # Diez tiempos de una exponencial con lambda = 3\n\n [1] 0.32072389 0.57998323 0.26019628 0.48896233 0.31041354 0.11871455\n [7] 0.10798351 0.18699902 0.10023100 0.03619832\n\n\nCálculos con Python\nLas funciones de densidad y de distribución de una variable exponencial de parámetro \\(\\lambda\\) en un valor x se pueden obtener en Python usando las funciones expon.pdf(x,scale=1/lambda) y expon.cdf(x,scale=1/lambda), respectivamente. Para generar n valores aleatorios de una variable exponencial de parámetro \\(\\lambda\\), hay que usar la función expon.rvs(scale=1/lambda,size=n). Veamos un ejemplo de aplicación de las tres funciones anteriores:\n\nfrom scipy.stats import expon\nexpon.pdf(0.0001,scale= 1./3)\n\n2.9991001349865014\n\nexpon.cdf(0.5,scale= 1./3) \n\n0.7768698398515702\n\nexpon.rvs(scale=1./3,size=10)\n\narray([0.3172856 , 0.24314435, 0.11107456, 0.02691132, 0.05826914,\n       0.19055301, 1.87109608, 0.368045  , 0.36402383, 0.00541118])\n\n\nResumen v.a con distribución exponencial, \\(Exp(\\lambda)\\)\n\n\n\n\n\n\n\n\\(X\\)\n\\(Exp(\\lambda)\\)\n\n\n\n\n\\(D_X=\\)\n\\((0,+\\infty)\\)\n\n\n\\(f_{X}(x)=\\)\n\\(\\left\\{\\begin{array}{ll} \\lambda e^{-\\lambda x}, & \\mbox{ si } x&gt;0,\\\\ 0, & \\mbox{ si } x\\leq 0. \\end{array}\\right.\\)\n\n\n\\(F_X(x)=P(X\\leq X)=\\)\n\\(\\left\\{\\begin{array}{ll} 0, &\\mbox{si } x\\leq 0,\\\\ 1-e^{-\\lambda x},& \\mbox{si } x&gt;0.\\end{array}\\right.\\)\n\n\n\\(E(X)=\\frac{1}{\\lambda}\\)\n\\(Var(X)=\\frac{1}{\\lambda^2}\\)\n\n\n\nGráficas de las funciones de densidad y de distribución de la variable aleatoria \\(Exp(\\lambda=10)\\)\n\nlambda=10\npar(mfrow=c(1,2))\ncurve(dexp(x,rate=lambda),xlim=c(-0.05,round(qexp(0.99,rate=lambda,2),2)+0.25),\n      ylim=c(0,dexp(0,lambda)+0.1),col=\"blue\",\n      main=paste0(\"Función densidad Exp(\",lambda,\")\"),\n      ylab=paste0(\"dexp(x,rate=\",lambda,\")\"))\ncurve(pexp(x,rate=lambda),xlim=c(-0.05,qexp(0.999,10)),ylim=c(0,1.1),col=\"blue\",\n      main=paste0(\"Función de distribución Exp(\",lambda,\")\"),\n      ylab=paste0(\"pexp(x,rate=\",lambda,\")\"))\npar(mfrow=c(1,1))\n\n\n\n\n\n\n\n\n\n\n\nEjercicio\nConsultad en el manual de Python scipy.stats.\nDibujad la función de densidad y de distribución de una \\(Exp(\\lambda=10).\\)\n\nGráficas interactivas de una variable \\(Exp(\\lambda)\\)\nPara ejecutar el siguiente gráfico interactivo, solamente tienes que cargar el paquete shiny en tu ordenador y luego copiar/pegar las siguientes instrucciones. De este modo podrás observar los cambios en las distribuciones variando los parámetros.\n\nfluidPage(\nfluidRow(\n  column(4,\n         sliderInput(\"le\", label = \"lambda\",\n              min = 0.1, max = 3, value =1 , step = 0.1)\n         ),\n  column(4,\n          sliderInput(\"xe\", label = \"X=x\",\n                     min = 0, max = 5, value = 5, step = 0.1)\n         ),\n  column(4,\n          sliderInput(\"pe\", label = \"Cuantil p\",\n                     min = 0.01, max = 1, value = 0.75, step = 0.01)\n         )\n)\n)\n\nrenderPlot({\n  lambda=input$le\n  p=input$pe\n  x=input$xe\n  #lambda=10;p=0.75;x=0.4\n  xx=seq(0,x,by=0.001)\n  par(mfrow=c(1,2))\n  curve(dexp(x,rate=lambda),xlim=c(-0.05,round(qexp(0.999,rate=lambda),2)),\n        ylim=c(0,dexp(0,lambda)+0.1),col=\"blue\",\n        main=paste0(\"Función densidad Exp(\",lambda,\")\"),\n  ylab=paste0(\"dexp(x,\",lambda,\")\"),xaxt=\"n\")\n  axis(side=1, at=c(0,x,round(qexp(0.999,rate=lambda),2)),cex.axis=0.8)\n  polygon(x=c(0,xx,max(x,xx)),y=c(0,dexp(xx, rate=lambda),0),\n          density=20,col=\"skyblue\")\n  curve(pexp(x,rate=lambda),xlim=c(0.01,qexp(0.999,rate=lambda)+0.1),\n        ylim=c(0,1.1),col=\"blue\",\n        main=paste0(\"Función de distribución Exp(\",lambda,\")\"),\n        ylab=paste0(\"pexp(x,\",lambda,\")\"),xaxt=\"n\",yaxt=\"n\")\n  segments(x0=qexp(p,lambda),x1=qexp(p,lambda),y0=0,y1=p,col=\"red\",lty=2)\n  segments(x0=0-0.05,y0=p,x1=qexp(p,lambda),y1=p,col=\"red\",lty=2)\n  axis(side=2, at=seq(0,1,0.1), labels = TRUE)\n  axis(side=1, at=seq(0,round(qexp(0.999,rate=lambda),2),by=0.1), labels = TRUE)\n  par(mfrow=c(1,1))\n})\n\n\n\nEjercicio: las bombillas que no envejecen.\nSupongamos que compramos una bombilla led que promete un valor esperado de duración de 10000 (1.14 años) horas de funcionamiento continuo. Además, nos aseguran que la distribución de \\(X\\), el número de horas de funcionamiento continuo de una bombilla led, sigue una ley exponencial.\n\nSi \\(X\\) es \\(Exp(\\lambda)\\) ¿cuál es el valor del parámetro \\(\\lambda\\)?.\n¿Cuál es la probabilidad de que una bombilla led ilumine más de 2 años?\nSupongamos que ya tengo una bombilla led funcionando 1 año ¿Cuál es la probabilidad de que dure dos años más?\n¿Cuál es la varianza de la duración en horas de este tipo de bombillas?\n\n\n\n\n4.3.3 Distribución normal o Gaussiana\nUna de las variables aleatorias continua más populares es la llamada distribución normal o Gaussiana.\n Distribución normal o de Gauss Diremos que una v.a. \\(X\\) sigue una ley normal de parámetros \\(\\mu\\) y \\(\\sigma\\) y la denotaremos por \\(N(\\mu,\\sigma)\\) si tiene por función de densidad:\n\\[\nf_{X}(x)=\\frac1{\\sqrt{2\\cdot\\pi\\cdot\\sigma^2}}\ne^{-\\frac{1}{2}\\cdot\\left(\\frac{x-\\mu}{\\sigma}\\right)^2},\n\\] para todo \\(x\\in \\mathbb{R}.\\)\nLa gráfica de esta función de densidad es conocida como campana de Gauss.\nLa v.a. normal con \\(\\mu=0\\) y \\(\\sigma=1\\) recibe el nombre de normal estándar y se suele denotar por la letra \\(Z\\sim N(0,1)\\).\n\ncurve(dnorm(x),main=\"Función de densidad de una normal estándar\",xlim=c(-3.9,3.9))\n\n\n\n\n\n\n\n\n Propiedades de la función de densidad de la distribución normal\nSea \\(X\\) una v.a. \\(N(\\mu,\\sigma)\\) y sean \\(f_{X}\\) su función de densidad y \\(F_X(x)=\\displaystyle\\int_{-\\infty}^x f_X(t)\\, dt\\) su función de distribución. Entonces:\n\nLa función \\(f_{X}\\) verifica todas las propiedades de las funciones de densidad: \\(f_X(x)&gt;0\\), para todo \\(x\\in\\mathbb{R}\\) y \\(\\displaystyle\\int_{-\\infty}^\\infty f_X(x)\\,dx=1\\).\nLa función \\(f_X(x)\\) es simétrica respecto de la recta \\(x=\\mu\\): \\(f_{X}(\\mu-x)=f_{X}(\\mu+x)\\), para todo \\(x\\in\\mathbb{R}\\).\n\\(f_{X}\\) tiene un único máximo absoluto en \\(x=\\mu\\) que vale \\(f_X(\\mu)=\\frac1{\\sqrt{2\\pi\\sigma^2}}\\).\nSi \\(F_{X}\\) es la función de distribución de \\(X\\), entonces \\(F_{X}(\\mu+x)=1-F_{X}(\\mu-x)\\), para todo \\(x\\in\\mathbb{R}\\).\nEn particular, si \\(Z\\) es una \\(N(0,1)\\), entonces \\(F_{Z}(-x)=1-F_{Z}(x)\\) para todo \\(x\\in\\mathbb{R}\\).\n\\(Z=\\frac{X-\\mu}{\\sigma}\\) es una v.a. \\(N(0,1)\\) y \\(X=\\sigma\\cdot Z+\\mu\\) es una \\(N(\\mu,\\sigma)\\) donde \\(Z\\) es la normal estándar.\nLa función \\(f_X\\) es continua.\n\\(\\lim\\limits_{x\\to+\\infty}f(x)=\\lim\\limits_{x\\to-\\infty}f(x)=0\\) es decir tiene asíntota horizontal a derecha e izquierda.\n\\(f\\) es estrictamente creciente si \\(x&lt;\\mu\\) y decreciente si \\(x&gt;\\mu\\).\nTiene dos puntos de inflexión en \\(x=\\mu+\\sigma\\) y en \\(x=\\mu-\\sigma\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFunción de distribución de la N(0,1)\nSu función de distribución es, como sabemos:\n\\[\nF(x)=\\displaystyle\\int_{-\\infty}^{x} {1\\over{\\sqrt{2\\cdot \\pi}}}\ne^{-{{x^2}\\over 2}} dt.\n\\]\nLa función \\(F(x)\\) no tiene ninguna expresión algebraica “decente”. Es por esta razón, y por comodidad, que esta función está tabulada o hay que calcularla usando un software estadístico.\nResumen v.a con distribución normal, \\(N(\\mu,\\sigma)\\)\n\n\n\n\n\n\n\n\\(X\\)\n\\(N(\\mu,\\sigma)\\)\n\n\n\n\n\\(D_X=\\)\n\\(\\mathbb{R}=(-\\infty,+\\infty)\\)\n\n\n\\(f_{X}(x)\\)\n\\(=\\frac{1}{\\sqrt{2\\pi\\cdot\\sigma^2}}\\cdot e^{\\frac{-(x-\\mu)^2}{2\\cdot \\sigma^2}}\\mbox{ para todo }x\\in \\mathbb{R}.\\)\n\n\n\\(F_X(x)=P(X\\leq X)=\\)\nUtilizad la función de R pnorm(x,mean=mu,sd=sigma) o la función correspondiente en Python\n\n\n\\(E(X)=\\mu.\\)\n\\(Var(X)=\\sigma^2.\\)\n\n\n\nCálculos con R\nLas funciones que calculan la función de densidad y de distribución de una variable \\(N(\\mu,\\sigma)\\) en un valor x son dnorm(x,mean=mu,sd=sigma) y pnorm(x,mean=mu,sd=sigma), respectivamente. Por ejemplo, para una variable \\(X\\sim N(\\mu=1,\\sigma=2)\\), la función de densidad \\(f_X(2)\\) se puede calcular de la forma siguiente:\n\ndnorm(2,mean=1,sd=2)\n\n[1] 0.1760327\n\n\ny la función de distribución \\(F_X(2) = P(X\\leq 2)\\) de la forma siguiente:\n\npnorm(2,mean=1,sd=2) \n\n[1] 0.6914625\n\n\nSi queremos calcular el cuantil \\(x_{q}\\) de una distribución normal \\(N(\\mu,\\sigma)\\) que, recordemos es el valor que cumple que \\(P(X\\leq x_{q})=q\\), tenemos que usar la función qnorm(q,mean=mu,sd=sigma):\n\nqnorm(0.95,mean=1,sd=2)\n\n[1] 4.289707\n\n\nPara generar n valores aleatorios de una distribución normal \\(N(\\mu,\\sigma)\\), hay que usar la función rnorm(n,mean=mu,sd=sigma):\n\nrnorm(n=5,mean=1,sd=2)\n\n[1] -1.7813382  0.7320555  2.7209162  2.9098518  2.5098235\n\n\nCálculos con Python\nPara poder trabajar con una distribución normal \\(N(\\mu,\\sigma)\\) en Python, tenemos que importar norm de scipy.stas. Los parámetros \\(\\mu\\) y \\(\\sigma\\) son loc y scale, respectivamente.\n\nfrom scipy.stats import norm\n\nPor ejemplo, para una variable \\(X\\sim N(\\mu=1,\\sigma=2)\\), la función de densidad \\(f_X(2)\\) se puede calcular de la forma siguiente:\n\nnorm.pdf(2,loc=1,scale=2)\n\n0.17603266338214976\n\n\ny la función de distribución \\(F_X(2) = P(X\\leq 2)\\), de la forma siguiente:\n\nnorm.cdf(2,loc=1,scale=2)\n\n0.6914624612740131\n\n\nSi queremos calcular el cuantil \\(x_{q}\\) de una distribución normal \\(N(\\mu,\\sigma)\\), tenemos que usar la función norm.ppf(q,loc=mu,scale=sigma):\n\nnorm.ppf(0.95,loc=1,scale=2)\n\n4.289707253902945\n\n\nPara generar n valores aleatorios de una distribución normal \\(N(\\mu,\\sigma)\\), hay que usar la función norm.rvs(loc=mu,scale=sigma,size=n):\n\nnorm.rvs(loc=1,scale=2,size=5)\n\narray([ 0.00632078,  0.50263671,  0.41757297,  0.84675575, -2.82744019])\n\n\n\nEjercicio\nConsultad SciPy.org para dibujar las funciones de densidad y de distribución con Python.\n\nGráficas interactivas usando los parámetros de la distribución normal\nPara ejecutar el siguiente gráfico interactivo, solamente tienes que cargar el paquete shiny en tu ordenador y luego copiar/pegar las siguientes instrucciones. De este modo podrás observar los cambios en las distribuciones variando los parámetros.\n\nfluidPage(\nfluidRow(\n  column(3,\n         sliderInput(\"m1\", label = \"mu1\",\n              min = -10, max = 10, value =0 , step = 0.05)\n         ),\n  column(3,\n          sliderInput(\"s1\", label = \"sigma1\",\n                     min =0.1, max = 5, value = 1, step = 0.1)\n         ),\n  column(3,\n         sliderInput(\"m2\", label=\"mu2\", value=4, min = -10, max = 10, step = 0.05)\n         ),\n  column(3,\n          sliderInput(\"s2\", label = \"sigma2\",\n                     min =0.1, max = 5, value = 1, step = 0.1)\n         )\n  \n)\n)\n\nrenderPlot({\n  m1=input$m1\n  m2=input$m2\n  s1=input$s1\n  s2=input$s2\n  mins2=min(c(s1^2,s2^2))\nm=min(c(qnorm(0.01,m1,s1),qnorm(0.01,m2,s2)))\nM=max(c(qnorm(0.99,m1,s1),qnorm(0.99,m2,s2)))\n\ncurve(dnorm(x,m1,s1),xlim=c(m,M),ylim=c(0,1/sqrt(2*pi*mins2)),col=\"red\",lty=1)\nlegend(\"toplef\",legend=c(expression(N(mu[1],sigma[1])),\n                         expression(N(mu[2],sigma[2]))),\n       col=c(\"red\",\"blue\"),lty=c(1,2))\ncurve(dnorm(x,m2,s2),add=TRUE,col=\"blue\",lty=2)\n})\n\n\nTransformaciones lineales de variables aleatorias normales\n Propiedad: transformación lineal la distribución normal \nSea \\(X\\) una variable \\(N(\\mu,\\sigma)\\) entonces la variable \\(Y=a X+b\\) con \\(a\\not=0,b\\in\\mathbb{R}\\) tiene distribución \\(N(a\\mu+b, |a|\\cdot\\sigma)\\)\nEn el caso particular en que \\(a=\\frac1{\\sigma}\\) y \\(b= \\frac{-\\mu}{\\sigma}\\) obtenemos la v.a. \\(Z={{X-\\mu}\\over {\\sigma}}\\), que se distribuye según una normal estándar \\(N(0,1)\\), es decir \\(E(X)=0\\) y \\(Var(X)=1\\). Dicha operación se denomina estandarización de la distribución normal.\nLa propiedad anterior nos permite calcular la función de distribución de cualquier v.a. \\(N(\\mu,\\sigma)\\) a partir de la función de distribución de la distribución \\(Z=N(0,1)\\): \\[\nF_X(x)=P(X\\leq x)=P\\left(\\frac{X-\\mu}{\\sigma}\\leq \\frac{x-\\mu}{\\sigma}\\right)=P\\left(Z\\leq \\frac{x-\\mu}{\\sigma}\\right)=F_Z \\left(\\frac{x-\\mu}{\\sigma}\\right).\n\\] Entonces, basta conocer la manera de calcular \\(F_Z\\) para poder calcular \\(F_X\\), para \\(X=N(\\mu,\\sigma)\\), para cualquier \\(\\mu\\) y cualquier \\(\\sigma&gt;0\\).\nPropiedades de la distribución normal estándar\nSea \\(Z\\) una \\(N(0,1)\\).\nEn este caso, \\(\\mu=0\\) y \\(\\sigma=1\\). Podemos escribir algunas de las propiedades vistas para una distribución normal cualquiera de la forma siguiente:\n\nLa propiedad \\(f_X(\\mu-x)=f_X(\\mu+x)\\) se traduce a \\(f_Z(-x)=f_Z(x)\\)\nLa propiedad \\(F_X(\\mu-x)=1-F_X(\\mu+x)\\) se traduce a \\(F_Z(-x)=1-F(x).\\)\nDado \\(\\delta&gt;0\\), \\[\nP(-\\delta\\leq Z \\leq \\delta)=F_{Z}(\\delta)-F_{Z}(-\\delta)=F_Z(\\delta)-(1-F_Z(\\delta))=\n2\\cdot F_Z(\\delta)-1.\n\\]\n\n\nEjercicio: cálculos con la distribución normal estándar\nSea \\(Z\\) una distribución \\(N(0,1)\\). Calcular las siguientes probabilidades en función de \\(F_Z\\):\n\n\\(P(-4\\leq Z \\leq 4).\\)\n\\(P(-2\\leq Z \\leq 2).\\)\n\\(P(Z\\leq -2).\\)\n\\(P( Z \\leq 2).\\)\n\\(P( Z \\geq 2).\\)\n\\(P( Z &gt; 2).\\)\n\\(P( Z = 2).\\)\n\\(P( Z \\geq -2).\\)\n\n\nResolución:\n\n\\(P(-4\\leq Z \\leq 4)=F_{Z}(4)-F_{Z}(-4)=2\\cdot F_Z(4)-1\\).\n\\(P(-2\\leq Z \\leq 2)=F_{Z}(2)-F_{Z}(-2)=2\\cdot F_Z(2)-1\\).\n\\(P(Z\\leq -2)=F_Z(-2)=1-F_Z(2)\\).\n\\(P( Z \\leq 2)=F_{Z}(2)\\).\n\\(P( Z \\geq 2)=1-P(Z&lt;2)=1-F_{Z}(2)\\).\n\\(P( Z &gt; 2)=1-P(Z\\leq 2)=1-F_{Z}(2)\\).\n\\(P( Z = 2)=0\\) ya que es una distribución continua.\n\\(P( Z \\geq -2)=1-P(Z&lt; -2)=1-F_{Z}(-2)=1-(1-F_Z(2))=F_Z(2).\\)\n\n\n\nCálculo de probabilidades de la distribución normal \\(X=N(\\mu,\\sigma)\\) en un intervalo usando la distribución \\(Z=N(0,1)\\)\nPara hallar la probabilidad de que \\(X\\) esté en un intervalo \\((a,b)\\) cualquiera, podemos usar la función de distribución de \\(Z\\) de la siguiente manera: \\[\n\\begin{array}{ll}\nP(a&lt;X&lt;b)&=P\\left(\\frac{a-\\mu}{\\sigma}&lt;\\frac{X-\\mu}{\\sigma}&lt;\\frac{b-\\mu}{\\sigma}\\right)= \\\\\n&=P\\left(\\frac{a-\\mu}{\\sigma}&lt;Z&lt;\\frac{b-\\mu}{\\sigma}\\right)=F_{Z}\\left(\\frac{b-\\mu}{\\sigma}\\right)-\nF_{Z}\\left(\\frac{a-\\mu}{\\sigma}\\right).\n\\end{array}\n\\]\nPara el caso particular en que el intervalo esté centrado en la media \\(\\mu\\), o sea existe un valor \\(\\delta&gt;0\\) tal que \\((a,b)=(\\mu-\\delta,\\mu+\\delta)\\), obtenemos: \\[\nP\\left(\\mu-\\delta\\leq X \\leq\\mu+\\delta\\right)=2\\cdot  F_Z\\left(\\frac{\\delta}{\\sigma}\\right)-1.\n\\]\n\nEjercicio: cálculo de probabilidades de una distribución normal\nSea \\(X\\) una normal con media \\(2\\) y varianza \\(4\\). Calcular\n\n\\(P(1&lt; X&lt; 2).\\)\n\\(P(X&gt;3).\\)\n\n\nSolución\nLa primera probabilidad se calcula de la forma siguiente: \\[\n\\begin{array}{ll}\nP(1&lt; X&lt; 2)&= P\\left(\\frac{1-2}{2}&lt;\\frac{X-2}{2}&lt;\\frac{2-2}{2}\\right)= P\\left(\\frac{-1}{2}&lt;Z&lt;0\\right)\\\\\n&= F_{Z}(0)-F_{Z}(-0.5)=\\frac12-1+F_{Z}(0.5)=-\\frac12+F_Z(0.5).\n\\end{array}\n\\]\nLa segunda probabilidad se calcular de la forma siguiente: \\[\nP(X&gt;3)=P\\left(\\frac{X-2}2&gt;\\frac{3-2}{2}\\right)=P(Z&gt;0.5)=1-F_{Z}(0.5).\n\\]\n\n\n\nEjercicio\nSea \\(X\\) una normal con media \\(2\\) y varianza \\(4\\). Calcular usando R y con Python las probabilidades siguientes:\n\n\\(P(1&lt; X&lt; 2).\\)\n\\(P(X&gt;3).\\)\n\n\nSolución usando R\n\npnorm(2,mean=2,sd=2)-pnorm(1,mean=2,sd=2) #P(1&lt; X&lt; 2)\n\n[1] 0.1914625\n\npnorm(3,mean=2,sd=2,lower.tail =FALSE) #P(X&gt;3)\n\n[1] 0.3085375\n\n1-pnorm(3,mean=2,sd=2,lower.tail=TRUE) #P(X&gt;3) = 1-P(X&lt;=3)\n\n[1] 0.3085375\n\n\nSolución usando Python\n\nnorm.cdf(2,loc=2,scale=2)-norm.cdf(1,loc=2,scale=2) #P(1&lt; X&lt; 2)\n\n0.19146246127401312\n\n1-norm.cdf(3,loc=2,scale=2) #P(X&gt;3) = 1-P(X&lt;=3)\n\n0.3085375387259869\n\n\n\n\nLa distribución normal aproxima otras distribuciones\nEn los temas que siguen veremos como, bajo determinadas condiciones,\n\nla distribución normal puede aproximar la distribución binomial,\nla distribución normal puede aproximar la distribución Poisson\nla distribución normal es la distribución límite de la media aritmética de una muestra de variables aleatorias."
  },
  {
    "objectID": "4.html#momentos-de-variables-aleatorias",
    "href": "4.html#momentos-de-variables-aleatorias",
    "title": "5  Variables Aleatorias. Complementos",
    "section": "5.1 Momentos de variables aleatorias",
    "text": "5.1 Momentos de variables aleatorias\n\n5.1.1 Momento de orden \\(n\\)\n Definición.  Sea \\(X\\) una variable aleatoria. Definimos el momento de orden \\(n\\) como \\(m_n = E\\left(X^n\\right)\\).\n Observación. El momento de orden \\(1\\) de una variable aleatoria es su valor medio o \\(E(X)\\).\nLos momentos de orden \\(n\\) caracterizan una variable \\(X\\). O sea, que si conocemos todos los momentos de orden \\(n\\), podemos deducir cuál es la distribución de \\(X\\).\nEn general, el cálculo de los momentos de orden \\(n\\) para una variable \\(X\\) es bastante tedioso.\nEjemplos de momentos de orden \\(n\\)\n\nEjemplo: momento de orden \\(n\\) de una variable de Bernoulli de parámetro \\(p\\)\nSea \\(X\\) una variable de Bernoulli de parámetro \\(p\\). Recordemos que su función de probabilidad es: \\[\nP_X(0)=q=1-p,\\ p_X(1)=p.\n\\] Su momento de orden \\(n\\) será: \\[\nm_n = E\\left(X^n\\right)=p\\cdot 1^n+(1-p)\\cdot 0^n = p.\n\\] En este caso, todos los momentos de orden \\(n\\) valen \\(p\\).\n\n\nEjemplo: momento de orden \\(n\\) de una variable exponencial de parámetro \\(\\lambda\\)\nConsideremos ahora una variable \\(X\\) exponencial de parámetro \\(\\lambda\\).\nRecordemos que su función de densidad es: \\(f_X(x)=\\lambda \\mathrm{e}^{-\\lambda x},\\) para \\(x\\geq 0\\) y \\(0\\), en caso contrario.\nSu momento de orden \\(n\\) será: \\[\nm_n = E\\left(X^n\\right)=\\int_0^\\infty \\lambda \\mathrm{e}^{-\\lambda x} x^n\\, dx =\\frac{n!}{\\lambda^n}.\n\\]\nLa expresión anterior se puede obtener integrando por partes \\(n\\) veces y resolviendo los límites correspondientes. Dejámos al lector los cálculos correspondientes.\nFijémonos que los momentos de orden \\(n\\) tienden a infinito a medida que \\(n\\) crece: \\(\\lim\\limits_{n\\to\\infty}m_n = \\lim\\limits_{n\\to\\infty}\\frac{n!}{\\lambda^n}=\\infty\\).\n\n\nEjemplo: momento de orden \\(n\\) de una variable normal de parámetros \\(m=0\\) y \\(\\sigma =1\\)\nRecordemos que su función de densidad es: \\(f_X(x)=\\frac{1}{\\sqrt{2\\pi}}\\mathrm{e}^{-\\frac{x^2}{2}},\\) para \\(x\\in \\mathbb{R}\\).\nSu momento de orden 1 será la esperanza de \\(X\\): \\(m_1 = 0\\) i su momento de orden 2 será: \\(m_2 = E\\left(X^2\\right)=\\int_{-\\infty}^\\infty \\frac{1}{\\sqrt{2\\pi}}\\mathrm{e}^{-\\frac{x^2}{2}}\\cdot x^2\\, dx = 1.\\) La integral anterior se resuelve usando técnicas de integrales de dos variables. Dicho valor también se puede obtener usando que su varianza vale 1: \\(m_2 = \\mathrm{Var}(X)+E(X)^2 = \\sigma^2 +0^2 = 1.\\)\nLos momentos de orden impar \\(n\\) serán cero ya que integramos una función impar: \\(m_n = E\\left(X^n\\right)=\\int_{-\\infty}^\\infty \\frac{1}{\\sqrt{2\\pi}}\\mathrm{e}^{-\\frac{x^2}{2}}\\cdot x^n\\, dx = 0.\\) O sea, si consideramos \\(g(x)=\\frac{1}{\\sqrt{2\\pi}}\\mathrm{e}^{-\\frac{x^2}{2}}\\cdot x^n\\), se verifica \\(g(-x)=-g(x)\\), para todo \\(x\\in\\mathbb{R}\\).\nSi intentamos calcular el momento de orden 4, obtenemos: \\(m_4 = E\\left(X^4\\right)=\\int_{-\\infty}^\\infty \\frac{1}{\\sqrt{2\\pi}}\\mathrm{e}^{-\\frac{x^2}{2}}\\cdot x^4\\, dx = 3,\\) usando técnicas de integración de dos variables otra vez.\n\n\n\n5.1.2 Momento central de orden \\(n\\)\n Definición.  Sea \\(X\\) una variable aleatoria. Definimos el momento central de orden \\(n\\) como \\(\\mu_n = E\\left((X-\\mu)^n\\right)\\), donde \\(\\mu =E(X)\\) es la media o la esperanza de la variable aleatoria \\(X\\).\n Observación. El momento central de orden \\(1\\) de una variable aleatoria es siempre 0: \\[\n\\mu_1 = E\\left((X-\\mu)\\right)=E(X)-E(\\mu)=E(X)-E(X)=0.\n\\]\n Observación. El momento central de orden \\(2\\) de una variable aleatoria es la varianza: \\[\n\\mu_2 = E\\left((X-\\mu)^2\\right):= \\mathrm{Var}(X).\n\\]\nLos momentos centrales de orden \\(n\\) caracterizan también una variable \\(X\\). O sea, que si conocemos todos los momentos centrales de orden \\(n\\), podemos deducir cuál es la distribución de \\(X\\).\n Proposición. La relación que hay entre los momentos centrales y los momentos de una variable aleatoria es la siguiente: \\[\n\\mu_n = \\sum_{k=0}^n (-1)^{n-k} \\binom{n}{k} \\mu^{n-k} m_k = \\sum_{k=0}^n (-1)^{k} \\binom{n}{k} \\mu^{k} m_{n-k},\n\\] donde \\(\\mu =E(X)\\) recordemos que es la esperanza de la variable aleatoria \\(X\\).\n\nDemostración\nRecordemos la definición de momento central de orden \\(n\\) y desarrollemos su expresión aplicando el binomio de Newton: \\[\n\\mu_n = E\\left((X-\\mu)^n\\right) =E\\left(\\sum_{k=0}^n (-1)^{n-k} \\binom{n}{k} X^k\\mu^{n-k}\\right).\n\\] Aplicando la propiedad de la esperanza que la esperanza de la suma es la suma de esperanzas, obtenemos la expresión dada por la proposición: \\[\n\\mu_n =\\sum_{k=0}^n (-1)^{n-k} \\binom{n}{k} \\mu^{n-k} E\\left(X^k\\right) = \\sum_{k=0}^n (-1)^{n-k} \\binom{n}{k} \\mu^{n-k} m_k.\n\\]\n\nEjemplos de momentos centrales de orden \\(n\\)\n\nEjemplo: momento central de orden \\(n\\) de una variable de Bernoulli de parámetro \\(p\\)\nSea \\(X\\) una variable de Bernoulli de parámetro \\(p\\). Recordemos que su función de probabilidad es: \\[\nP_X(0)=q=1-p,\\ P_X(1)=p.\n\\] Usando que \\(E(X)=p\\), su momento central de orden \\(n\\) será: \\[\n\\mu_n = E\\left((X-p)^n\\right)=p\\cdot (1-p)^n+(1-p)\\cdot (0-p)^n = p(1-p)^n + (-1)^n (1-p) p^n.\n\\]\n\n\nEjercicio\nDemostrar que la expresión anterior corresponde a un polinomio de grado \\(n\\).\n\n\nEjemplo: momento central de orden \\(n\\) de una variable exponencial de parámetro \\(\\lambda\\)\nConsideremos ahora una variable \\(X\\) exponencial de parámetro \\(\\lambda\\).\nRecordemos que su función de densidad es: \\(f_X(x)=\\lambda \\mathrm{e}^{-\\lambda x},\\) para \\(x\\geq 0\\).\nUsando que \\(E(X)=\\frac{1}{\\lambda}\\), su momento central de orden \\(n\\) será: \\[\n\\mu_n = E\\left(\\left(X-\\frac{1}{\\lambda}\\right)^n\\right)=\\int_0^\\infty \\lambda \\mathrm{e}^{-\\lambda x} \\left(x-\\frac{1}{\\lambda}\\right)^n\\, dx =\\frac{a_n}{\\lambda^n},\n\\] donde \\(a_n = n!\\sum\\limits_{k=0}^n \\frac{(-1)^k}{k!}.\\)\nLa expresión anterior fijado \\(n\\) se puede obtener integrando por partes \\(n\\) veces y resolviendo los límites correspondientes. Dejámos al lector los cálculos correspondientes. Sin embargo, la obtención de la fórmula general para \\(n\\) se sale del nivel del curso.\nFijémonos que los momentos centrales de orden \\(n\\) también tienden a infinito a medida que \\(n\\) crece: \\(\\lim\\limits_{n\\to\\infty}\\mu_n = \\lim\\limits_{n\\to\\infty}\\frac{a_n}{\\lambda^n}=\\infty\\): \\[\n\\lim_{n\\to\\infty}\\mu_n =\\lim_{n\\to\\infty} \\frac{n!\\sum\\limits_{k=0}^n \\frac{(-1)^k}{k!}}{\\lambda^n}=\n\\lim_{n\\to\\infty}\\sum\\limits_{k=0}^n \\frac{(-1)^k}{k!}\\cdot \\lim_{n\\to\\infty} \\frac{n!}{\\lambda^n}= \\mathrm{e}^{-1}\\cdot \\infty = \\infty.\n\\]\n\n\nEjemplo: momento central de orden \\(n\\) de una variable normal de parámetros \\(\\mu\\) y \\(\\sigma\\)\nRecordemos que su función de densidad es: \\(f_X(x)=\\frac{1}{\\sqrt{2\\pi}\\sigma}\\mathrm{e}^{-\\frac{(x-\\mu)^2}{2\\sigma^2}},\\) para \\(x\\in \\mathbb{R}\\).\nSu momento central de orden 2 será la varianza \\(\\sigma^2\\): \\(\\mu_2 =\\sigma^2.\\)\nLos momentos centrales de orden impar \\(n\\) serán cero ya que integramos una función impar respecto \\(x=\\mu\\): \\(\\mu_n = E\\left((X-\\mu)^n\\right)=\\int_{-\\infty}^\\infty \\frac{1}{\\sqrt{2\\pi}\\sigma}\\mathrm{e}^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\cdot (x-\\mu)^n\\, dx = 0.\\) O sea, si consideramos \\(g(x)=\\frac{1}{\\sqrt{2\\pi}\\sigma}\\mathrm{e}^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\cdot (x-\\mu)^n\\), se verifica \\(g(\\mu-x)=-g(\\mu +x)\\), para todo \\(x\\in\\mathbb{R}\\).\nSi intentamos calcular el momento central de orden 4, obtenemos: \\(\\mu_4 = E\\left((X-\\mu)^4\\right)=\\int_{-\\infty}^\\infty \\frac{1}{\\sqrt{2\\pi}\\sigma}\\mathrm{e}^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\cdot (x-\\mu)^4\\, dx = 3\\sigma^4.\\) La integral anterior puede resolverse con el cambio de variable \\(t=\\frac{x-\\mu}{\\sigma}\\) y usando que: \\(\\int_{-\\infty}^\\infty \\frac{1}{\\sqrt{2\\pi}}\\mathrm{e}^{-\\frac{x^2}{2}}\\cdot x^4\\, dx = 3.\\)"
  },
  {
    "objectID": "4.html#asimetría-de-una-variable-aleatoria",
    "href": "4.html#asimetría-de-una-variable-aleatoria",
    "title": "5  Variables Aleatorias. Complementos",
    "section": "5.2 Asimetría de una variable aleatoria",
    "text": "5.2 Asimetría de una variable aleatoria\nUna variable aleatoria tiene asimetría positiva si su función de densidad o de probabilidad presenta una cola a la derecha y asimetría negativa, si su función de densidad o de probabilidad presenta cola a la izquierda.\nPor ejemplo, en la figura siguiente, vemos la gráfica de la función de probabilidad de una variable aleatoria que presenta asimetría negativa a la izquierda y una función de densidad de una variable aleatoria que presenta asimetría positiva a la derecha:\n\n\n\n\n\n\n\n¿Cómo calcular la asimetría de una variable aleatoria?\nLa asimetría de una variable aleatoria \\(X\\) se calcula a partir de sus momentos centrales de segundo y tercer orden: \\[\n\\gamma_1 = E\\left({\\left(\\frac{X-\\mu}{\\sigma}\\right)}^3\\right)=\\frac{\\mu_3}{\\sigma^3},\n\\] donde \\(\\mu = E(X)\\) y \\(\\sigma^2 =\\mathrm{Var}(X)\\).\nDicho valor se denomina coeficiente de asimetría de Pearson.\nUsando la relación ya vista entre los momentos centrales y los momentos, podemos expresar el coeficiente de asimetría en función de los momentos: \\[\n\\gamma_1 = \\frac{m_3 -3\\mu\\sigma^2-\\mu^3}{\\sigma^3}.\n\\] Dejamos al lector la comprobación de la expresión anterior.\nPor tanto, una variable aleatoria \\(X\\) tendrá simetría positiva o a la derecha si \\(\\gamma_1 &gt;0\\) y tendrá asimetría negativa o a la izquierda, si \\(\\gamma_1 &lt;0\\).\n\nEjemplo: cálculo del coeficiente de asimetría para una variable de Bernoulli de parámetro \\(p\\)\nSea \\(X\\) una variable de Bernoulli de parámetro \\(p\\). Usando que \\(m_n =p\\), para todo \\(n\\) y que \\(\\mu_2 = \\sigma^2 = p-p^2\\), el coeficiente de asimetría \\(\\gamma_1\\) será: \\[\n\\gamma_1 = \\frac{p-3p(p-p^2)-p^3}{\\sqrt{(p-p^2)^3}} = \\frac{p (1-p) (1-2p)}{{\\sqrt{(p-p^2)^3}}}.\n\\] Por tanto, la variable de Bernoulli de parámetro \\(p\\) tendrá simetria negativa si \\(p&gt;\\frac{1}{2}\\) y positiva, si \\(p&lt;\\frac{1}{2}\\):\n\n\n\n\n\n\n\n\n\nEjemplo: cálculo del coeficiente de asimetría para una variable exponencial de parámetro \\(\\lambda\\)\nSea \\(X\\) una variable exponencial de parámetro \\(\\lambda\\). Usando que \\(\\sigma^2=\\frac{1}{\\lambda^2}\\) y \\(\\mu_3 =\\frac{a_3}{\\lambda^3}=\\frac{2}{\\lambda^3}\\), su coeficiente de asimetría de Pearson será: \\(\\gamma_1 = \\frac{\\frac{2}{\\lambda^3}}{\\frac{1}{\\lambda^3}}=2.\\)\nEntonces presenta asimetría positiva o a la derecha tal como se observa en su función de densidad:\n\n\n\n\n\n\n\n\n\nEjemplo: cálculo del coeficiente de asimetría para una variable normal de parámetros \\(\\mu\\) y \\(\\sigma\\)\nSea \\(X\\) una variable aleatoria normal de parámetros \\(\\mu\\) y \\(\\sigma\\).\nTal como se ha indicado anteriormente, los momentos centrales de orden impar son nulos.\nPor tanto, en este caso \\(\\mu_3=0\\) y, por tanto, \\(\\gamma_1=0\\).\nDeducimos que la distribución normal es totalmente simétrica.\nDe hecho, usando que su función de densidad es \\(f_X(x)=\\frac{1}{\\sqrt{2\\pi}\\sigma}\\mathrm{e}^{-\\frac{(x-\\mu)^2}{2\\sigma^2}},\\) para \\(x\\in \\mathbb{R}\\), se puede comprobar que \\(f_X(\\mu-x)=f_X(\\mu +x)\\), o sea, tiene el eje de simetría \\(x=\\mu\\):"
  },
  {
    "objectID": "4.html#curtosis-o-apuntamiento-de-una-variable-aleatoria",
    "href": "4.html#curtosis-o-apuntamiento-de-una-variable-aleatoria",
    "title": "5  Variables Aleatorias. Complementos",
    "section": "5.3 Curtosis o apuntamiento de una variable aleatoria",
    "text": "5.3 Curtosis o apuntamiento de una variable aleatoria\nLa curtosis de una variable aleatoria \\(X\\) es una medida de cómo son las colas de su función de densidad.\nDicho en otras palabras, queremos medir de alguna manera la tendencia que tiene la variable aleatoria a tener valores atípicos o outliers.\nLa manera estándard de medir la curtosis de una variable aleatoria \\(X\\) es a partir de su momento central de cuarto orden: \\[\n\\gamma_2 = E\\left(\\left(\\frac{X-\\mu}{\\sigma}\\right)^4\\right) = \\frac{\\mu_4}{\\sigma^4},\n\\] donde recordemos que \\(\\mu=E(X)\\) y \\(\\sigma^2 =\\mathrm{Var}(X)\\).\nA la expresión anterior se le denomina medida de curtosis de Pearson.\n\nDiremos que una variable aleatoria no tiene exceso de curtosis o mesocúrtica si \\(\\gamma_2 \\approx 3\\).\nDiremos que una variable aleatoria tiene exceso positivo de curtosis o leptocúrtica si \\(\\gamma_2 &gt;3\\).\nDiremos que una variable aleatoria tiene exceso negativo de curtosis o platicúrtica si \\(\\gamma_2 &lt;3\\).\n\n\nEjemplo: cálculo del coeficiente de curtosis para una variable de Bernoulli de parámetro \\(p\\)\nSea \\(X\\) una variable aleatoria de parámetro \\(p\\).\nEl momento central de cuarto orden de \\(X\\) será: \\[\n\\mu_4 = p (1-p)^4 +(1-p)p^4 = p (1-p) (3 p^2-3p+1).\n\\] La medida de curtosis de Pearson será: \\[\n\\gamma_2 = \\frac{p (1-p) (3 p^2-3p+1)}{p^2 (1-p)^2} = \\frac{3 p^2-3p+1}{p(1-p)}.\n\\] Se puede comprobar (ejercicio para el lector) que si \\(p\\in \\left(\\frac{3-\\sqrt{3}}{6},\\frac{3+\\sqrt{3}}{6}\\right)\\approx (0.211,0.789)\\), \\(\\gamma_2 &lt;3\\) y, por tanto \\(X\\) será platicúrtica y en caso contrario, si \\(p\\in \\left(0,\\frac{3-\\sqrt{3}}{6}\\right)\\cup \\left(\\frac{3+\\sqrt{3}}{6},1\\right)\\), \\(\\gamma_2 &gt;3\\) y, por tanto, \\(X\\) será leptocúrtica.\n\n\nEjemplo: cálculo del coeficiente de curtosis para una variable exponencial de parámetro \\(\\lambda\\)\nSea \\(X\\) una variable exponencial de parámetro \\(\\lambda\\). Usando que \\(\\sigma^2=\\frac{1}{\\lambda^2}\\) y \\(\\mu_4 =\\frac{a_4}{\\lambda^3}=\\frac{9}{\\lambda^4}\\), su coeficiente de asimetría de Pearson será: \\(\\gamma_2 = \\frac{\\frac{9}{\\lambda^4}}{\\frac{1}{\\lambda^4}}=9.\\)\nComo \\(\\gamma_2 &gt;3\\), se trataría de una distribución leptocúrtica.\n\n\nEjemplo: cálculo del coeficiente de curtosis para una variable normal de parámetros \\(\\mu\\) y \\(\\sigma\\)\nSea \\(X\\) una variable aleatoria normal de parámetros \\(\\mu\\) y \\(\\sigma\\).\nTal como se ha indicado anteriormente, el momento central de orden 4 vale: \\(\\mu_4 = 3\\sigma^4\\).\nSu coeficiente de curtosis será: \\[\n\\gamma_2 =\\frac{\\mu_4}{\\sigma^4}=\\frac{3\\sigma^4}{\\sigma^4}=3.\n\\] Deducimos, por tanto, que toda distribución normal es mesocúrtica o no tiene exceso (ni positivo ni negativo) de curtosis."
  },
  {
    "objectID": "4.html#métodos-de-transformación",
    "href": "4.html#métodos-de-transformación",
    "title": "5  Variables Aleatorias. Complementos",
    "section": "5.4 Métodos de transformación",
    "text": "5.4 Métodos de transformación\nHemos visto anteriormente que el cálculo de los momentos o los momentos centrados de una variable aleatoria \\(X\\) puede ser muy complicado y muy tedioso.\nPor dicho motivo, vamos a introducir un conjunto de funciones que nos permitirán calcular los momentos de la variable \\(X\\) de forma relativamente sencilla.\n\n5.4.1 Función generatriz de momentos\nDefinición de función generatriz de momentos:  Sea \\(X\\) una variable aleatoria \\(X\\) con función de probabilidad \\(P_X\\) en el caso discreto o función de densidad \\(f_X\\) en el caso continuo.\nSea \\(t\\in\\mathbb{R}\\) un valor real cualquiera.\nDefinimos la función generatriz de momentos \\(m_X(t)\\) en el valor \\(t\\) como: \\(m_X(t)=E\\left(\\mathrm{e}^{tX}\\right).\\)\n\nEjemplo: cálculo de la función generatriz de momentos para una variable de Bernoulli de parámetro \\(p\\)\nSea \\(X\\) una variable aleatoria de Bernoulli de parámetro \\(p\\). Recordemos que su función de probabilidad es: \\[\nP_X(0)=q=1-p,\\ p_X(1)=p.\n\\] Su función generatriz de momentos será: \\[\nm_X (t)=E\\left(\\mathrm{e}^{tX}\\right) =p\\mathrm{e}^{t\\cdot 1}+(1-p)\\mathrm{e}^{t\\cdot 0}=p\\mathrm{e}^t+(1-p)=1+p\\left(\\mathrm{e}^t -1 \\right).\n\\]\n\n\nEjemplo: cálculo de la función generatriz de momentos para una variable exponencial de parámetro \\(\\lambda\\)\nSea \\(X\\) una variable aleatoria exponencial de parámetro \\(\\lambda\\). Recordemos que su función de densidad es: \\(f_X(x)=\\lambda \\mathrm{e}^{-\\lambda x},\\) para \\(x\\geq 0\\) y \\(0\\), en caso contrario.\nSu función generatriz de momentos será: \\[\nm_X (t)=E\\left(\\mathrm{e}^{tX}\\right)=\\int_0^\\infty \\mathrm{e}^{t x}\\lambda \\mathrm{e}^{-\\lambda x}\\, dx = \\lambda \\int_0^\\infty\\mathrm{e}^{(t-\\lambda)x}\\, dx = \\lambda\\left[\\frac{\\mathrm{e}^{(t-\\lambda)x}}{t-\\lambda}\\right]_{x=0}^{x=\\infty} = \\frac{\\lambda}{\\lambda -t},\\ \\mbox{si } t&lt;\\lambda.\n\\] En este caso vemos que el dominio de la función generatriz de momentos \\(m_X\\) es \\((-\\infty,\\lambda)\\), ya que si \\(t\\geq \\lambda\\), la integral anterior no es convergente.\nFijémonos por lo que vendrá más adelante que, como \\(\\lambda &gt;0\\), el valor \\(0\\) pertenece al dominio de \\(m_X\\).\n\n\nEjemplo: cálculo de la función generatriz de momentos para una variable normal de parámetros \\(\\mu\\) y \\(\\sigma\\)\nSea \\(X\\) una variable normal de parámetros \\(\\mu\\) y \\(\\sigma\\).\nRecordemos que su función de densidad es: \\(f_X(x)=\\frac{1}{\\sqrt{2\\pi}\\sigma}\\mathrm{e}^{-\\frac{(x-\\mu)^2}{2\\sigma^2}},\\) para \\(x\\in \\mathbb{R}\\).\nSu función generatriz de momentos será:\n\\[\n\\begin{array}{rl}\nm_X (t) & =E\\left(\\mathrm{e}^{tX}\\right)=\\displaystyle\\int_{-\\infty}^\\infty \\mathrm{e}^{tx}\\frac{1}{\\sqrt{2\\pi}\\sigma}\\mathrm{e}^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\, dx = \\frac{1}{\\sqrt{2\\pi}\\sigma} \\int_{-\\infty}^\\infty \\mathrm{e}^{tx-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\, dx \\\\[1ex]  & =  \\displaystyle\\frac{1}{\\sqrt{2\\pi}\\sigma} \\int_{-\\infty}^\\infty \\mathrm{e}^{-\\frac{1}{2\\sigma^2}\\left((x-(\\sigma^2 t+\\mu))^2-2\\sigma^2 t \\mu-\\sigma^4t^2\\right)}\\, dx = \\frac{1}{\\sqrt{2\\pi}\\sigma} \\mathrm{e}^{\\frac{1}{2}(2 t \\mu +\\sigma^2 t^2)}\\int_{-\\infty}^\\infty \\mathrm{e}^{-\\frac{1}{2\\sigma^2}(x-(\\sigma^2 t+\\mu))^2}\\, dx\\\\[1ex] &  = \\displaystyle\\mathrm{e}^{\\frac{1}{2}(2 t \\mu +\\sigma^2 t^2)} \\left( \\frac{1}{\\sqrt{2\\pi}\\sigma} \\int_{-\\infty}^\\infty \\mathrm{e}^{-\\frac{1}{2\\sigma^2}(x-(\\sigma^2 t+\\mu))^2}\\, dx\\right) =  \\mathrm{e}^{ t \\mu +\\frac{\\sigma^2 t^2}{2}}.\n\\end{array}\n\\] La integral del último paréntesis se resuelve haciento el cambio de variable \\(u=x-\\sigma^2 t\\) y usando que la integral de la función de densidad de \\(X\\) sobre todo \\(\\mathbb{R}\\) vale 1.\n\nRelación entre la función generatriz de momentos y los momentos\nLa razón del nombre que lleva la función generatriz de momentos es que podemos obtener todos los momentos de la variable a partir de ella:\n Proposición.  Sean \\(X\\) una variable aleatoria con función generatriz de momentos \\(m_X(t)\\). Entonces, el momento de orden \\(n\\) de \\(X\\) se puede obtener de la forma siguiente: \\[\nm_n =E\\left(X^n\\right)=\\frac{d}{d t^n}m_X(t)|_{t=0} =m_X^{(n)}(0).\n\\] O sea, el momento de orden \\(n\\) de \\(X\\) es la derivada \\(n\\)-ésima de la función generatriz de momentos evaluada en \\(t=0\\).\n\nDemostración\nRecordemos la definición de la función generatriz de momentos: \\(m_X(t)=E\\left(\\mathrm{e}^{tX}\\right).\\)\nLa idea de la demostración es probar por inducción que \\(m_X^{(n)}(t) =E\\left(\\mathrm{e}^{tX}\\cdot X^n\\right)\\).\nVeámoslo para \\(n=1\\): \\(m_X'(t)=E\\left(\\mathrm{e}^{tX}\\cdot X\\right)\\).\nSeguidamente, apliquemos inducción sobre \\(n\\). Supongamos que \\(m_X^{(n)}(t) =E\\left(\\mathrm{e}^{tX}\\cdot X^n\\right)\\) y veamos que \\(m_X^{(n+1)}(t) =E\\left(\\mathrm{e}^{tX}\\cdot X^{n+1}\\right)\\): \\(m_X^{(n+1)}(t) =\\frac{d}{dt}(m_X^{(n)}(t)) =\\frac{d}{dt}E\\left(\\mathrm{e}^{tX}\\cdot X^n\\right) = E\\left(\\mathrm{e}^{tX}\\cdot X^{n+1}\\right),\\) tal como queríamos demostrar.\nAhora si aplicamos la expresión demostrada \\(m_X^{(n)}(t) =E\\left(\\mathrm{e}^{tX}\\cdot X^n\\right)\\) a \\(t=0\\), obtenemos: \\(m_X^{(n)}(0) =E\\left(X^n\\right)=m_n,\\) tal como dice la proposición.\n\n\nEjemplo: aplicación de la proposición en el caso en que \\(X\\) es una variable de Bernoulli de parámetro \\(p\\)\nEn este caso, recordemos que: \\(m_X (t)=1+p\\left(\\mathrm{e}^t -1 \\right).\\)\nSe puede comprobar que \\(m_X^{(n)}(t)=p\\mathrm{e}^t\\). Por tanto: \\[\nm_n = m_X^{(n)}(0)=p,\n\\] tal como habíamos calculado anteriormente.\n\n\nEjemplo: aplicación de la proposición en el caso en que \\(X\\) es una variable exponencial de parámetro \\(\\lambda\\)\nEn este caso, recordemos que: \\(m_X (t)=\\frac{\\lambda}{\\lambda -t},\\) para \\(t&lt;\\lambda\\) pero como \\(\\lambda &gt;0\\), \\(t=0\\) cumple la expresión anterior.\nDejamos como ejercicio para el lector comprobar que: \\(m_X^{(n)}(t)=\\frac{\\lambda n!}{(\\lambda-t)^{n+1}}\\).\nPor tanto: \\[\nm_n = m_X^{(n)}(0) = \\frac{\\lambda n!}{\\lambda^{n+1}}=\\frac{n!}{\\lambda^n},\n\\] expresión que ya habíamos obtenido anteriormente.\n\n\nEjemplo: aplicación de la proposición en el caso en que \\(X\\) es una variable normal de parámetros \\(\\mu\\) y \\(\\sigma\\)\nEn este caso, recordemos que: \\(m_X (t)=\\mathrm{e}^{ t \\mu +\\frac{\\sigma^2 t^2}{2}}.\\)\nAplicando la fórmula de los momentos para \\(n=1\\) obtenemos: \\(m'(t)=\\mathrm{e}^{ t \\mu +\\frac{\\sigma^2 t^2}{2}} \\left(\\mu+t\\sigma^2\\right)\\), que en \\(t=0\\) vale: \\(m'(0)=\\mu=E(X)\\), tal como ya sabemos.\nSi la aplicamos para \\(n=2\\), obtenemos: \\(m''(t)=\\mathrm{e}^{ t \\mu +\\frac{\\sigma^2 t^2}{2}} \\left((\\mu+t\\sigma^2)^2+ \\sigma^2 \\right) =\\mathrm{e}^{ t \\mu +\\frac{\\sigma^2 t^2}{2}} \\left(t^2\\sigma^4+\\mu^2+\\sigma^2+ 2t\\mu\\sigma^2 \\right)\\), que en \\(t=0\\) vale: \\(m''(0)=\\mu^2+\\sigma^2=E\\left(X^2\\right)\\), tal como ya sabemos.\nPara \\(n=3\\)m obtenemos: \\(m'''(t)=e^{\\mu t+\\frac{\\sigma ^2 t^2}{2}}\\left(\\mu +\\sigma ^2 t\\right) \\left(\\left(\\mu +\\sigma ^2 t\\right)^2+3 \\sigma ^2\\right)\\), que en \\(t=0\\) vale: \\(m'''(0)=3\\sigma^2\\mu = E\\left(X^3\\right)\\), valor que correspondería al momento de tercer orden de \\(X\\).\nPor último, para \\(n=4\\), obtenemos: \\(m^{(iv)}(t)=e^{\\mu t+\\frac{\\sigma ^2 t^2}{2}}  \\left(6 \\sigma ^2 \\left(\\mu  +\\sigma ^2 t\\right)^2+\\left(\\mu  +\\sigma ^2 t\\right)^4+3 \\sigma  ^4\\right)\\), que en \\(t=0\\) vale: \\(m^{(iv)}(0)=6\\sigma^2\\mu^2+\\mu^4+3\\sigma^4=E\\left(X^4\\right)\\), valor que correspondería al momento de cuarto orden de \\(X\\).\n\n\n\n5.4.2 Función característica\nDefinición de función característica:  Sea \\(X\\) una variable aleatoria \\(X\\) con función de probabilidad \\(P_X\\) en el caso discreto o función de densidad \\(f_X\\) en el caso continuo.\nSea \\(w\\in\\mathbb{R}\\) un valor real cualquiera.\nDefinimos la función característica \\(\\phi_X(w)\\) en el valor \\(w\\) como: \\(\\phi_X(w)=E\\left(\\mathrm{e}^{\\mathrm{i} w X}\\right),\\) donde \\(\\mathrm{i}\\) es el número complejo \\(\\mathrm{i}=\\sqrt{-1}\\).\nObservación:  Si \\(X\\) es una variable continua, la función característica \\(\\phi_X(w)\\) puede interpretarse como la transformada de Fourier de la función de densidad de \\(X\\): \\(\\phi(w)=\\int_{-\\infty}^\\infty f_X(x)\\mathrm{e}^{\\mathrm{i}w x}\\, dx.\\)\nPor tanto, usando la fórmula de la antitransformada de Fourier, podemos escribir la función de densidad \\(f_X(x)\\) como función de la función característica de \\(X\\), \\(\\phi(w)\\): \\(f_X(x)=\\frac{1}{2\\pi}\\int_{-\\infty}^\\infty \\phi_X(w)\\mathrm{e}^{-\\mathrm{i}w x}\\, dw.\\)\nObservación:  En el caso discreto, o sea, Si \\(X\\) es una variable discreta, la función característica \\(\\phi_X(w)\\) se escribe como función de la función de probabilidad \\(P_X(x_k)\\) con Dominio \\(D_X=\\{x_k,\\ k\\}\\) como: \\(\\phi(w)=\\sum_{k} P_X(x_k)\\mathrm{e}^{\\mathrm{i}w x_k}.\\)\nEn los casos en que los \\(x_k\\) sean enteros, \\(x_k=k\\), que son la mayoría, la ecuación anterior es la tranformada de Fourier de la secuencia \\(P_X(k)\\). Dicha función es una función periódica en \\(w\\) de periodo \\(2\\pi\\) ya que \\(\\mathrm{e}^{\\mathrm{i}(w+2\\pi)k}=\\mathrm{e}^{\\mathrm{i}wk}.\\)\nPor tanto, usando la fórmula de inversión, podemos escribir la función de probabilidad \\(P_X(k)\\) como función de la función característica de \\(X\\), \\(\\phi(w)\\): \\(P_X(k)=\\frac{1}{2\\pi}\\int_{0}^{2\\pi} \\phi_X(w)\\mathrm{e}^{-\\mathrm{i}w k}\\, dw.\\)\n\nEjemplo: cálculo de la función característica para una variable de Bernoulli de parámetro \\(p\\)\nSea \\(X\\) una variable aleatoria de Bernoulli de parámetro \\(p\\). Recordemos que su función de probabilidad es: \\[\nP_X(0)=q=1-p,\\ p_X(1)=p.\n\\] Su función característica será: \\[\n\\phi_X (w)=E\\left(\\mathrm{e}^{\\mathrm{i}wX}\\right) =p\\mathrm{e}^{\\mathrm{i}w\\cdot 1}+(1-p)\\mathrm{e}^{\\mathrm{i}w\\cdot 0}=p\\mathrm{e}^{\\mathrm{i}w}+(1-p)=1+p\\left(\\mathrm{e}^{\\mathrm{i}w} -1 \\right).\n\\] Comprobemos la fórmula de la inversión: \\[\n\\begin{array}{rl}\nP_X(1) & = \\frac{1}{2\\pi}\\int_0^{2\\pi} \\left(1+p\\left(\\mathrm{e}^{\\mathrm{i}w} -1 \\right)\\right) e^{-\\mathrm{i}w\\cdot 1}\\, dw =\\frac{1}{2\\pi}\\left(\\int_0^{2\\pi} (1-p)e^{-\\mathrm{i}w}\\, dw + \\int_0^{2\\pi} p\\, dw\\right) \\\\ & = \\frac{1}{2\\pi}\\left( (1-p) \\left[\\frac{\\mathrm{e}^{-\\mathrm{i}w}}{-\\mathrm{i}}\\right]_0^{2\\pi} +2\\pi p\\right)=\\frac{1}{2\\pi}\\left((1-p)\\cdot 0 +2\\pi p\\right)=p, \\\\\nP_X(0) & = \\frac{1}{2\\pi}\\int_0^{2\\pi} \\left(1+p\\left(\\mathrm{e}^{\\mathrm{i}w} -1 \\right)\\right) e^{-\\mathrm{i}w\\cdot 0}\\, dw =\\frac{1}{2\\pi}\\left(\\int_0^{2\\pi} (1-p) \\, dw + \\int_0^{2\\pi} p \\mathrm{e}^{\\mathrm{i}w}\\, dw\\right) \\\\ & = \\frac{1}{2\\pi}\\left( (1-p) \\cdot 2\\pi  +p \\left[\\frac{\\mathrm{e}^{\\mathrm{i}w}}{\\mathrm{i}}\\right]_0^{2\\pi}\\right)=\\frac{1}{2\\pi}\\left((1-p)\\cdot 2\\pi + p\\cdot 0\\right)=1-p.\n\\end{array}\n\\]\n\n\nEjemplo: cálculo de la función característica para una variable exponencial de parámetro \\(\\lambda\\)\nSea \\(X\\) una variable aleatoria exponencial de parámetro \\(\\lambda\\). Recordemos que su función de densidad es: \\(f_X(x)=\\lambda \\mathrm{e}^{-\\lambda x},\\) para \\(x\\geq 0\\) y \\(0\\), en caso contrario.\nSu función característica será: \\[\n\\phi_X (w)=E\\left(\\mathrm{e}^{\\mathrm{i}wX}\\right)=\\int_0^\\infty \\mathrm{e}^{\\mathrm{i}w x}\\lambda \\mathrm{e}^{-\\lambda x}\\, dx = \\lambda \\int_0^\\infty\\mathrm{e}^{(\\mathrm{i}w-\\lambda)x}\\, dx = \\lambda\\left[\\frac{\\mathrm{e}^{(\\mathrm{i}w-\\lambda)x}}{\\mathrm{i}w-\\lambda}\\right]_{x=0}^{x=\\infty} = \\frac{\\lambda}{\\lambda -\\mathrm{i} w}.\n\\] La expresión anterior es válida para todo \\(w\\in\\mathbb{R}\\) ya que su valor sería: \\(\\phi_X (w)=\\frac{\\lambda}{\\lambda -\\mathrm{i} w}\\cdot \\frac{\\lambda +\\mathrm{i} w}{\\lambda +\\mathrm{i} w}=\\frac{\\lambda^2+\\mathrm{i}\\lambda w}{\\lambda^2+w^2}=\\frac{\\lambda^2}{\\lambda^2+w^2}+\\mathrm{i}\\frac{\\lambda w}{\\lambda^2+w^2}.\\) En la última expresión hemos separado la parte real de la imaginaria.\nCalculemos la función de densidad a partir de la función característica: \\[\nf_X(x)=\\frac{1}{2\\pi}\\int_{-\\infty}^\\infty \\frac{\\lambda}{\\lambda -\\mathrm{i} w}\\mathrm{e}^{-\\mathrm{i}wx}\\, dw = a\\mathrm{e}^{-a x},\n\\] si \\(x&gt;0\\) y \\(0\\) en caso contrario. El cálculo de la integral anterior debe realizarse usando el Teorema de los Residuos, Residue theorem y se sale de los objetivos de este curso.\n\n\nEjemplo: cálculo de la función característica para una variable normal de parámetros \\(\\mu\\) y \\(\\sigma\\)\nSea \\(X\\) una variable normal de parámetros \\(\\mu\\) y \\(\\sigma\\).\nRecordemos que su función de densidad es: \\(f_X(x)=\\frac{1}{\\sqrt{2\\pi}\\sigma}\\mathrm{e}^{-\\frac{(x-\\mu)^2}{2\\sigma^2}},\\) para \\(x\\in \\mathbb{R}\\).\nSu función característica será:\n\\[\n\\begin{array}{rl}\n\\phi_X (w) & =\\displaystyle E\\left(\\mathrm{e}^{\\mathrm{i}w X}\\right)=\\int_{-\\infty}^\\infty \\mathrm{e}^{\\mathrm{i}w x}\\frac{1}{\\sqrt{2\\pi}\\sigma}\\mathrm{e}^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\, dx = \\frac{1}{\\sqrt{2\\pi}\\sigma} \\int_{-\\infty}^\\infty \\mathrm{e}^{\\mathrm{i}wx-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\, dx \\\\[1ex]  & =\\displaystyle  \\frac{1}{\\sqrt{2\\pi}\\sigma} \\int_{-\\infty}^\\infty \\mathrm{e}^{-\\frac{1}{2\\sigma^2}\\left((x-(\\sigma^2 \\mathrm{i}w+\\mu))^2-2\\sigma^2 \\mathrm{i}w \\mu+\\sigma^4 w^2\\right)}\\, dx = \\frac{1}{\\sqrt{2\\pi}\\sigma} \\mathrm{e}^{\\frac{1}{2}(2 \\mathrm{i}w \\mu -\\sigma^2 w^2)}\\int_{-\\infty}^\\infty \\mathrm{e}^{-\\frac{1}{2\\sigma^2}(x-(\\sigma^2 \\mathrm{i}w+\\mu))^2}\\, dx\\\\[1ex] &  = \\displaystyle\\mathrm{e}^{\\frac{1}{2}(2 \\mathrm{i}w \\mu -\\sigma^2 w^2)} \\left( \\frac{1}{\\sqrt{2\\pi}\\sigma} \\int_{-\\infty}^\\infty \\mathrm{e}^{-\\frac{1}{2\\sigma^2}(x-(\\sigma^2 \\mathrm{i}w+\\mu))^2}\\, dx\\right) =  \\mathrm{e}^{ \\mathrm{i}w \\mu -\\frac{\\sigma^2 w^2}{2}}.\n\\end{array}\n\\] La integral del último paréntesis se resuelve haciento el cambio de variable \\(u=x-\\sigma^2 \\mathrm{i}w\\) y usando que la integral de la función de densidad de \\(X\\) sobre todo \\(\\mathbb{R}\\) vale 1.\nCalculemos la función de densidad a partir de la función característica: \\[\n\\begin{array}{rl}\nf_X(x) & =\\displaystyle\\frac{1}{2\\pi}\\int_{-\\infty}^\\infty \\mathrm{e}^{ \\mathrm{i}w \\mu -\\frac{\\sigma^2 w^2}{2}}\\mathrm{e}^{-\\mathrm{i} w x}\\, dw = \\frac{1}{2\\pi}\\int_{-\\infty}^\\infty \\mathrm{e}^{\\left(\\frac{\\mathrm{i}w\\sigma}{\\sqrt{2}}+\\frac{\\mu-x}{\\sigma\\sqrt{2}}\\right)^2-\\frac{(\\mu-x)^2}{2\\sigma^2}}\\, dw =\\frac{1}{2\\pi}\\mathrm{e}^{-\\frac{(\\mu-x)^2}{2\\sigma^2}}\\int_{-\\infty}^\\infty \\mathrm{e}^{\\left(\\frac{\\mathrm{i}w\\sigma}{\\sqrt{2}}+\\frac{\\mu-x}{\\sigma\\sqrt{2}}\\right)^2}\\, dw \\\\[1ex] & =\\displaystyle \\frac{1}{2\\pi}\\mathrm{e}^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\int_{-\\infty}^\\infty \\mathrm{e}^{-\\left(\\frac{w\\sigma}{\\sqrt{2}}+\\frac{\\mu-x}{\\mathrm{i}\\sigma\\sqrt{2}}\\right)^2}\\, dw \\stackrel{\\mbox{cambio de variable } u=\\frac{w\\sigma}{\\sqrt{2}}+\\frac{\\mu-x}{\\mathrm{i}\\sigma\\sqrt{2}}}{=} \\frac{1}{2\\pi}\\mathrm{e}^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\int_{-\\infty}^\\infty \\frac{\\sqrt{2}}{\\sigma}\\mathrm{e}^{-u^2}\\, du \\\\[1ex] & \\displaystyle\\stackrel{\\int_{-\\infty}^\\infty \\mathrm{e}^{-u^2}\\, du =\\sqrt{\\pi}}{=} \\frac{1}{\\sqrt{2}\\pi\\sigma} \\mathrm{e}^{-\\frac{(x-\\mu)^2}{2\\sigma^2}} \\sqrt{\\pi} = \\frac{1}{\\sqrt{2\\pi}\\sigma}\\mathrm{e}^{-\\frac{(x-\\mu)^2}{2\\sigma^2}},\n\\end{array}\n\\] función que coincide con la densidad de la distribución \\(N(\\mu,\\sigma)\\).\n\nRelación entre la función característica y los momentos\nLa relación entre la función característica y los momentos es la siguiente:\n Proposición.  Sean \\(X\\) una variable aleatoria con función característica \\(\\phi_X(w)\\). Entonces, el momento de orden \\(n\\) de \\(X\\) se puede obtener de la forma siguiente: \\[\nm_n =E\\left(X^n\\right)=\\frac{1}{\\mathrm{i}^n}\\frac{d}{d w^n}\\phi_X(w)|_{w=0} =\\frac{1}{\\mathrm{i}^n}\\phi_X^{(n)}(0).\n\\] O sea, el momento de orden \\(n\\) de \\(X\\) es la derivada \\(n\\)-ésima de la función característica evaluada en \\(w=0\\) dividido por \\(\\mathrm{i}^n\\).\n\nEjercicio\nLa demostración se realiza de forma similar a la demostración de la proposición que relaciona la función generatriz de momentos y los momentos.\nSe deja como ejercicio al lector.\n\n\nEjercicio\nRealizar los mismos ejemplos que los realizados para la función generatriz de momentos. O sea:\n\nSi \\(X\\) es una variable de Bernoulli de parámetro \\(p\\), demostrar usando la función característica que para todo \\(n\\), \\(m_n = E\\left(X^n\\right)=p\\).\nSi \\(X\\) es una variable exponencial de parámetro \\(\\lambda\\), demostrar usando la función característica que para todo \\(n\\), \\(m_n = E\\left(X^n\\right)=\\frac{n!}{\\lambda^n}\\).\nSi \\(X\\) es una variable normal de parámetros \\(\\mu\\) y \\(\\sigma\\), demostrar usando la función característica que \\(E(X)=\\mu\\), \\(E\\left(X^2\\right)=\\mu^2+\\sigma^2\\), \\(E\\left(X^3\\right)=3\\sigma^2\\mu\\) y \\(E\\left(X^4\\right)=6\\sigma^2\\mu^2+\\mu^4+3\\sigma^4\\)."
  },
  {
    "objectID": "4.html#fiabilidad",
    "href": "4.html#fiabilidad",
    "title": "5  Variables Aleatorias. Complementos",
    "section": "5.5 Fiabilidad",
    "text": "5.5 Fiabilidad\nSea \\(T\\geq 0\\) una variable aleatoria que nos da, por ejemplo, el tiempo de vida de cierto componente o dispositivo.\nVamos a definir medidas para estudiar la fiabilidad de este tipo de variables aleatorias.\nDefinición: Sea \\(T\\geq 0\\) una variable aleatoria. La fiabilidad de \\(T\\) en el tiempo \\(t\\) se define como la probabilidad que el sistema, componente o dispositivo funcione en el tiempo \\(t\\): \\(R(t)=P(T&gt;t)\\).\nObservación: Dada una variable \\(T\\geq 0\\), la relación existente entre la fiabilidad \\(R\\) y la función de distribución \\(F_T\\) es la siguiente: \\[\nR(t)=P(T&gt;t)=1-P(T\\leq t)=1-F_T (t)\n\\]\n\n5.5.1 Tiempo medio de vida\nObservación: Dada una variable \\(T\\geq 0\\) continua, el tiempo medio de vida de la variable \\(T\\) sería \\(E(T)\\). Entonces, este tiempo medio de vida se puede calcular como: \\(E(T)=\\int_0^\\infty R(t)\\, dt.\\)\nVeámoslo. Para ello basta ver que \\(E(T)=\\int_0^\\infty (1-F_T(t))\\, dt\\), donde \\(F_T(t)\\) es la función de distribución de la variable \\(T\\): \\[\n\\begin{array}{rl}\nE(T) & =\\displaystyle\\int_{t=0}^{t=\\infty} 1-F_T(t)\\, dt=\\int_{t=0}^{t=\\infty}\\int_{u=t}^{u=\\infty} f_T(u)\\,du\\,dt \\\\[1ex] & =\\displaystyle\\int_{u=0}^{u=\\infty} f_T(u)\\int_{t=0}^{t=u} \\, dt\\, du =\\int_{u=0}^{u=\\infty} f_T(u)\\cdot u\\, du = E(T),\n\\end{array}\n\\] donde \\(f_T(u)\\) seria la función de densidad de la variable \\(T\\) en el valor \\(u\\).\n\nEjemplo\nSea \\(T\\) una variable aleatoria exponencial de parámetro \\(\\lambda\\).\nLa fiabilidad de \\(T\\) sería: \\(R(t)=P(T&gt;t)=1-F_T(t)=\\mathrm{e}^{-\\lambda t}\\):"
  },
  {
    "objectID": "4.html#generación-de-muestras-de-variables-aleatorias-por-ordenador",
    "href": "4.html#generación-de-muestras-de-variables-aleatorias-por-ordenador",
    "title": "5  Variables Aleatorias. Complementos",
    "section": "5.6 Generación de muestras de variables aleatorias por ordenador",
    "text": "5.6 Generación de muestras de variables aleatorias por ordenador\nLa simulación por computadora de cualquier fenómeno aleatorio implica la generación de variables aleatorias con distribuciones prefijadas de antemano.\nPor ejemplo, la simulación de un sistema de colas implica generar el tiempo entre las llegadas de los clientes, así como los tiempos de servicio de cada cliente.\nFijémonos que fijar la variable aleatoria \\(X\\) es equivalente a fijar la función de distribución \\(F_X(x)\\) o la función de densidad \\(f_X(x)\\) en el caso continuo o la función de probabilidad \\(P_X(x)\\) en el caso discreto.\nTodos los métodos que vamos a describir presuponen que podemos generar números aleatorios que se distribuyen uniformemente entre 0 y 1. En R se puede hacer usando la función runif(n), donde n es la cantidad de números aleatorios entre 0 y 1 a generar.\n\n5.6.1 Método de transformación\nEl método de transformación se basa en el resultado siguiente:\nProposición.  Sea \\(X\\) una variable aleatoria con función de distribución \\(F_X(x)\\). Supongamos que \\(F_X(x)\\) es estrictamente creciente o que existe \\(F_X^{-1}(y)\\), para todo \\(y\\in [0,1]\\). Sea \\(Y\\) la variable aleatoria definida como: \\(Y=F_X(X)\\). Entonces la distribución de \\(Y\\) es uniforme en el intervalo \\([0,1]\\).\n\nDemostración:\nClaramente, por propia definición de \\(Y\\), tenemos que el dominio de \\(Y\\) es \\([0,1]\\) ya que el conjunto recorrido de la función de distribución de cualquier variable es el intervalo \\([0,1]\\).\nPara ver que la distribución de \\(Y\\) es \\(U[0,1]\\) basta comprobar que \\(F_Y(y)=y\\), para todo \\(y\\in [0,1]\\): \\[\n\\begin{array}{rl}\nF_Y(y) & =P(Y\\leq y)=P(F_X(X)\\leq y)\\stackrel{\\mbox{usando que $F_X$ es estrictamente creciente}}{=} P(X\\leq F_X^{-1}(y)) \\\\ & =F_X(F_X^{-1}(y))=y.\n\\end{array}\n\\]\n\nUsando la proposición anterior, dada una variable \\(X\\), como la distribución de la variable aleatoria \\(Y=F_X(X)\\) es \\(U[0,1]\\), si hacemos \\(X=F_X^{-1}(Y)\\), tendremos que si sabemos generar una muestra de \\(Y\\), aplicándole a la muestra la función \\(F_X^{-1}\\) tendremos generada una muestra de \\(X\\).\n\nEjemplo: generar una muestra de una variable exponencial de parámetro \\(\\lambda\\)\nRecordemos que si \\(X\\) es exponencial de parámetro \\(\\lambda\\), su función de distribución es: \\(F_X(x)=1-\\mathrm{e}^{-\\lambda x}\\).\nHallemos a continuación \\(F_X^{-1}\\): \\[\ny=1-\\mathrm{e}^{-\\lambda x},\\ \\Leftrightarrow 1-y=\\mathrm{e}^{-\\lambda x},\\ \\Leftrightarrow \\ln(1-y)=-\\lambda x,\\ \\Leftrightarrow x=-\\frac{1}{\\lambda}\\ln(1-y).\n\\] Por tanto, \\(F_X^{-1}(y)=-\\frac{1}{\\lambda}\\ln(1-y)\\).\nGeneremos una muestra con R de 25 valores de una variable exponencial de parámetro \\(\\lambda=2\\) usando el método anterior:\n\nn=25\nlambda=2\nmuestra.y = runif(n)\nmuestra.x = -(1/lambda)*log(1-muestra.y)\nmuestra.x\n\n [1] 0.15180860 0.37232459 1.10938234 0.01363114 0.02497869 0.12913003\n [7] 0.06869490 0.16503293 0.02769114 1.72580033 0.21095823 0.43858898\n[13] 1.19300954 0.74605377 0.39200164 0.62509776 0.04245918 0.16351714\n[19] 0.25378997 0.36720423 0.45184400 0.47511264 1.08034369 0.45831998\n[25] 0.78862207\n\n\nVamos a testear si nuestro método funciona.\nPara ello generaremos una muestra de 500 valores usando el método de transformación y dibujaremos su histograma de frecuencias relativas.\nSeguidamente dibujaremos la función de densidad de la variable exponencial de parámetro \\(\\lambda\\) y compararemos los resultados:\n\nn=500\nlambda=2\nmuestra.y = runif(n)\nmuestra.x = -(1/lambda)*log(1-muestra.y)\nhist(muestra.x,freq=FALSE,main=\"Histograma de la muestra\")\nx2=seq(from=0,to=2.5,by=0.01)\nlines(x2,dexp(x2,lambda),col=\"red\")\n\n\n\n\n\n\n\n\n\n\n\n5.6.2 Método de rechazo\nSea \\(X\\) una variable aleatoria continua tal que su función de densidad verifica:\n\nExisten valores \\(a\\) y \\(b\\) tal que \\(f_X(x)= 0\\) si \\(x\\not\\in [a,b]\\).\nExisten valores \\(c\\) y \\(d\\) tal que \\(f_X(x)\\in [c,d]\\), si \\(x\\in [a,b]\\).\n\nEn resumen, los puntos \\((x,f(x))\\) pertenecen al rectángulo \\([a,b]\\times [c,d]\\) y en caso contrario \\(f_X(x)=0\\).\nEn el gráfico siguiente, \\(a=0\\), \\(b=2\\), \\(c=0\\) y \\(d=1\\).\n\n\n\n\n\n\n\nPara generar una muestra aleatoria de la variable \\(X\\), hacemos lo siguiente:\n\ngeneramos un valor aleatorio \\(x\\) entre \\(a\\) y \\(b\\).\ngeneramos un valor aleatorio \\(y\\) entre \\(c\\) y \\(d\\).\nsi \\(y\\leq f_X(x)\\), aceptamos \\(x\\) como valor de la muestra. En caso contrario, volvemos a empezar en 1.\n\n\nEjemplo\nEl gráfico de la figura anterior corresponde a la función de densidad siguiente: \\[\nf_X(x)=\\begin{cases}\nx, & \\mbox{ si }0\\leq x\\leq 1,\\\\\n2-x, & \\mbox{ si }1\\leq x\\leq 2,\\\\\n0, & \\mbox{en caso contrario.}\n\\end{cases}\n\\]\nVamos a generar una muestra de \\(25\\) valores usando el método del rechazo:\n\na=0; b=2; c=0; d=1; n=25; i=1;\nf = function(x){ifelse(x&gt;=0 & x&lt;=1,x,ifelse(x&gt;=1&x&lt;=2,2-x,0))}\nmuestra=c()\nwhile(i &lt;=n){\n  x=runif(1,a,b)\n  y=runif(1,c,d)\n  if(y &lt;= f(x)){muestra=c(muestra,x); i=i+1}\n}\nmuestra\n\n [1] 0.8934245 1.0241151 0.7485943 1.1318743 1.1174486 0.8956145 1.6270601\n [8] 0.6737312 0.9584882 0.5762514 0.7390811 0.4276671 1.5210227 0.8506008\n[15] 0.4126257 0.9897587 1.2166050 1.6098125 0.8727734 0.8756381 1.4853939\n[22] 1.1539542 1.2353779 0.5157756 1.2873756\n\n\nComo hicimos con el ejemplo del método de transformación, vamos a generar una muestra de 500 valores de la variable \\(X\\), vamos a dibujar el histograma de frecuencias relativas junto con la función de densidad para ver si ésta se aproxima a dicho histograma:\n\na=0; b=2; c=0; d=1; n=500; i=1;\nf = function(x){ifelse(x&gt;=0 & x&lt;=1,x,ifelse(x&gt;=1&x&lt;=2,2-x,0))}\nmuestra=c()\nwhile(i &lt;=n){\n  x=runif(1,a,b)\n  y=runif(1,c,d)\n  if(y &lt;= f(x)){muestra=c(muestra,x); i=i+1}\n}\nhist(muestra,freq=FALSE,main=\"Histograma de la muestra\")\nx2=seq(from=0,to=2,by=0.01)\nlines(x2,f(x2),col=\"red\")"
  },
  {
    "objectID": "4.html#entropía",
    "href": "4.html#entropía",
    "title": "5  Variables Aleatorias. Complementos",
    "section": "5.7 Entropía",
    "text": "5.7 Entropía\nLa entropía es una medida de la incertidumbre en un experimento aleatorio.\nVeremos cómo la entropía cuantifica la incertidumbre por la cantidad de información requerida para especificar el resultado de un experimento aleatorio.\n\n5.7.1 Entropía de una variable aleatoria\nSupongamos que tenemos una variable aleatoria \\(X\\) discreta con valores enteros: \\(D_X=\\{1,2,\\ldots,N\\}\\).\nSea \\(k\\in D_X\\) un valor de la variable. Estamos interesados en cuantificar la incertidumbre del suceso \\(A_k =\\{X=k\\}\\).\nO sea, cuánta menos incertidumbre tenga \\(A_k\\), más alta será su probabilidad, y cuánta más incertidumbre, menos probabilidad de aparecer \\(A_k\\).\nUna medida que cumple las condiciones anteriores es la siguiente: \\(I(A_k)=I(\\{X=k\\})=\\ln\\left(\\frac{1}{P(X=k)}\\right)=-\\ln\\left(P(X=k)\\right).\\)\nPor ejemplo, si \\(P(A_k)=1\\), o sea, \\(A_k\\) aparece “seguro”, entonces tiene incertidumbre nula, \\(I(A_k)=0\\), y si \\(P(A_k)=0\\), o sea, \\(A_k\\) no aparece “nunca”, tiene incertidumbre máxima, \\(I(A_k)=\\infty\\).\nLa motivación anterior hace que definamos la entropía de una variable aleatoria de la forma siguiente:\nDefinición: Sea \\(X\\) una variable aleatoria con función de densidad \\(f_X(x)\\) en el caso continuo o función de probabilidad \\(P_X(x)\\) en el caso discreto. Definimos entropía de X como: \\(H_X = \\displaystyle E\\left(-\\ln(f_X)\\right)=\\int_{-\\infty}^\\infty -\\ln(f_X(x)) f_X(x)\\, dx,\\) en el caso continuo y, \\(H_X = \\displaystyle E\\left(-\\ln(P_X)\\right)=\\sum_{x_k\\in D_X} -\\ln(P_X(x_k)) P_X(x_k),\\) en el caso discreto.\n\nEjemplo: entropía de una variable de Bernoulli de parámetro \\(p\\)\nSea \\(X\\) una variable de Bernoulli de parámetro \\(p\\).\nRecordemos que su función de probabilidad \\(P_X\\) es: \\(P_X(0)=1-p=q,\\) \\(P_X(1)=p\\).\nLa entropía de \\(X\\) será: \\[\nH_X = E\\left(-\\ln(P_X)\\right) = -(1-p)\\cdot \\ln(1-p)-p\\cdot \\ln p.\n\\] El gráfico de la entropía se puede observar en el gráfico siguiente donde \\(X\\) tiene entropía máxima cuando \\(p=\\frac{1}{2}\\) que sería cuando \\(X\\) tiene incertidumbre máxima al tratar de adivinar el resultado de \\(X\\) y \\(X\\) tiene entropía mínima cuando \\(p=0\\) o \\(p=1\\) ya que en estos casos el resultado de \\(X\\) sería siempre \\(0\\) o \\(1\\), respectivamente.\n\n\n\n\n\n\n\n\n\nEjemplo: Entropía de una variable aleatoria exponencial de parámetro \\(\\lambda\\)\nSea \\(X\\) una variable aleatoria exponencial de parámetro \\(\\lambda\\).\nRecordemos que su función de densidad es: \\(f_X(x)=\\lambda \\mathrm{e}^{-\\lambda x}\\), si \\(x\\geq 0\\) y \\(f_X(x)=0\\), en caso contrario.\nSu entropía será: \\[\n\\begin{array}{rl}\nH_X & = \\displaystyle E\\left(-\\ln(f_X)\\right)=-\\int_0^\\infty \\ln\\left(\\lambda\\mathrm{e}^{-\\lambda x}\\right)\\lambda\\mathrm{e}^{-\\lambda x}\\, dx = -\\lambda \\int_0^\\infty (\\ln(\\lambda) -\\lambda x)\\mathrm{e}^{-\\lambda x}\\, dx \\\\[1ex] & =\\displaystyle -\\ln (\\lambda)\\int_0^\\infty \\lambda\\mathrm{e}^{-\\lambda x}\\, dx+\\lambda \\int_0^\\infty \\lambda x \\mathrm{e}^{-\\lambda x}\\, dx =-\\ln(\\lambda)\\int_0^\\infty f_X(x)\\, dx +\\lambda E(X)\\\\[1ex] & =\\displaystyle -\\ln(\\lambda)+\\lambda \\frac{1}{\\lambda} =1-\\ln(\\lambda).\n\\end{array}\n\\] El gráfico de la entropía se puede observar en el gráfico siguiente donde \\(X\\) tiene entropía máxima cuando \\(\\lambda=0\\) que sería cuando \\(X\\) tiene incertidumbre máxima al tratar de adivinar el resultado de \\(X\\) al tener media \\(E(X)=\\frac{1}{\\lambda}=\\infty\\) y \\(X\\) tiene entropía mínima cuando \\(\\lambda\\) tiende a \\(\\infty\\) ya que su media \\(E(X)=\\frac{1}{\\lambda}\\) tendería a 0."
  },
  {
    "objectID": "5.html#dos-variables-aleatorias",
    "href": "5.html#dos-variables-aleatorias",
    "title": "6  Vectores aleatorios bidimensionales",
    "section": "6.1 Dos variables aleatorias",
    "text": "6.1 Dos variables aleatorias\nMuchos experimentos aleatorios involucran varias variables aleatorias.\nPor ejemplo, dado un individuo de 30 años escogido al azar de una cierta población, medir su altura y su peso conjuntamente.\nOtro ejemplo más complejo es la medición continuada de un fenómeno aleatorio que se repite en el tiempo, como sería medir la temperatura media un día determinado del año, por ejemplo el día 1 de enero en un cierto lugar.\nLa variable aleatoria que nos da la medición en 10 años es una variable aleatoria de varias variables que involucra 10 variables aleatorias supuestas independientes e idénticamente distribuidas, lo que en estadística inferencial se le llama una muestra aleatoria simple.\n\n6.1.1 Definición\nRecordemos que una variable aleatoria \\(X\\) es una aplicación que toma valores numéricos para cada resultado de un experimento aleatorio: \\[\n\\begin{array}{rl}\nX: \\Omega & \\longrightarrow \\mathbb{R}\\\\\nw & \\longrightarrow X(w).\n\\end{array}\n\\] A partir de la definición anterior, generalizamos la noción de variable aleatoria unidimensional a variable aleatoria bidimensional:\nDefinición de variable aleatoria bidimensional: Dado un experimento aleatorio con espacio muestral \\(\\Omega\\), definimos variable aleatoria bidimensional \\((X,Y)\\) a toda aplicación \\[\n\\begin{array}{rl}\n(X,Y): \\Omega & \\longrightarrow \\mathbb{R}^2\\\\\nw & \\longrightarrow (X(w),Y(w)).\n\\end{array}\n\\]\n\nEjemplo: lanzamiento dos dados\nConsideremos el experimento aleatorio de lanzar un dado no trucado dos veces.\nSea \\(S\\) la suma de los resultados obtenidos y \\(P\\) el producto de los mismos.\nLa variable aleatoria \\((S,P)\\) que asigna a cada resultado \\(w=(x_1,x_2)\\) donde \\(x_1\\) es el resultado obtenido en el primer lanzamiento y \\(x_2\\), el resultado obtenido en el segundo, los valores: \\(S(w)=x_1+x_2\\) y \\(P(w)=x_1\\cdot x_2\\) es una variable aleatoria bidimensional.\n\nEl suceso \\(\\{2\\leq S\\leq 4,\\ 3\\leq P\\leq 6\\}\\) seria: \\[\n\\{2\\leq S\\leq 4,\\ 3\\leq P\\leq 6\\} = \\{(1,3),(3,1),(2,2)\\}.\n\\]\n\n\n\nEjemplo\nConsideremos el experimento aleatorio de elegir al azar un estudiante de primer curso de grado. Sea \\(w\\) el estudiante elegido. Consideremos la variable aleatoria \\((H,W)\\) que asigna a dicho estudiante \\(w\\), \\(H(w):\\) la altura de dicho estudiante en cm. y \\(W(w):\\) el peso de dicho estudiante en kg.\n\nEstamos interesado en sucesos del tipo \\(A=\\{H\\leq 176,\\ W\\leq 85\\}\\), es decir, el conjunto de estudiantes que miden menos de 1.76 m. y que pesan menos de 85 kg.\n\n\n\n\n6.1.2 Representación del dominio de una variable aleatoria bidimensional\nLos sucesos que se derivan de una variable aleatoria bidimensional estan especificados por regiones del plano. Veamos algunos ejemplos:\nSuceso: \\(\\{X+Y\\leq 1\\}\\). Es la zona sombreada del gráfico siguiente:\n\n\n\n\n\n\n\nSuceso: \\(\\{X^2+Y^2\\leq 4\\}\\). Es la zona sombreada del gráfico siguiente:\n\n\n\n\n\n\n\n\n\n\n\nSuceso: \\(\\{\\max\\{X,Y\\}\\geq 1\\}\\). Esta zona es la sombreada del gráfico siguiente:\n\n\n\n\n\n\nRepresentación suceso\n\n\n\n\n\nLa probabilidad de que la variable bidimensional pertenezca a una cierta región del plano \\(B\\) se define de la forma siguiente: \\[\nP((X,Y)\\in B)=P\\{w\\in \\Omega,\\ |\\ (X(w),Y(w))\\in B\\},\n\\] es decir, la probabilidad anterior es la probabilidad del suceso formado por los elementos de \\(w\\in\\Omega\\) que cumplen que su imagen por la variable aleatoria bidimensional \\((X,Y)\\) esté en \\(B\\).\nPor ejemplo, si consideramos \\(B=\\{X+Y\\leq 1\\}\\), \\(P((X,Y)\\in B)\\) es la probabilidad del suceso formado por los elementos \\(w\\) de \\(\\Omega\\) tal que la suma de las imágenes por \\(X\\) e \\(Y\\) sea menor o igual que 1: \\(X(w)+Y(w)\\leq 1\\)."
  },
  {
    "objectID": "5.html#función-de-distribución-conjunta",
    "href": "5.html#función-de-distribución-conjunta",
    "title": "6  Vectores aleatorios bidimensionales",
    "section": "6.2 Función de distribución conjunta",
    "text": "6.2 Función de distribución conjunta\n\n6.2.1 Definición\nDada una variable aleatoria bidimensional \\((X,Y)\\), queremos estudiar cómo se distribuye la probabilidad de sucesos cualesquiera de la forma \\(\\{(X,Y)\\in B\\}\\), donde \\(B\\) es una región del plano.\nPara ello, definimos la función de distribución conjunta:\nDefinición de función de distribución conjunta: Dada una variable bidimensional \\((X,Y)\\), definimos su función de distribución conjunta \\(F_{XY}\\) a la función definida sobre \\(\\mathbb{R}^2\\) de la manera siguiente: \\[\n\\begin{array}{rl}\nF_{XY}: \\mathbb{R}^2 & \\longrightarrow \\mathbb{R}\\\\\n(x,y) & \\longrightarrow F_{XY}(x,y)=P(X\\leq x,\\ Y\\leq y).\n\\end{array}\n\\]\nPor lo tanto, dado un valor \\((x,y)\\in \\mathbb{R}^2\\), consideramos la región del plano \\((-\\infty,x]\\times (-\\infty,y]\\):\n\n\n\n\n\n\n\nEntonces la función de distribución conjunta en el valor \\((x,y)\\) es la probabilidad del suceso formado por aquellos elementos tal que la imagen por la variable aleatoria bidimensional \\((X,Y)\\) caen dentro de la región sombreada en el gráfico anterior:\n\\[\n\\begin{array}{rl}\nF_{XY}(x,y) &= P\\{w\\in\\Omega,\\ |\\ (X(w),Y(w))\\in (-\\infty,x]\\times (-\\infty,y]\\} \\\\\n&= P\\{w\\in\\Omega,\\ |\\ X(w)\\leq x,\\ Y(w)\\leq y\\}.\n\\end{array}\n\\]\n\n\n6.2.2 Propiedades\nSea \\((X,Y)\\) una variable bidimensional. y sea \\(F_{XY}\\) su función de distribución conjunta. Dicha función satisface las propiedades siguientes:\n\nLa función de distribución conjunta es no decreciente en cada una de las variables: \\[\n\\mbox{Si }x_1\\leq x_2, \\mbox{ y }y_1\\leq y_2,\\mbox{ entonces, }F_{XY}(x_1,y_1)\\leq F_{XY}(x_2,y_2).\n\\]\n\\(F_{XY}(x,-\\infty)=F_{XY}(-\\infty,y)=0,\\) \\(F_{XY}(\\infty,\\infty)=1\\), para todo \\(x,y\\in\\mathbb{R}\\).\nLas variables aleatorias \\(X\\) e \\(Y\\) se llaman variables aleatorias marginales y sus funciones de distribución \\(F_X\\) y \\(F_Y\\) pueden hallarse de la forma siguiente como función de la función de distribución conjunta \\(F_{XY}\\): \\[\nF_X(x)=F_{XY}(x,\\infty),\\ F_Y(y)=F_{XY}(\\infty,y),\n\\] para todo \\(x,y\\in\\mathbb{R}\\).\nLa función de distribución conjunta es continua por el “norte” y por el “este”: \\[\n\\begin{array}{rl}\n\\lim_{x\\to a^+}F_{XY}(x,y) & =\\lim_{x\\to a, x&gt; a}F_{XY}(x,y)=F_{XY}(a,y), \\\\\n\\lim_{y\\to b^+}F_{XY}(x,y) & =\\lim_{y\\to b, y&gt; b}F_{XY}(x,y)=F_{XY}(x,b),\n\\end{array}\n\\] para todo \\(a,b\\in\\mathbb{R}\\). Ver la siguiente figura.\n\n\n\n\n\n\n\n\n\n\nFunción de distribución conjunta"
  },
  {
    "objectID": "5.html#variables-aleatorias-bidimensionales-discretas",
    "href": "5.html#variables-aleatorias-bidimensionales-discretas",
    "title": "6  Vectores aleatorios bidimensionales",
    "section": "6.3 Variables aleatorias bidimensionales discretas",
    "text": "6.3 Variables aleatorias bidimensionales discretas\nDefinición de variable aleatoria bidimensional discreta: Sea \\((X,Y)\\) una variable aleatoria bidimensional. Diremos que es discreta cuando su conjunto de valores en \\(\\mathbb{R}^2\\), \\((X,Y)(\\Omega)\\) es un conjunto finito o numerable.\nEn la mayoría de los casos, dicho conjunto es un subconjunto de los enteros naturales.\n\nEjemplo: lanzamiento de dos dados (continuación)\nLa variable aleatoria bidimensional anterior que nos daba la suma y el producto de los resultados obtenidos por los dos lanzamientos, respectivamente es discreta ya que: \\[\n\\begin{array}{rl}\n(S,P)(\\Omega) =&\\{(2,1),(3,2),(4,3),(4,4),(5,4),(5,6),(6,5),(6,8),(6,9),(7,6),\\\\ &\n(7,10),(7,12),(8,12), (8,15),(8,16),(9,18),(9,20),(10,24),\\\\ & (10,25),(11,30), (12,36)\\}.\n\\end{array}\n\\]\nComprobar que el conjunto \\((S,P)(\\Omega)\\) dado por el ejemplo coincide con la expresión dada. O lo que es lo mismo, hallar el conjunto \\((S,P)(\\Omega)\\):\n\n\\[\n\\begin{array}{rl}\n(S,P): \\Omega & \\longrightarrow \\mathbb{R}^2\\\\\n(1,1) & \\longrightarrow (S(1,1),P(1,1))=(2,1),\\\\\n(1,2) & \\longrightarrow (S(1,2),P(1,2))=(3,2),\\\\\n\\vdots & \\vdots \\\\\n(6,6) & \\longrightarrow (S(6,6),P(6,6))=(12,36).\n\\end{array}\n\\]\n\n\n\n6.3.1 Función de probabilidad conjunta\nDefinición de función de probabilidad conjunta: Dada una variable aleatoria bidimensional discreta \\((X,Y)\\) con \\((X,Y)(\\Omega)=\\{(x_i,y_j),\\ i=1,2,\\ldots,\\ j=1,2,\\ldots,\\}\\), definimos la función de probabilidad discreta \\(P_{XY}\\) para un valor \\((x,y)\\in\\mathbb{R}^2\\) de la siguiente forma:\n\\[\n\\begin{array}{rl}\nP_{XY}: \\mathbb{R}^2 & \\longrightarrow \\mathbb{R}\\\\\n(x,y) & \\longrightarrow P_{XY}(x,y)=P(X= x,\\ Y= y).\n\\end{array}\n\\]\nObservación: Si \\((x,y)\\not\\in (X,Y)(\\Omega)\\), el valor de la función de probabilidad conjunta en \\((x,y)\\) es nula: \\(P_{XY}(x,y)=0\\). El motivo es que, en este caso, el conjunto \\(\\{w\\in\\Omega,\\ | (X(w),Y(w))=(x,y)\\}=\\emptyset\\) es vacío pues \\((x,y)\\not\\in (X,Y)(\\Omega)\\).\n\nEjemplo: lanzamiento de dos dados (continuación) Por tanto, de cara a calcular \\(P_{XY}\\) basta conocer los valores de \\(P_{XY}(x_i,y_j)\\) para \\((x_i,y_j)\\in (X,Y)(\\Omega)\\):\n\n\n\n\n\n\n\n\n\n\n\n\n\\(X/Y\\)\n\\(y_1\\)\n\\(y_2\\)\n\\(\\ldots\\)\n\\(y_N\\)\n\n\n\n\n\\(x_1\\)\n\\(P_{XY}(x_1,y_1)\\)\n\\(P_{XY}(x_1,y_2)\\)\n\\(\\ldots\\)\n\\(P_{XY}(x_1,y_N)\\)\n\n\n\\(x_2\\)\n\\(P_{XY}(x_2,y_1)\\)\n\\(P_{XY}(x_2,y_2)\\)\n\\(\\ldots\\)\n\\(P_{XY}(x_2,y_N)\\)\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n\\(x_M\\)\n\\(P_{XY}(x_M,y_1)\\)\n\\(P_{XY}(x_M,y_2)\\)\n\\(\\ldots\\)\n\\(P_{XY}(x_M,y_N)\\)\n\n\n\n\nLa función de probabilidad conjunta es:\n\n\n\n\n\n\\(P/S\\)\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n\n1\n\\(\\frac{1}{36}\\)\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n0\n\\(\\frac{2}{36}\\)\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n3\n0\n0\n\\(\\frac{2}{36}\\)\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4\n0\n0\n\\(\\frac{1}{36}\\)\n\\(\\frac{2}{36}\\)\n0\n0\n0\n0\n0\n0\n0\n\n\n5\n0\n0\n0\n0\n\\(\\frac{2}{36}\\)\n0\n0\n0\n0\n0\n0\n\n\n6\n0\n0\n0\n\\(\\frac{2}{36}\\)\n0\n\\(\\frac{2}{36}\\)\n0\n0\n0\n0\n0\n\n\n8\n0\n0\n0\n0\n\\(\\frac{2}{36}\\)\n0\n0\n0\n0\n0\n0\n\n\n9\n0\n0\n0\n0\n\\(\\frac{1}{36}\\)\n0\n0\n0\n0\n0\n0\n\n\n10\n0\n0\n0\n0\n0\n\\(\\frac{2}{36}\\)\n0\n0\n0\n0\n0\n\n\n12\n0\n0\n0\n0\n0\n\\(\\frac{2}{36}\\)\n\\(\\frac{2}{36}\\)\n0\n0\n0\n0\n\n\n15\n0\n0\n0\n0\n0\n0\n\\(\\frac{2}{36}\\)\n0\n0\n0\n0\n\n\n16\n0\n0\n0\n0\n0\n0\n\\(\\frac{1}{36}\\)\n0\n0\n0\n0\n\n\n18\n0\n0\n0\n0\n0\n0\n0\n\\(\\frac{2}{36}\\)\n0\n0\n0\n\n\n20\n0\n0\n0\n0\n0\n0\n0\n\\(\\frac{2}{36}\\)\n0\n0\n0\n\n\n24\n0\n0\n0\n0\n0\n0\n0\n0\n\\(\\frac{2}{36}\\)\n0\n0\n\n\n25\n0\n0\n0\n0\n0\n0\n0\n0\n\\(\\frac{1}{36}\\)\n0\n0\n\n\n30\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\\(\\frac{2}{36}\\)\n0\n\n\n36\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\\(\\frac{1}{36}\\)\n\n\n\n\nVamos a definir unas funciones en R para calcular la función de probabilidad conjunta.\nLa función pdado devuelve la probabilidad de que salga la cara x en un dado de n caras donde por defecto \\(n=6\\):\n\npdado =function(x,n=6)  sapply(x,FUN=function(x) \n  if( x %in% c(1:n))  {return(1/n)} else {return(0)})\n\nVamos a probarla. La probabilidad de que salga la cara 4 en un dado de 6 caras vale:\n\npdado(4,6)\n\n[1] 0.1666667\n\n\nLa función pdado2 devuelve la probabilidad de que salgan las caras x e y cuando lanzamos un dado de n caras dos veces:\n\npdado2 =function(x,y,n=6) {pdado(x,n)*pdado(y,n)}\n\nPor ejemplo la probabilidad de que salgan las caras 3 y 4 en un dado de 6 caras es:\n\npdado2(3,4,6)\n\n[1] 0.02777778\n\n\nLa función psum_prod nos da la función de probabilidad conjunta de la suma y el producto cuando lanzamos dos dados de n caras:\n\npsum_prod=function(x,y,n=6){\n  Dxy=data.frame(d1=rep(1:n,each=n),d2=rep(1:n,times=n))\n  Dxy$suma=Dxy$d1+Dxy$d2\n  Dxy$producto=Dxy$d1*Dxy$d2\n  aux=Dxy[Dxy$suma==x& Dxy$producto==y,]\n  sum(apply(aux[,1:2],FUN=function(x) {pdado2(x[1],x[2],n=n)},1 ))\n}\n\nPor ejemplo, sabemos que \\(P_{SP}(6,8)=\\frac{2}{36}=0.0556\\):\n\npsum_prod(6,8)\n\n[1] 0.05555556\n\n\nla tabla de la función de probabilidad conjunta para la variable \\((S,P)\\) hacemos lo siguiente:\n\nn=6\nDxy=data.frame(d1=rep(1:n,each=n),d2=rep(1:n,times=n))\nDxy$suma=Dxy$d1+Dxy$d2\nDxy$producto=Dxy$d1*Dxy$d2\ntabla.func.prob.conjunta=prop.table(table(Dxy$suma,Dxy$producto))\nknitr::kable(round(tabla.func.prob.conjunta[,1:9],3))\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n8\n9\n10\n\n\n\n\n2\n0.028\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n\n\n3\n0.000\n0.056\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n\n\n4\n0.000\n0.000\n0.056\n0.028\n0.000\n0.000\n0.000\n0.000\n0.000\n\n\n5\n0.000\n0.000\n0.000\n0.056\n0.000\n0.056\n0.000\n0.000\n0.000\n\n\n6\n0.000\n0.000\n0.000\n0.000\n0.056\n0.000\n0.056\n0.028\n0.000\n\n\n7\n0.000\n0.000\n0.000\n0.000\n0.000\n0.056\n0.000\n0.000\n0.056\n\n\n8\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n\n\n9\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n\n\n10\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n\n\n11\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n\n\n12\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n\n\n\n\nknitr::kable(round(tabla.func.prob.conjunta[,10:18],3))\n\n\n\n\n\n12\n15\n16\n18\n20\n24\n25\n30\n36\n\n\n\n\n2\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n\n\n3\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n\n\n4\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n\n\n5\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n\n\n6\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n\n\n7\n0.056\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n\n\n8\n0.056\n0.056\n0.028\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n\n\n9\n0.000\n0.000\n0.000\n0.056\n0.056\n0.000\n0.000\n0.000\n0.000\n\n\n10\n0.000\n0.000\n0.000\n0.000\n0.000\n0.056\n0.028\n0.000\n0.000\n\n\n11\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.056\n0.000\n\n\n12\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.028\n\n\n\n\n\nlo cual finaliza nuestros cálculos en R.\n\n\n\n6.3.1.1 Propiedades de la función de probabilidad conjunta\nSea \\((X,Y)\\) una variable aleatoria bidimensional discreta con conjunto de valores \\((X,Y)(\\Omega)=\\{(x_i,y_j)\\, i=1,2,\\ldots,\\ j=1,2,\\ldots\\}\\). Entonces su función de probabilidad conjunta verifica las propiedades siguientes:\nLa suma de todos los valores de la función de probabilidad conjunta sobre el conjunto de valores siempre vale 1: \\[\\sum_{i}\\sum_j P_{XY}(x_i,y_j)=1.\\]\nSea \\(B\\) una región del plano. El valor de la probabilidad \\(P((X,Y)\\in B)\\) se puede calcular de la forma siguiente:\n\\[\nP((X,Y)\\in B) =\\sum_{(x_i,y_j)\\in B} P_{XY}(x_i,y_j).\n\\]\nEs decir, la probabilidad de que la variable bidimensional tome valores en \\(B\\) es igual a la suma de todos aquellos valores de la función de probabilidad conjunta que están en \\(B\\).\nEn particular, tenemos la sigueinte propiedad que relaciona la función de distribución conjunta con la función de probabilidad conjunta:\n\\[\nF_{XY}(x,y)=\\sum_{x_i\\leq x, y_j\\leq y} P_{XY}(x_i,y_j).\n\\] Dicha expresión se deduce de la expresión anterior considerando \\(B=(-\\infty,x]\\times (-\\infty,y]\\).\n\nEjercicio\nEn el ejemplo del lanzamiento de los dos dados. Comprobad usando la tabla de la función de probabilidad conjunta que la suma de todos sus valores suma 1.\n\n\nEjemplo: lanzamientos de dados (continuación)\nApliquemos la fórmula que relaciona la función de distribución conjunta con la función de probabilidad conjunta para \\((x,y)=(5,4)\\).\n\nRecordemos la tabla de la función de probabilidad conjunta hasta \\(S=5\\) y \\(P=4\\):\n\n\n\n\n\\(S/P\\)\n1\n2\n3\n4\n\n\n\n\n\n2\n\\(\\frac{1}{36}\\)\n0\n0\n0\n\\(\\ldots\\)\n\n\n3\n0\n\\(\\frac{2}{36}\\)\n0\n0\n\\(\\ldots\\)\n\n\n4\n0\n0\n\\(\\frac{2}{36}\\)\n\\(\\frac{1}{36}\\)\n\\(\\ldots\\)\n\n\n5\n0\n0\n0\n\\(\\frac{2}{36}\\)\n\\(\\ldots\\)\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n\n\nObservamos que los únicos valores \\((x_i,y_j)\\in (X,Y)(\\Omega)\\) que verifican \\(x_i\\leq 5\\) y \\(y_j\\leq 4\\) son \\((2,1)\\), \\((3,2)\\), \\((4,3)\\), \\((4,4)\\) y \\((5,4)\\). Por tanto,\n\\[\n\\begin{array}{rl}\nF_{SP}(5,4) &= P_{SP}(2,1)+P_{SP}(3,2)+P_{SP}(4,3)+P_{SP}(4,4)+P_{SP}(5,4) \\\\ & = \\frac{1}{36}+\\frac{2}{36}+\\frac{2}{36}+\\frac{1}{36}+\\frac{2}{36} = \\frac{8}{36}=\\frac{2}{9}.\n\\end{array}\n\\] Es decir, “a largo plazo”, de cada 9 ocasiones que lanzamos un dado dos veces; en 2 ocasiones obtenemos un resultado cuya suma es menor o igual que 5 y cuyo producto es menor o igual que 4.\nPara definir la función de distribución conjunta definimos la función siguiente en R:\n\nfunc.dist.conj = function(x,y,n=6){\n  sum(tabla.func.prob.conjunta[as.integer(rownames(tabla.func.prob.conjunta))&lt;=x,\n                            as.integer(colnames(tabla.func.prob.conjunta)) &lt;=y])\n}\n\nComprobemos que \\(F_{SP}(5,4)=\\frac{2}{9}=0.2222\\):\n\nfunc.dist.conj(5,4)\n\n[1] 0.2222222\n\n\n\n\n\n\n\n6.3.2 Distribuciones marginales\nConsideremos una variable aleatoria bidimensional discreta \\((X,Y)\\) con función de probabilidad conjunta \\(P_{XY}(x_i,y_j)\\), con \\((x_i,y_j)\\in (X,Y)(\\Omega)\\), \\(i=1,2,\\ldots\\), \\(j=1,2,\\ldots\\).\nLa tabla de la función de probabilidad conjunta contiene suficiente información para obtener las funciones de probabilidad de las variables \\(X\\) e \\(Y\\).\nDichas variables \\(X\\) e \\(Y\\) se denominan distribuciones marginales y sus correspondientes funciones de probabilidad, funciones de probabilidad marginales \\(P_X\\) de la variable \\(X\\) y \\(P_Y\\) de la variable \\(Y\\).\nVeamos cómo obtener \\(P_X\\) y \\(P_Y\\) a partir de la tabla \\(P_{XY}\\).\nProposición. Expresión de las funciones de probabilidad marginales.  Sea \\((X,Y)\\) una variable aleatoria bidimensional discreta con función de probabilidad conjunta \\(P_{XY}(x_i,y_j)\\), con \\((x_i,y_j)\\in (X,Y)(\\Omega)\\), \\(i=1,2,\\ldots\\), \\(j=1,2,\\ldots\\).\nLas funciones de probabilidad marginales \\(P_X(x_i)\\) y \\(P_Y(y_j)\\) se calculan usando las expresiones siguientes:\n\\[\n\\begin{array}{rl}\nP_X(x_i)  & = \\sum_{j=1} P_{XY}(x_i,y_j),\\  i=1,2,\\ldots,\\\\ P_Y(y_j) &  = \\sum_{i=1} P_{XY}(x_i,y_j),\\ \\ j=1,2,\\ldots\n\\end{array}\n\\]\nSi consideramos la funciòn \\(P_{XY}\\) como una tabla bidimensional en la que en la primera fila están los valores de la variable \\(Y\\) (\\(y_1,y_2,\\ldots\\)) y en la primera columna están los valores de la variable \\(X\\) (\\(x_1,x_2,\\ldots\\)). Para obtener la función de probabilidad marginal de la variable \\(X\\) en el valor \\(x_i\\), \\(P_X(x_i)\\), hay que sumar todos los valores de \\(P_{XY}(x_i,y_j)\\) correspondientes a la fila \\(i\\)-ésima y para obtener la función de probabilidad marginal de la variable \\(Y\\) en el valor \\(y_j\\), \\(P_Y(y_j)\\), hay que sumar todos los valores de \\(P_{XY}(x_i,y_j)\\) correspondientes a la columna \\(j\\)-ésima.\n\n6.3.2.1 Ejemplo\n\nEjemplo de la suma y el producto de los resultados de dos lanzamientos de un dado\nHallemos la función de probabilidad marginal para la suma de los resultados \\(S\\):\n\nUsando la expresión de la probabilidad marginal tenemos que\n\\[\n\\begin{array}{rl}\nP_S(2) & = P_{SP}(2,1)=\\frac{1}{36},\\\\\nP_S(3) & = P_{SP}(3,2)=\\frac{2}{36},\\\\\nP_S(4) & = P_{SP}(4,3)+P_{SP}(4,4)=\\frac{2}{36}+\\frac{1}{36}=\\frac{3}{36}=\\frac{1}{12},\\\\\nP_S(5) & = P_{SP}(5,4)+P_{SP}(5,6)=\\frac{2}{36}+\\frac{2}{36}=\\frac{4}{36}=\\frac{1}{9},\\\\\nP_S(6) & = P_{SP}(6,5)+P_{SP}(6,8)+P_{SP}(6,9)=\\frac{2}{36}+\\frac{2}{36}+\\frac{1}{36}=\\frac{5}{36},\\\\\nP_S(7) & = P_{SP}(7,6)+P_{SP}(7,10)+P_{SP}(7,12)=\\frac{2}{36}+\\frac{2}{36}+\\frac{2}{36}=\\frac{6}{36}=\\frac{1}{6},\\\\\nP_S(8) & = P_{SP}(8,12)+P_{SP}(8,15)+P_{SP}(8,16)=\\frac{2}{36}+\\frac{2}{36}+\\frac{1}{36}=\\frac{5}{36},\\\\\nP_S(9) & = P_{SP}(9,18)+P_{SP}(9,20)=\\frac{2}{36}+\\frac{2}{36}=\\frac{4}{36}=\\frac{1}{9},\\\\\nP_S(10) & = P_{SP}(10,24)+P_{SP}(10,25)=\\frac{2}{36}+\\frac{1}{36}=\\frac{3}{36}=\\frac{1}{12},\\\\\nP_S(11) & = P_{SP}(11,30)=\\frac{2}{36},\\\\\nP_S(12) & = P_{SP}(12,36)=\\frac{1}{36}.\n\\end{array}\n\\]\nLa función de probabilidad marginal de la suma \\(S\\) queda resumida en la tabla siguiente:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(S\\)\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n\n\\(P_S\\)\n\\(\\frac{1}{36}\\)\n\\(\\frac{2}{36}\\)\n\\(\\frac{3}{36}\\)\n\\(\\frac{4}{36}\\)\n\\(\\frac{5}{36}\\)\n\\(\\frac{6}{36}\\)\n\\(\\frac{5}{36}\\)\n\\(\\frac{4}{36}\\)\n\\(\\frac{3}{36}\\)\n\\(\\frac{2}{36}\\)\n\\(\\frac{1}{36}\\)\n\n\n\n\nPara hallar la función de probabilidad marginal de la suma basta sumar las filas de la tabla que nos daba la función de probabilidad conjunta:\n\nmarginal.suma = apply(tabla.func.prob.conjunta,1,sum)\nmarginal.suma\n\n         2          3          4          5          6          7          8 \n0.02777778 0.05555556 0.08333333 0.11111111 0.13888889 0.16666667 0.13888889 \n         9         10         11         12 \n0.11111111 0.08333333 0.05555556 0.02777778 \n\n\nDe la misma manera, para hallar la función de probabilidad marginal del producto basta sumar las columnas de la tabla anterior:\n\nmarginal.producto = apply(tabla.func.prob.conjunta,2,sum)\nmarginal.producto\n\n         1          2          3          4          5          6          8 \n0.02777778 0.05555556 0.05555556 0.08333333 0.05555556 0.11111111 0.05555556 \n         9         10         12         15         16         18         20 \n0.02777778 0.05555556 0.11111111 0.05555556 0.02777778 0.05555556 0.05555556 \n        24         25         30         36 \n0.05555556 0.02777778 0.05555556 0.02777778"
  },
  {
    "objectID": "5.html#variables-aleatorias-bidimensionales-continuas",
    "href": "5.html#variables-aleatorias-bidimensionales-continuas",
    "title": "6  Vectores aleatorios bidimensionales",
    "section": "6.4 Variables aleatorias bidimensionales continuas",
    "text": "6.4 Variables aleatorias bidimensionales continuas\n\n6.4.1 Definición\nRecordemos la definición de variable continua unidimensional: \\(X\\) es continua si existe una función \\(f_X:\\mathbb{R}\\longrightarrow \\mathbb{R}\\), llamada función de densidad no negativa \\(f_X(x)\\geq 0\\), para todo \\(x\\in\\mathbb{R}\\) tal que para cualquier intervalo \\((a,b)\\), la probabilidad de que \\(X\\) esté en \\((a,b)\\) se calcula de la forma siguiente:\n\\[\nP(X\\in B)=P(a&lt; X &lt; b)=\\int_B f_{X}(x)\\,du=\\int_a^b f_{X}(x)\\,dx.\n\\]\nLa generalización natural es, entonces:\nDefinición de variable aleatoria bidimensional continua.  Sea \\((X,Y)\\) una variable aleatoria bidimensional. Diremos que \\((X,Y)\\) es continua si existe una función \\(f_{XY}:\\mathbb{R}^2\\longrightarrow \\mathbb{R}\\) llamada función de densidad no negativa \\(f_{XY}(x,y)\\geq 0\\) para todo \\((x,y)\\in\\mathbb{R}^2\\) tal que dado cualquier región \\(B\\) del plano, la probabilidad de que \\((X,Y)\\) esté en \\(B\\) se calcula de la forma siguiente: \\[\nP((X,Y)\\in B)=\\int\\int_B f_{XY}(x,y)\\,dx\\,dy.\n\\]\n\n6.4.1.1 Ejemplos\n\nEjemplo: cálculo probabilidad distribución bidimensional\nConsideremos la siguiente función de densidad: \\[\nf_{XY}(x,y)=\\begin{cases}\n1, & \\mbox{ si }0\\leq x\\leq 1,\\ 0\\leq y\\leq 1, \\\\\n0, & \\mbox{en caso contrario.}\n\\end{cases}\n\\] En este caso, si consideramos \\(B=\\left[-1,\\frac{1}{2}\\right]\\times \\left[-1,\\frac{1}{2}\\right]\\), la probabilidad de que \\((X,Y)\\) esté en \\(B\\) se calcularía de la forma siguiente:\n\n\\[\n\\begin{array}{rl}\nP((X,Y)\\in B)&=\\int\\int_{B} f_{XY}(x,y)\\, dx\\, dy=\\int_{-1}^{\\frac{1}{2}}\\int_{-1}^{\\frac{1}{2}} f_{XY}(x,y)\\, dx\\, dy \\\\\n&=\\int_0^{\\frac{1}{2}}\\int_0^{\\frac{1}{2}} 1\\, dx\\,dy=\\int_0^{\\frac{1}{2}} 1\\, dx\\int_0^{\\frac{1}{2}} 1\\, dy=\\frac{1}{2}\\cdot\\frac{1}{2}=\\frac{1}{4}.\n\\end{array}\n\\]\nEn la figura siguiente hemos dibujado en morado la región donde \\(f_{XY}\\) no es cero, es decir \\([0,1]\\times [0,1]\\), la región \\(B\\) en verde y la región intersección de las dos anteriores que es donde tenemos que integrar la función de densidad dada.\n\n\n\n\n\n\n\n\n\n\n\n\n6.4.2 Propiedades de la función de densidad\nSea \\((X,Y)\\) una variable aleatoria bidimensional continua con función de densidad \\(f_{XY}\\). Esta función verifica las propiedades siguientes:\n\nLa integral de dicha función sobre todo el plano vale 1:\n\n\\[\n\\int\\int_{\\mathbb{R}^2} f_{XY}(x,y)\\,dx\\,dy =1.\n\\]\nPara ver dicha propiedad, basta considerar \\(B=\\mathbb{R}^2\\), tener en cuenta que el suceso \\((X,Y)\\in \\mathbb{R}^2\\) es el total \\(\\Omega\\) y aplicar la definición de \\(f_{XY}\\):\n\\[\nP((X,Y)\\in \\mathbb{R}^2)=1= \\int\\int_{\\mathbb{R}^2} f_{XY}(x,y)\\,dx\\,dy.\n\\]\n\nLa relación que hay entre la función de distribución \\(F_{XY}\\) y la función de densidad \\(f_{XY}\\) es la siguiente:\n\n\\[\nF_{XY}(x,y)=\\int_{-\\infty}^x\\int_{-\\infty}^y f_{XY}(u,v)\\,du\\,dv.\n\\]\nPara ver dicha propiedad, basta considerar \\(B=(-\\infty,x]\\times (-\\infty,y]\\) y aplicar la definición de función de distribución:\n\\[\nF_{XY}(x,y)=P((X,Y)\\in (-\\infty,x]\\times (-\\infty,y])=\\int_{-\\infty}^x\\int_{-\\infty}^y f_{XY}(u,v)\\,du\\,dv.\n\\]\n\nLa relación que hay entre la función de densidad \\(f_{XY}\\) y la función de distribución \\(F_{XY}\\) es la siguiente:\n\n\\[\nf_{XY}(x,y)=\\frac{\\partial^2 F_{XY}(x,y)}{\\partial x\\partial y}.\n\\]\nDicha propiedad se deduce de la anterior, derivando primero respecto a \\(x\\) y después respecto a \\(y\\) para eliminar las dos integrales.\n\nLas funciones de densidad marginales de las variables \\(X\\) e \\(Y\\), \\(f_X(x)\\) y \\(f_Y(y)\\) respectivamente, se calculan de la forma siguiente:\n\n\\[\nf_X(x)=\\int_{-\\infty}^\\infty f_{XY}(x,y)\\, dy,\\ f_Y(y)=\\int_{-\\infty}^\\infty f_{XY}(x,y)\\, dx\n\\]\n\n6.4.2.1 Ejemplos\n\nEjemplo: continuación ejemplo anterior\nComprobemos las propiedades usando la función de densidad del ejemplo anterior: \\(f_{XY}(x,y)=\\begin{cases} 1, & \\mbox{ si }0\\leq x\\leq 1,\\ 0\\leq y\\leq 1, \\\\ 0, & \\mbox{en caso contrario.} \\end{cases}\\)\n\n\nLa integral de \\(f_{XY}\\) sobre todo el plano vale 1:\n\n\\[\n\\begin{array}{rcl}\n\\int\\int_{\\mathbb{R}^2} f_{XY}(x,y)\\,dx\\, dy &=&\\int_0^1\\int_0^1 1\\, dx\\, dv\\\\\n&=&\\int_0^1 1\\, dx\\int_0^1 1\\, dy=1\\cdot 1=1.\n\\end{array}\n\\]\n\nVamos a calcular la función de distribución \\(F_{XY}\\). Para ello dividimos el plano en 5 zonas tal como muestra la figura siguiente:\n\n\n\n\n\n\n\n\nSea \\((x,y)\\) un punto cualquiera de \\(\\mathbb{R}^2\\). De cara a calcular \\(F_{XY}(x,y)\\) tenemos que averiguar el conjunto intersección siguiente: \\(([0,1]\\times [0,1])\\cap ((-\\infty,x]\\times (-\\infty,y])\\) ya que el dominio donde \\(f_{XY}\\) es no nula es \\([0,1]\\times [0,1]\\) y la función de distribución \\(F_{XY}(x,y)\\) valdrá:\n\\[\n\\begin{array}{rl}\nF_{XY}(x,y)&=\\int_{-\\infty}^x\\int_{-\\infty}^y f_{XY}(u,v)\\,du\\,dv\\\\ &=\n\\int\\int_{([0,1]\\times [0,1])\\cap ((-\\infty,x]\\times (-\\infty,y])} f_{XY}(u,v)\\,du\\,dv.\n\\end{array}\n\\]\n\nCaso \\((x,y)\\in \\mbox{Zona A}\\) o \\(x&lt;0\\) o \\(y&lt;0\\) En este caso: \\(([0,1]\\times [0,1])\\cap ((-\\infty,x]\\times (-\\infty,y])=\\emptyset.\\) Ver figura siguiente donde la zona morada \\(([0,1]\\times [0,1]\\)) no se interseca con la zona verde (\\((-\\infty,x]\\times (-\\infty,y]\\)).\n\nPor tanto en este caso, \\(F_{XY}(x,y)=0\\).\n\n\n\n\n\n\n\n\nCaso \\((x,y)\\in \\mbox{Zona B}\\), o \\((x,y)\\in [0,1]\\times [0,1]\\). En este caso: \\(([0,1]\\times [0,1])\\cap ((-\\infty,x]\\times (-\\infty,y])=[0,x]\\times [0,y].\\) Ver figura siguiente.\n\nPor tanto en este caso, 19\n\\[\nF_{XY}(x,y)=\\int_0^x \\int_0^y 1\\,du\\,dv =\\int_0^x 1\\, du\\int_0^y 1\\, dy =x\\cdot y.\n\\]\n\n\n\n\n\n\n\nDejamos como ejercicio los otros casos. En resumen: \\[\nF_{XY}(x,y)=\\begin{cases}\n0, & \\mbox{ si }x&lt;0, \\mbox{ o }y&lt;0,\\\\\nx y, & \\mbox{ si }(x,y)\\in [0,1]\\times [0,1],\\\\\nx, & \\mbox{ si }0\\leq x\\leq 1,\\ y&gt;1,\\\\\ny, & \\mbox{ si }x&gt;1,\\ 0\\leq y\\leq 1,\\\\\n1, & \\mbox{ si } x&gt;1,\\ y&gt;1.\n\\end{cases}\n\\] ¿Os suena?\nVer el primer ejemplo que pusimos del tema. Es la misma variable aleatoria bidimensional. Ahora sabemos que se trata de una variable aleatoria bidimensional continua.\nComprobemos ahora que si derivamos dos veces la expresión de \\(F_{XY}\\), primero respecto \\(x\\) y después respecto \\(y\\), obtendremos la función de densidad \\(f_{XY}\\).\nSi derivamos respecto \\(x\\) obtenemos:\n\\[\n\\frac{\\partial F_{XY}(x,y)}{\\partial x}=\\begin{cases}\n0, & \\mbox{ si }x&lt;0, \\mbox{ o }y&lt;0,\\\\\ny, & \\mbox{ si }(x,y)\\in [0,1]\\times [0,1],\\\\\n1, & \\mbox{ si }0\\leq x\\leq 1,\\ y&gt;1,\\\\\n0, & \\mbox{ si }x&gt;1,\\ 0\\leq y\\leq 1,\\\\\n0, & \\mbox{ si } x&gt;1,\\ y&gt;1.\n\\end{cases}\n\\] Si ahora derivamos respecto \\(y\\) obtenemos:\n\\[\n\\frac{\\partial^2 F_{XY}(x,y)}{\\partial y\\partial x}=\\begin{cases}\n0, & \\mbox{ si }x&lt;0, \\mbox{ o }y&lt;0,\\\\\n1, & \\mbox{ si }(x,y)\\in [0,1]\\times [0,1],\\\\\n0, & \\mbox{ si }0\\leq x\\leq 1,\\ y&gt;1,\\\\\n0, & \\mbox{ si }x&gt;1,\\ 0\\leq y\\leq 1,\\\\\n0, & \\mbox{ si } x&gt;1,\\ y&gt;1,\n\\end{cases}\n\\] expresión que coincide con la función de densidad \\(f_{XY}(x,y)\\).\nHallemos para finalizar las funciones de densidad marginales. Empecemos con \\(f_X(x)\\): \\[\nf_X(x)=\\int_{-\\infty}^\\infty  f_{XY}(x,y)\\, dy.\n\\] Recordemos que la región donde no se anulaba la función de densidad conjunta \\(f_{XY}\\) era el cuadrado \\([0,1]\\times [0,1]\\). Por tanto, fijado \\(x\\), el valor de \\(f_X(x)\\) es no nulo si la recta vertical \\(X=x\\) interseca dicho cuadrado. Y esto ocurre siempre que \\(x\\in (0,1)\\). Por tanto, \\[\nf_X(x)=\\begin{cases}\n\\int_{0}^1  f_{XY}(x,y)\\, dy=\\int_{0}^1  1\\, dy=1, & \\mbox{ si }x\\in (0,1),\\\\\n0, & \\mbox{en caso contrario.}\n\\end{cases}\n\\] Por tanto la variable \\(X\\) sigue la distribución uniforme en el intervalo \\([0,1]\\).\nDejamos como ejercicio comprobar que la variable \\(Y\\) también sigue la distribución uniforme en el mismo intervalo.\n\n\n\nEjemplo: otra función de densidad bidimensional\nConsideremos la variable aleatoria bidimensional \\((X,Y)\\) con función de densidad: \\[\nf_{XY}(x,y)=\\begin{cases}\nc\\cdot \\mathrm{e}^{-x}\\cdot\\mathrm{e}^{-y}, & 0\\leq y\\leq x &lt; \\infty,\\\\\n0, & \\mbox{ en caso contrario,}\n\\end{cases}\n\\] donde \\(c\\) es un valor que se tiene que hallar para que \\(f_{XY}\\) sea función de densidad. Calcular \\(c\\) y comprobar todas las propiedades de la función de densidad.\n\nPara hallar \\(c\\), hemos de imponer que la integral de la función anterior debe ser 1 sobre todo el plano \\(\\mathbb{R}^2\\).\nPrimero fijémonos en como es la región de integración (zona morada de la figura). Fijado un valor \\(x\\geq 0\\), el valor \\(y\\) va desde \\(y=0\\) hasta \\(y=x\\). Por tanto, para calcular el valor de \\(c\\), hay que hacer lo siguiente:\n\n\n\n\n\n\n\n\\[\n\\begin{array}{rl}\n1 &=  \\int\\int_{\\mathbb{R}^2}f_{XY}(x,y)\\, dx\\, dy=\\int_{x=0}^{x=\\infty}\\int_{y=0}^{y=x} c \\cdot\\mathrm{e}^{-x}\\cdot\\mathrm{e}^{-y} \\, dy\\, dx \\\\\n  &=  c\\cdot \\int_{x=0}^{x=\\infty}\\mathrm{e}^{-x}\\cdot\\int_{y=0}^{y=x}\\mathrm{e}^{-y}\\, dy\\, dx = c \\cdot  \\int_{x=0}^{x=\\infty}\\mathrm{e}^{-x}\\cdot\\left[-\\mathrm{e}^{-y}\\right]_{y=0}^{y=x}\\, dx \\\\\n  &=  c \\cdot\\int_{x=0}^{x=\\infty}\\mathrm{e}^{-x}\\cdot\\left(1-\\mathrm{e}^{-x}\\right)\\, dx =c \\cdot\\int_{x=0}^{x=\\infty}\\left(\\mathrm{e}^{-x}-\\mathrm{e}^{-2x}\\right)\\, dx\n  \\\\ & =  c \\cdot\\left[-\\mathrm{e}^{-x}+\\frac{1}{2}\\mathrm{e}^{-2x}\\right]_{x=0}^{x=\\infty} = c\\left(1-\\frac{1}{2}\\right)=\\frac{c}{2}.\n\\end{array}\n\\]\nEl valor de \\(c\\) es \\(c=2\\).\nVamos a calcular seguidamente su función de distribución.\nFijémonos que, en este caso, si \\(x&lt;0\\) o \\(y&lt;0\\), \\(F_{XY}(x,y)=0\\), ya que el dominio \\(B=(-\\infty,x]\\times (-\\infty,y]\\) no interseca la zona morada del gráfico anterior.\nSuponemos entonces que \\(x\\geq 0\\) e \\(y\\geq 0\\).\nVamos a considerar dos casos:\n\n\\(x\\leq y\\). Ver zona verde del gráfico siguiente.\n\\(x\\geq y\\). Ver zona morada del gráfico siguiente.\n\n\n\n\n\n\n\n\n\nCaso \\(x\\leq y\\) (zona verde de la figura adjunta). En este caso, si hacemos la intersección de la región \\(B=(-\\infty,x]\\times (-\\infty,y]\\) (zona azul) con la zona morada o región donde \\(f_{XY}(x,y)\\neq 0\\) obtenemos el triángulo \\(T_{x,y}=\\{(u,v)\\in\\mathbb{R}^2,\\ 0\\leq u\\leq x,\\ 0\\leq v\\leq u\\}.\\) Ver figura adjunta.\n\nPor tanto, \\[\n\\begin{array}{lcr}\nF_{XY}(x,y) & = & \\int_{u=0}^{u=x}\\int_{v=0}^{v=u} f_{XY}(u,v)\\,dv\\,du= 2 \\cdot\\int_{u=0}^{u=x} \\mathrm{e}^{-u}\\int_{v=0}^{v=u}  \\mathrm{e}^{-v}\\,dv\\,du\\\\ &  = &\n2 \\cdot\\int_{u=0}^{u=x} \\mathrm{e}^{-u}\\cdot\\left[-\\mathrm{e}^{-v}\\right]_{v=0}^{v=u}\\, du =  2 \\cdot\\int_{u=0}^{u=x} \\mathrm{e}^{-u} \\cdot (1-\\mathrm{e}^{-u})\\, du\n\\\\ & = & 2 \\int_{u=0}^{u=x} \\left(\\mathrm{e}^{-u}-\\mathrm{e}^{-2u}\\right)\\, du=2\\cdot \\left[-\\mathrm{e}^{-u}+\\frac{1}{2}\\cdot\\mathrm{e}^{-2u}\\right]_{u=0}^{u=x}  \\\\ & = &\n2\\cdot\\left(-\\mathrm{e}^{-x}+\\frac{1}{2}\\cdot\\mathrm{e}^{-2x}+1-\\frac{1}{2}\\right) =1-2\\cdot\\mathrm{e}^{-x}+\\mathrm{e}^{-2x}.\n\\end{array}\n\\]\n\n\n\n\n\n\n\n\nCaso \\(x\\geq y\\) (zona morada de la figura adjunta). En este caso, si hacemos la intersección de la región \\(B=(-\\infty,x]\\times (-\\infty,y]\\) (zona azul) con la zona morada o región donde \\(f_{XY}(x,y)\\neq 0\\) obtenemos el trapecio \\(T_{x,y}=\\{(u,v)\\in\\mathbb{R}^2,\\ 0\\leq v\\leq y,\\ v\\leq u\\leq x\\}.\\) Ver figura adjunta.\n\nPor tanto,\n\\[\n\\begin{array}{rl}\nF_{XY}(x,y) &=  \\int_{v=0}^{v=y}\\int_{u=v}^{u=x} f_{XY}(u,v)\\,dv\\,du= 2\\cdot\\int_{v=0}^{v=y} \\mathrm{e}^{-v}\\int_{u=v}^{u=x} \\mathrm{e}^{-u}\\,du\\,dv \\\\\n&=  2 \\cdot\\int_{v=0}^{v=y} \\mathrm{e}^{-v}\\cdot\\left[-\\mathrm{e}^{-u}\\right]_{u=v}^{u=x}\\, dv  = 2 \\cdot\\int_{v=0}^{v=y} \\mathrm{e}^{-v}\\cdot (\\mathrm{e}^{-v}-\\mathrm{e}^{-x})\\, du \\\\\n&=  2 \\cdot\\int_{v=0}^{v=y} \\left(\\mathrm{e}^{-2v}-\\mathrm{e}^{-v-x}\\right)\\, du=2 \\cdot\\left[-\\frac{1}{2}\\mathrm{e}^{-2v}+\\mathrm{e}^{-v-x}\\right]_{v=0}^{v=y}  \n\\\\ &=  2\\cdot\\left(-\\frac{1}{2}\\cdot\\mathrm{e}^{-2y}+\\mathrm{e}^{-x-y}+\\frac{1}{2}-\\mathrm{e}^{-x}\\right) \\\\&=  1-2\\cdot\\mathrm{e}^{-x}-\\mathrm{e}^{-2y}+2\\cdot\\mathrm{e}^{-x-y}.\n\\end{array}\n\\]\n\n\n\n\n\n\n\nEn resumen: \\[\nF_{XY}(x,y)=\\begin{cases}\n1-2\\cdot\\mathrm{e}^{-x}+\\mathrm{e}^{-2x}, & \\mbox{si }x\\geq 0,\\ y\\geq 0,\\ x\\leq y,\\\\\n1-2\\cdot\\mathrm{e}^{-x}-\\mathrm{e}^{-2y}+2\\cdot\\mathrm{e}^{-x-y}, & \\mbox{si }x\\geq 0,\\ y\\geq 0,\\ x\\geq y,\\\\\n0, & \\mbox{en caso contrario.}\n\\end{cases}\n\\]\nComprobemos a continuación que si derivamos dos veces la expresión de \\(F_{XY}\\), primero respecto \\(x\\) y después respecto \\(y\\), obtendremos la función de densidad \\(f_{XY}\\).\nSi derivamos respecto \\(x\\) obtenemos: \\[\n\\frac{\\partial F_{XY}(x,y)}{\\partial x}=\\begin{cases}\n2\\cdot\\mathrm{e}^{-x}-2\\cdot\\mathrm{e}^{-2x}, & \\mbox{si }x\\geq 0,\\ y\\geq 0,\\ x\\leq y,\\\\\n2\\cdot\\mathrm{e}^{-x}-2\\cdot\\mathrm{e}^{-x-y}, & \\mbox{si }x\\geq 0,\\ y\\geq 0,\\ x\\geq y,\\\\\n0, & \\mbox{en caso contrario.}\n\\end{cases}\n\\] Si ahora derivamos respecto \\(y\\) obtenemos: \\[\n\\frac{\\partial^2 F_{XY}(x,y)}{\\partial y\\partial x}=\\begin{cases}\n0, & \\mbox{si }x\\geq 0,\\ y\\geq 0,\\ x\\leq y,\\\\\n2\\cdot\\mathrm{e}^{-x-y}, & \\mbox{si }x\\geq 0,\\ y\\geq 0,\\ x\\geq y,\\\\\n0, & \\mbox{en caso contrario.}\n\\end{cases}\n\\] expresión que coincide con la función de densidad \\(f_{XY}(x,y)\\).\nHallemos las funciones de densidad marginales. Fijémonos que basta tener en cuenta los casos en que \\(x\\geq 0\\) e \\(y\\geq 0\\) ya que en caso contrario tanto \\(f_X(x)\\) como \\(f_Y(y)\\) son nulas.\n\\[\n\\begin{array}{rl}\nf_X(x) &=   \\int_{-\\infty}^{\\infty} f_{XY}(x,y)\\, dy =\\int_{y=0}^{y=x}2\\cdot\\mathrm{e}^{-x-y}\\, dy = 2\\cdot\\left[-\\mathrm{e}^{-x-y}\\right]_{y=0}^{y=x} \\\\ &=   2\\left(\\mathrm{e}^{-x}-\\mathrm{e}^{-2x}\\right),\\mbox{ si }x\\geq 0,\n\\end{array}\n\\]\n\\[\n\\begin{array}{rl}\nf_Y(y) & =  \\int_{-\\infty}^{\\infty} f_{XY}(x,y)\\, dx =\\int_{x=y}^{x=\\infty}2\\cdot\\mathrm{e}^{-x-y}\\, dx = 2\\cdot\\left[-\\mathrm{e}^{-x-y}\\right]_{x=y}^{x=\\infty}\\\\ &= 2\\cdot\\mathrm{e}^{-2y}, \\mbox{ si }y\\geq 0.\n\\end{array}\n\\]\nVemos que la variable \\(Y\\) corresponde a una distribución exponencial de parámetro \\(\\lambda =2\\).\nDibujemos la función de densidad conjunta y la función de distribución conjunta con R. Primero las definimos:\n\nfun.den.con = function(x,y){ifelse(x&gt;=0 & y&gt;=0 & x&gt;=y,\n                                   2*exp(-x-y),0)}\nfun.dist.con = function(x,y){ifelse(x&gt;=0 & y&gt;=0 & x&lt;=y,\n                    1-2*exp(-x)+exp(-2*x),ifelse(x&gt;=0 & y&gt;=0 & x&gt;=y,\n                    1-2*exp(-x)-exp(-2*y)+2*exp(-x-y),0))}\n\nA continuación las dibujamos para \\(x\\) e \\(y\\) entre \\(-1\\) y \\(4\\):\n\nx=seq(from=-1,to=4,by=0.1)\ny=seq(from=-1,to=4,by=0.1)\nz.fun.den.con=outer(x,y,fun.den.con)\nz.fun.dist.con = outer(x,y,fun.dist.con)\npersp(x,y,z.fun.den.con,theta=50,phi=40,col=\"green\",shade=0.25,ticktype=\"detailed\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n6.4.3 La distribución gaussiana bidimensional\nVamos a generalizar la distribución normal a dos dimensiones.\nDefinición de distribución gaussiana bidimensional.  Diremos que la distribución de la variable aleatoria bidimensional \\((X,Y)\\) es gaussiana bidimensional dependiendo del parámetro \\(\\rho\\) si su función de densidad conjunta es: \\[\nf_{XY}(x,y)=\\frac{1}{2\\cdot\\pi\\cdot\\sqrt{1-\\rho^2}}\\mathrm{e}^{-\\frac{(x^2-2\\cdot\\rho\\cdot x\\cdot y+y^2)}{2\\cdot(1-\\rho^2)}},\\ -\\infty &lt;x,y&lt;\\infty.\n\\]\nPropiedades de la función de densidad de la variable gaussiana bidimensional:\n\nPara cualquier punto \\((x,y)\\in\\mathbb{R}^2\\), la función de densidad es no nula: \\(f_{XY}(x,y)&gt;0\\).\nLa función de densidad tiene un único máximo absoluto en el punto \\((0,0)\\) que vale \\(f_{XY}(0,0)=\\frac{1}{2\\pi\\sqrt{1-\\rho^2}}.\\) Por tanto, para \\(\\rho=0\\), dicho máximo alcanza el mínimo valor posible y si \\(\\rho\\to \\pm 1\\), dicho máximo tiende a \\(\\infty\\).\nLas densidades marginales \\(f_X(x)\\) y \\(f_Y(y)\\) son normales \\(N(0,1)\\).\n\n\nVeámoslo con \\(f_X(x)\\). Por simetría, quedaría deducido para \\(f_Y(y)\\): \\[\n\\begin{array}{rl}\nf_X(x) & =\\frac{1}{2\\cdot\\pi\\cdot\\sqrt{1-\\rho^2}}\\int_{-\\infty}^\\infty \\mathrm{e}^{-\\frac{(x^2-2\\cdot\\rho \\cdot x \\cdot y+y^2)}{2\\cdot(1-\\rho^2)}}\\, dy =\n\\frac{1}{2\\cdot\\pi\\cdot\\sqrt{1-\\rho^2}}\\mathrm{e}^{-\\frac{x^2}{2\\cdot(1-\\rho^2)}}\\int_{-\\infty}^\\infty \\mathrm{e}^{-\\frac{(-2\\cdot\\rho x\\cdot y+y^2)}{2\\cdot(1-\\rho^2)}}\\, dy \\\\ & = \\frac{1}{2\\cdot\\pi\\cdot\\sqrt{1-\\rho^2}}\\mathrm{e}^{-\\frac{x^2}{2\\cdot(1-\\rho^2)}} \\int_{-\\infty}^\\infty \\mathrm{e}^{-\\frac{(y-\\rho x)^2}{2(1-\\rho^2)}} \\mathrm{e}^{\\frac{\\rho^2 \\cdot x^2}{2\\cdot(1-\\rho^2)}}\\, dy \\\\ & =\\frac{1}{2\\cdot\\pi\\cdot\\sqrt{1-\\rho^2}}\\mathrm{e}^{-\\frac{x^2}{2}} \\int_{-\\infty}^\\infty \\mathrm{e}^{-\\frac{(y-\\rho x)^2}{2(1-\\rho^2)}}\\, dy,  \\mbox{ hacemos el cambio $z=\\frac{y-\\rho x}{\\sqrt{1-\\rho^2}}$}\\\\ & = \\frac{1}{2\\cdot\\pi\\cdot\\sqrt{1-\\rho^2}}\\mathrm{e}^{-\\frac{x^2}{2}} \\int_{-\\infty}^\\infty \\mathrm{e}^{-\\frac{z^2}{2}}\\sqrt{1-\\rho^2}\\, dy =\\frac{1}{\\sqrt{2\\pi}}\\mathrm{e}^{-\\frac{x^2}{2}},\n\\end{array}\n\\] función que coincide con la función de densidad de la variable \\(N(0,1)\\).\nEn el último paso hemos usado que \\[\n\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^\\infty \\mathrm{e}^{-\\frac{z^2}{2}}\\, dz=1,\n\\] ya que correspondería al área de una función de densidad de una distribución \\(N(0,1)\\).\n\n\n6.4.3.1 La distribución gaussiana bidimensional en R\n\nmnormt La función que nos la densidad de la distribución normal bidimensional es nbvpdf y tiene 5 parámetros: la media de \\(X\\) (\\(\\mu_X\\)), la media de \\(Y\\) (\\(\\mu_Y\\)), la desviación típica de \\(X\\) (\\(\\sigma_X\\)), la desviación típica de \\(Y\\) (\\(\\sigma_Y\\)) y un concepto que veremos más adelante, la correlación entre \\(X\\) e \\(Y\\) (\\(\\rho_{XY}\\)).\n\nEn el ejemplo que estamos tratando, los valores de los parámetros anteriores son: \\(\\mu_X=\\mu_Y=0\\), \\(\\sigma_X=\\sigma_Y=1\\) y \\(\\rho_{XY}=\\rho.\\)\nVamos a hacer un gráfico de la distribución normal bidimensional para \\(\\rho=\\frac{1}{2}.\\)\n\nlibrary(mnormt)\nx     &lt;- seq(-3, 3, 0.1) \ny     &lt;- seq(-3, 3, 0.1)\nmu    &lt;- c(0, 0)\nsigma &lt;- matrix(c(1, 0.5, 0.5, 1), nrow=2)\nf     &lt;- function(x, y) dmnorm(cbind(x, y), mu, sigma)\nz     &lt;- outer(x, y, f)\n\n# Ahora dibujo  los puntos de la densidad\npersp(x, y, z, theta=-30, phi=25, expand=0.6,\n      ticktype='detailed',col=\"blue\",\n      cex.axis=0.5,cex.lab=0.8)"
  },
  {
    "objectID": "5.html#independencia-de-variables-aleatorias",
    "href": "5.html#independencia-de-variables-aleatorias",
    "title": "6  Vectores aleatorios bidimensionales",
    "section": "6.5 Independencia de variables aleatorias",
    "text": "6.5 Independencia de variables aleatorias\n\n6.5.1 Independencia de variables aleatorias discretas\nRecordemos que dos sucesos \\(A\\) y \\(B\\) son independientes si \\(P(A\\cap B)=P(A)\\cdot P(B)\\).\n¿Cómo trasladar dicho concepto al caso de variables aleatorias?\nEn el caso de variables aleatorias discretas bidimensionales vimos que, dada una variable aleatoria bidimensional discreta \\((X,Y)\\) con \\((X,Y)(\\Omega)=\\{(x_i,y_j),\\ i=1,2,\\ldots,j=1,2,\\ldots\\}\\), los sucesos de la forma \\(\\{X=x_i,\\  Y=y_j\\}\\) determinaban cómo se distribuían los valores de la variable \\((X,Y)\\). De ahí la definición siguiente:\nDefinición de independencia para variables aleatorias bidimensionales discretas.  Sean \\((X,Y)\\) una variable aleatoria bidimensional discreta con \\((X,Y)(\\Omega)=\\{(x_i,y_j),\\ i=1,2,\\ldots,j=1,2,\\ldots\\}\\) y función de probabilidad \\(P_{XY}\\) y funciones de probabilidad marginales \\(P_X\\) y \\(P_Y\\). Entonces \\(X\\) e \\(Y\\) son independientes si: \\[\nP_{XY}(x_i,y_j)=P_X(x_i)\\cdot P_Y(y_k),\\ i=1,2,\\ldots,j=1,2,\\ldots\n\\] o dicho de otra forma: \\[\nP(X=x_i,\\ Y=y_k)=P(X=x_i)\\cdot P(Y=y_k),\\ i=1,2,\\ldots,j=1,2,\\ldots\n\\]\n\nEjemplo: suma y el producto de los resultados de dos lanzamientos de un dado\nConsideramos la variable aleatoria \\((S,P)\\) donde \\(S\\) representa la suma de los valores obtenidos al lanzar dos veces un dado y \\(P\\), su producto.\n\nEn este caso \\(S\\) y \\(P\\) no son independientes ya que recordemos que por ejemplo \\(P_{SP}(3,2)=\\frac{2}{36}\\), \\(P_S(3)=\\frac{2}{36}\\) y \\(P_P(2)=\\frac{2}{36}\\), ya que en este último caso, sólo hay dos posibles resultados en los que el producto dé 2: el \\((1,2)\\) y el \\((2,1)\\).\nEntonces no se cumple que \\(P_{SP}(3,2)=P_S(3)\\cdot P_P(2)\\), ya que \\(\\frac{2}{36}\\neq \\frac{2}{36}\\cdot \\frac{2}{36}\\).\nDe ahí que no sean independientes ya que la condición anterior se debería cumplir para todos los valores \\(x_i\\) e \\(y_k\\) y hemos encontrado un contraejemplo en donde no se cumple.\n\n\nObservación.  Si la tabla de la función de probabilidad conjunta de \\((X,Y)\\) contiene algún \\(0\\), \\(X\\) e \\(Y\\) no pueden ser independientes. ¿Podéis decir por qué?\n\nEjemplo: un caso de imdependencia\nVeamos un caso de independencia. Consideramos el experimento aleatorio de lanzar un dado dos veces. Sea \\(X\\) el resultado del primer lanzamiento e \\(Y\\), el resultado del segundo lanzamiento.\nVeamos que, en este caso, \\(X\\) e \\(Y\\) son independientes.\n\nEl valor de \\((X,Y)(\\Omega)=\\{(1,1),(1,2),\\ldots,(6,6)\\}\\), en total 36 resultados.\nLa función de probabilidad conjunta en un valor cualquiera \\((i,j)\\) con \\(i,j\\in\\{1,2,3,4,5,6\\}\\) es: \\(P_{XY}(i,j)=\\frac{1}{36}\\) ya que la probabilidad que salga \\(i\\) en el primer lanzamiento es \\(\\frac{1}{6}\\) y la probabilidad de que salga \\(j\\) en el segundo lanzamiento, también. Por tanto, la probabilidad de que salga \\(i\\) en el primer lanzamiento y \\(j\\) en el segundo es: \\(\\frac{1}{6}\\cdot \\frac{1}{6}=\\frac{1}{36}.\\)\nLas funciones de densidad marginales de \\(X\\) e \\(Y\\) son:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(X\\) o \\(Y\\)\n1\n2\n3\n4\n5\n6\n\n\n\n\n\\(P_X\\) o \\(P_Y\\)\n\\(\\frac{1}{6}\\)\n\\(\\frac{1}{6}\\)\n\\(\\frac{1}{6}\\)\n\\(\\frac{1}{6}\\)\n\\(\\frac{1}{6}\\)\n\\(\\frac{1}{6}\\)\n\n\n\n\nPor tanto, para todo \\((i,j)\\) con \\(i,j\\in\\{1,2,3,4,5,6\\}\\) se cumplirá: \\[\nP_{XY}(i,j)=\\frac{1}{36}=\\frac{1}{6}\\cdot \\frac{1}{6}=P_X(i)\\cdot P_Y(j).\n\\] Deducimos que son independientes.\nPara comprobar si dos variables aleatorias \\(X\\) e \\(Y\\) son independientes o no en R en general, una vez calculada la tabla de la función de probabilidad, podemos calcular la tabla de independencia teórica \\(P_T(x_i,y_j)\\) y compararlas. Ésta segunda tabla se define de la forma siguiente: \\[\nP_T(x_i,y_j)=P_X(x_i)\\cdot P_Y(y_j),\n\\] donde \\(P_X\\) y \\(P_Y\\) son las distribuciones marginales.\nLa tabla de independencia teórica  en el caso de la suma y el producto se calcularían de la forma siguiente:\n\ntabla.ind.teor =  marginal.suma%*%t(marginal.producto)\ntabla.ind.teor = as.data.frame(tabla.ind.teor)\nrownames(tabla.ind.teor)=rownames(tabla.func.prob.conjunta)\ncolnames(tabla.ind.teor)=colnames(tabla.func.prob.conjunta)\n\nSi comparamos los resultados de la tabla de independencia teórica mostrada a continuación con los resultados de la tabla de la función de probabilidad conjunta, veréis que no son iguales. Por tanto, \\(S\\) y \\(P\\) no son independientes.\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n8\n9\n10\n\n\n\n\n2\n0.001\n0.002\n0.002\n0.002\n0.002\n0.003\n0.002\n0.001\n0.002\n\n\n3\n0.002\n0.003\n0.003\n0.005\n0.003\n0.006\n0.003\n0.002\n0.003\n\n\n4\n0.002\n0.005\n0.005\n0.007\n0.005\n0.009\n0.005\n0.002\n0.005\n\n\n5\n0.003\n0.006\n0.006\n0.009\n0.006\n0.012\n0.006\n0.003\n0.006\n\n\n6\n0.004\n0.008\n0.008\n0.012\n0.008\n0.015\n0.008\n0.004\n0.008\n\n\n7\n0.005\n0.009\n0.009\n0.014\n0.009\n0.019\n0.009\n0.005\n0.009\n\n\n8\n0.004\n0.008\n0.008\n0.012\n0.008\n0.015\n0.008\n0.004\n0.008\n\n\n9\n0.003\n0.006\n0.006\n0.009\n0.006\n0.012\n0.006\n0.003\n0.006\n\n\n10\n0.002\n0.005\n0.005\n0.007\n0.005\n0.009\n0.005\n0.002\n0.005\n\n\n11\n0.002\n0.003\n0.003\n0.005\n0.003\n0.006\n0.003\n0.002\n0.003\n\n\n12\n0.001\n0.002\n0.002\n0.002\n0.002\n0.003\n0.002\n0.001\n0.002\n\n\n\n\n\n\n\n\n\n12\n15\n16\n18\n20\n24\n25\n30\n36\n\n\n\n\n2\n0.003\n0.002\n0.001\n0.002\n0.002\n0.002\n0.001\n0.002\n0.001\n\n\n3\n0.006\n0.003\n0.002\n0.003\n0.003\n0.003\n0.002\n0.003\n0.002\n\n\n4\n0.009\n0.005\n0.002\n0.005\n0.005\n0.005\n0.002\n0.005\n0.002\n\n\n5\n0.012\n0.006\n0.003\n0.006\n0.006\n0.006\n0.003\n0.006\n0.003\n\n\n6\n0.015\n0.008\n0.004\n0.008\n0.008\n0.008\n0.004\n0.008\n0.004\n\n\n7\n0.019\n0.009\n0.005\n0.009\n0.009\n0.009\n0.005\n0.009\n0.005\n\n\n8\n0.015\n0.008\n0.004\n0.008\n0.008\n0.008\n0.004\n0.008\n0.004\n\n\n9\n0.012\n0.006\n0.003\n0.006\n0.006\n0.006\n0.003\n0.006\n0.003\n\n\n10\n0.009\n0.005\n0.002\n0.005\n0.005\n0.005\n0.002\n0.005\n0.002\n\n\n11\n0.006\n0.003\n0.002\n0.003\n0.003\n0.003\n0.002\n0.003\n0.002\n\n\n12\n0.003\n0.002\n0.001\n0.002\n0.002\n0.002\n0.001\n0.002\n0.001\n\n\n\n\n\nlo cual finaliza nuestros cálculos en R.\n\n\n\n\n6.5.2 Independencia de variables aleatorias continuas\nLa definición dada para variables aleatorias discretas se traslada de forma natural a las variables aleatorias continuas:\nDefinición de independencia para variables aleatorias bidimensionales continuas.  Sean \\((X,Y)\\) una variable aleatoria bidimensional continua con función de densidad conjunta \\(f_{XY}\\) y funciones de densidad marginales \\(f_X\\) y \\(f_Y\\). Entonces \\(X\\) e \\(Y\\) son independientes si: \\[\nf_{XY}(x,y)=f_X(x)\\cdot f_Y(y),\\ \\mbox{para todo $x,y\\in\\mathbb{R}$.}\n\\]\n\n6.5.2.1 Ejemplos\n\nEjemplo: densidad uniforme en el cuadrado unidad\nRecordemos el ejemplo siguiente visto donde teníamos una variable aleatoria bidimensional continua \\((X,Y)\\) con función de densidad conjunta: \\[\nf_{XY}(x,y)=\\begin{cases}\n1, & \\mbox{ si }0\\leq x\\leq 1,\\ 0\\leq y\\leq 1, \\\\\n0, & \\mbox{en caso contrario.}\n\\end{cases}\n\\] y con densidad marginales: \\[\nf_{X}(x)=\\begin{cases}\n1, & \\mbox{ si }0\\leq x\\leq 1,\\\\\n0, & \\mbox{en caso contrario.}\n\\end{cases}\\quad f_{Y}(y)=\\begin{cases}\n1, & \\mbox{ si }0\\leq y\\leq 1,\\\\\n0, & \\mbox{en caso contrario.}\n\\end{cases}\n\\]\nVeamos que son independientes.\n\nConsideremos dos casos:\n\n\\((x,y)\\in [0,1]\\times [0,1]\\). En este caso: \\[\nf_{XY}(x,y) =1 =1\\cdot 1=f_X(x)\\cdot f_Y(y).\n\\]\n\\((x,y)\\not\\in [0,1]\\times [0,1]\\). En este caso: \\[\nf_{XY}(x,y) =0 = f_X(x)\\cdot f_Y(y),\n\\] ya que si \\((x,y)\\not\\in [0,1]\\times [0,1]\\), o \\(x\\not\\in [0,1]\\) o \\(y\\not\\in [0,1]\\). Por tanto \\(f_X(x)=0\\) o \\(f_Y(y)=0\\). En cualquier caso, \\(f_X(x)\\cdot f_Y(y)=0\\).\n\n\n\n\nEjemplo: otra función de densidad (continuación)\nRecordemos el ejemplo siguiente visto donde teníamos una variable aleatoria bidimensional continua \\((X,Y)\\) con función de densidad conjunta: \\[\nf_{XY}(x,y)=\\begin{cases}\n2 \\cdot \\mathrm{e}^{-x}\\cdot\\mathrm{e}^{-y}, & 0\\leq y\\leq x &lt; \\infty,\\\\\n0, & \\mbox{ en caso contrario,}\n\\end{cases}\n\\] y con densidad marginales: \\[\nf_X(x)  = 2\\left(\\mathrm{e}^{-x}-\\mathrm{e}^{-2x}\\right),\\mbox{ si }x\\geq 0, \\quad\nf_Y(y)  =  2\\mathrm{e}^{-2y}, \\mbox{ si }y\\geq 0.\n\\]\nEn este caso no son independientes ya que claramente \\(f_{XY}(x,y)\\neq f_X(x)\\cdot f_Y(y)\\).\n\nEn este caso, recordemos que la función de densidad conjunta de \\((X,Y)\\) es: \\[\nf_{XY}(x,y)=\\frac{1}{2\\cdot \\pi\\cdot \\sqrt{1-\\rho^2}}\\mathrm{e}^{-\\frac{(x^2-2\\cdot \\rho \\cdot x\\cdot y+y^2)}{2\\cdot (1-\\rho^2)}},\\ -\\infty &lt;x,y&lt;\\infty.\n\\] Las funciones de densidad marginales de \\(X\\) e \\(Y\\) correspondían a \\(N(0,1)\\): \\[\n\\begin{array}{rl}\nf_X(x) & =\\frac{1}{\\sqrt{2\\pi}}\\cdot \\mathrm{e}^{-\\frac{x^2}{2}},\\ -\\infty &lt;x&lt;\\infty,\\\\ f_Y(y) & =\\frac{1}{\\sqrt{2\\pi}}\\mathrm{e}^{-\\frac{y^2}{2}},\\ -\\infty &lt;y&lt;\\infty.\n\\end{array}\n\\]\n¿Para qué valor(es) de \\(\\rho\\) las variables normales estándar \\(X\\) e \\(Y\\) son independientes?\no, ¿para qué valor(es) de \\(\\rho\\) se cumple?\n\\[\nf_X(x)\\cdot f_Y(y)=\\frac{1}{2\\cdot\\pi}\\mathrm{e}^{-\\frac{x^2+y^2}{2}} = \\frac{1}{2\\cdot\\pi\\cdot\\sqrt{1-\\rho^2}}\\mathrm{e}^{-\\frac{(x^2-2\\cdot\\rho x \\cdot y+y^2)}{2\\cdot (1-\\rho^2)}}.\n\\] La respuesta es claramente para \\(\\rho=0\\).\nPor tanto, \\(\\rho\\) se puede interpretar como un parámetro de independencia, cuánto más cercano a cero esté, más cerca de la independencia estarán las variables \\(X\\) e \\(Y\\).\n\n\n\n\n\n6.5.3 Relación de la independencia y la función de distribución\nEl siguiente resultado nos da la relación entre la independencia de variables aleatorias y su función de distribución conjunta:\nTeorema.  Sea \\((X,Y)\\) una variable aleatoria bidimensional. Entonces \\(X\\) e \\(Y\\) son independientes si, y sólo si, la función de distribución conjunta es el producto de las funciones de distribución marginales en todo valor \\((x,y)\\in\\mathbb{R}^2\\): \\[\nF_{XY}(x,y)=F_X(x)\\cdot F_Y(y),\\ (x,y)\\in\\mathbb{R}^2.\n\\]\n\nEjemplo\nConsideramos el experimento aleatorio de lanzar un dado dos veces. Sea \\(X\\) el resultado del primer lanzamiento e \\(Y\\), el resultado del segundo lanzamiento.\nRecordemos que, en este caso, \\(X\\) e \\(Y\\) son independientes.\n\nEn primer lugar notemos que si \\(x&lt;1\\) o \\(y&lt;1\\), \\(F_{XY}(x,y)=0\\) ya que el suceso \\(\\{X\\leq x,\\ Y\\leq y\\}\\) es vacío.\nDe la misma forma como \\(x&lt;1\\) o \\(y&lt;1\\), o el suceso \\(\\{X\\leq x\\}\\) o el suceso \\(\\{Y\\leq y\\}\\) son vacíos. Por tanto, o \\(F_X(x)=0\\) o \\(F_Y(y)=0\\).\nEn cualquier caso, se cumple \\(F_{XY}(x,y)=0=F_X(x)\\cdot F_Y(y)\\).\nPodemos suponer, por tanto, que \\(x\\geq 1\\) e \\(y\\geq 1\\).\nSea \\((x,y)\\in \\mathbb{R}^2\\) con \\(x\\geq 1\\) e \\(y\\geq 1\\). Podemos suponer tal que existen dos valores \\(i\\) y \\(j\\) en \\(\\{1,2,\\ldots\\}\\) con \\(i\\leq x &lt; i+1\\) y \\(j\\leq y &lt;j+1\\).\nEl valor de la función de distribución conjunta en \\((x,y)\\) es: \\[\nF_{XY}(x,y)=\\begin{cases}\n\\frac{i\\cdot j}{36}, & \\mbox{si }i\\leq 6, \\ j\\leq 6, \\\\\n\\frac{6 \\cdot i}{36}, & \\mbox{si }i\\leq 6,\\ j\\geq 6,\\\\\n\\frac{6\\cdot j}{36}, & \\mbox{si }i\\geq 6,\\ j\\leq 6,\\\\\n1, & \\mbox{ si }i\\geq 6,\\ j\\geq 6,\n\\end{cases}\n\\]\nya que:\n\\[\n\\begin{array}{rl}\nF_{XY}(x,y) & =P(X\\leq i,\\ Y\\leq j)=P(\\{(k,l)\\in \\{1,2,3,4,5,6\\}^2,\\ |\\ k\\leq i,\\ l\\leq j\\})\\\\ & =P(\\{(1,1),\\ldots,(1,j),\\ldots,(i,1),\\ldots,(i,j)\\})\n\\\\\n& =\\begin{cases}\n\\frac{i\\cdot j}{36}, & \\mbox{si }i\\leq 6, \\ j\\leq 6, \\\\\n\\frac{6\\cdot i}{36}, & \\mbox{si }i\\leq 6,\\ j\\geq 6,\\\\\n\\frac{6\\cdot j}{36}, & \\mbox{si }i\\geq 6,\\ j\\leq 6,\\\\\n1, & \\mbox{ si }i\\geq 6,\\ j\\geq 6,\n\\end{cases},\n\\end{array}\n\\] ya que claramente el cardinal del conjunto \\[\\{(1,1),\\ldots,(1,j),\\ldots,(i,1),\\ldots,(i,j)\\}\\] es\n\\[\n\\begin{cases}\ni\\cdot j, & \\mbox{si }i\\leq 6, \\ j\\leq 6, \\\\\n\\cdot  i, & \\mbox{si }i\\leq 6,\\ j\\geq 6,\\\\\n6\\cdot j, & \\mbox{si }i\\geq 6,\\ j\\leq 6,\\\\\n36, & \\mbox{ si }i\\geq 6,\\ j\\geq 6.\n\\end{cases}\n\\]\nHallemos ahora la función de distribución de \\(X\\) e \\(Y\\) que consiste en el resultado del lanzamiento de un dado.\nDado \\(x\\in\\mathbb{R}\\) con \\(x\\geq 1\\), existe un \\(i\\) con \\(i\\in\\{1,2,\\ldots,\\}\\) con \\(i\\leq x &lt;i+1\\). En este caso, el valor de \\(F_X(x)\\) es:\n\\[\nF_X(x)=\\begin{cases}\n\\frac{i}{6}, &\\mbox{si }i\\leq 6,\\\\\n1, & \\mbox{si }i\\geq 6,\n\\end{cases}\n\\] ya que:\n\\[\n\\begin{array}{rl}\nF_X(x) = & F_X(i)=P(X\\leq i)=P(\\{k\\in\\{1,2,3,4,5,6\\},\\ |\\ k\\leq i\\})\n\\\\ = &\n\\begin{cases}\n\\frac{i}{6}, &\\mbox{si }i\\leq 6,\\\\\n1, & \\mbox{si }i\\geq 6,\n\\end{cases}\n\\end{array}\n\\] \\begin ya que el cardinal del conjunto \\(\\{k\\in\\{1,2,3,4,5,6\\},\\ |\\ k\\leq i\\}\\) es \\(\\begin{cases} i, &\\mbox{si }i\\leq 6,\\\\ 6, & \\mbox{si }i\\geq 6. \\end{cases}\\)\nLa función de distribución de \\(Y\\) es de la misma forma.\nPor último, comprobemos que se verifica que \\(F_{XY}(x,y)=F_X(x)\\cdot F_Y(y)\\), si \\(x\\geq 1\\) e \\(y\\geq 1\\).\nSea \\((x,y)\\in\\mathbb{R}^2\\) y sean los enteros \\(i\\) y \\(j\\) tales que \\(i\\leq x&lt;i+1\\) y \\(j\\leq y&lt;j+1\\). Consideremos 4 casos:\n\n\\(i\\leq 6, \\ j\\leq 6\\). En este caso: \\[\nF_{XY}(x,y)=\\frac{i\\cdot j}{36}=\\frac{i}{6}\\cdot \\frac{j}{6}=F_X(x)\\cdot F_Y(y).\n\\]\n\\(i\\leq 6,\\ j\\geq 6\\). En este caso: \\[\nF_{XY}(x,y)=\\frac{6i}{36}=\\frac{i}{6}\\cdot 1=F_X(x)\\cdot F_Y(y).\n\\]\n\\(i\\geq 6,\\ j\\leq 6\\). En este caso: \\[\nF_{XY}(x,y)=\\frac{6j}{36}=1\\cdot \\frac{j}{6}=F_X(x)\\cdot F_Y(y).\n\\]\n\\(i\\geq 6,\\ j\\geq 6\\). En este caso: \\[\nF_{XY}(x,y)=1=1\\cdot 1=F_X(x)\\cdot F_Y(y).\n\\]\n\nEn resumen, para todo \\((x,y)\\in \\mathbb{R}^2\\) se verifica que \\(F_{XY}(x,y)=F_X(x)\\cdot F_Y(y)\\), tal como queríamos ver.\n\n\n\nEjemplo\nRecordemos la variable aleatoria bidimensional continua con función de densidad conjunta: \\[\nf_{XY}(x,y)=\\begin{cases}\n1, & \\mbox{ si }0\\leq x\\leq 1,\\ 0\\leq y\\leq 1, \\\\\n0, & \\mbox{en caso contrario.}\n\\end{cases}\n\\] Su función de distribución conjunta es:\n\n\\[\nF_{XY}(x,y)=\\begin{cases}\n0, & \\mbox{si }x&lt;0,\\mbox{ o }y&lt;0,\\\\\nx\\cdot y, & \\mbox{si }0\\leq x\\leq 1,\\ 0\\leq y\\leq 1, \\\\\nx, & \\mbox{si }0\\leq x\\leq 1,\\ y&gt; 1, \\\\\ny, & \\mbox{si }0\\leq y\\leq 1,\\ x&gt; 1, \\\\\n1, & x\\geq 1,\\ y\\geq 1.\n\\end{cases}\n\\]\nRecordemos también que las distribuciones marginales de \\(X\\) e \\(Y\\) eran uniformes en el intervalo \\([0,1]\\). Por tanto, las funciones de distribución marginales son: \\[\nF_X(x)=\\begin{cases}\n0, & \\mbox{si }x\\leq 0, \\\\\nx, & \\mbox{si }0\\leq x\\leq 1, \\\\\n1, & \\mbox{si }x\\geq 1. \\\\\n\\end{cases},\\quad\nF_Y(y)=\\begin{cases}\n0, & \\mbox{si }y\\leq 0, \\\\\ny, & \\mbox{si }0\\leq y\\leq 1, \\\\\n1, & \\mbox{si }y\\geq 1. \\\\\n\\end{cases}\n\\] Recordemos que \\(X\\) e \\(Y\\) son independientes. Verifiquemos que \\(F_{XY}(x,y)=F_X(x)\\cdot F_Y(y)\\).\nDistinguiremos cinco casos:\n\n\\(x&lt;0\\) o \\(y&lt;0\\). En este caso, \\(F_{XY}(x,y)=0\\) y, o \\(F_X(x)=0\\), si \\(x&lt;0\\), o \\(F_Y(y)=0\\), si \\(y&lt;0\\). En cualquier caso, se cumple que \\(F_{XY}(x,y)=F_X(x)\\cdot F_Y(y)\\).\n\\(0\\leq x\\leq 1,\\ 0\\leq y\\leq 1\\). En este caso, \\(F_{XY}(x,y)=xy\\), \\(F_X(x)=x\\) y \\(F_Y(y)=y\\). Claramente, se cumple que \\(F_{XY}(x,y)=F_X(x)\\cdot F_Y(y)\\).\n\\(0\\leq x\\leq 1,\\ y&gt; 1\\). En este caso, \\(F_{XY}(x,y)=x\\), \\(F_X(x)=x\\) y \\(F_Y(y)=1\\). Claramente, se cumple que \\(F_{XY}(x,y)=F_X(x)\\cdot F_Y(y)\\).\n\\(x &gt;1,\\ 0\\leq y\\leq 1\\). En este caso, \\(F_{XY}(x,y)=y\\), \\(F_X(x)=1\\) y \\(F_Y(y)=y\\). Claramente, se cumple que \\(F_{XY}(x,y)=F_X(x)\\cdot F_Y(y)\\).\n\\(x\\geq 1,\\ y\\geq 1\\). En este caso, \\(F_{XY}(x,y)=1\\), \\(F_X(x)=1\\) y \\(F_Y(y)=1\\). Claramente, se cumple que \\(F_{XY}(x,y)=F_X(x)\\cdot F_Y(y)\\)."
  },
  {
    "objectID": "5.html#momentos-conjuntos-y-valores-esperados-conjuntos",
    "href": "5.html#momentos-conjuntos-y-valores-esperados-conjuntos",
    "title": "6  Vectores aleatorios bidimensionales",
    "section": "6.6 Momentos conjuntos y valores esperados conjuntos",
    "text": "6.6 Momentos conjuntos y valores esperados conjuntos\nEl valor esperado de una variable aleatoria \\(X\\) se identifica con el centro de masa de la distribución de \\(X\\).\nLa varianza proporciona una medida de la extensión de la distribución.\nEn el caso de dos variables aleatorias, estamos interesados en cómo \\(X\\) e \\(Y\\) varían juntas.\nEn particular, nos interesa saber si la variación de \\(X\\) e \\(Y\\) está correlacionada. Por ejemplo, si \\(X\\) aumenta, ¿Y tiende a aumentar o disminuir?\nLos momentos conjuntos de \\(X\\) e \\(Y\\), que se definen como valores esperados de las funciones de \\(X\\) e \\(Y\\), proporcionan esta información.\n\n6.6.1 Valor esperado de una función de dos variables aleatorias\nSea \\((X,Y)\\) una variable aleatoria bidimensional.\nSea \\(P_{XY}\\) su función de probabilidad conjunta en el caso en que \\((X,Y)\\) sea discreta y \\(f_{XY}\\) su función de densidad conjunta en el caso en que \\((X,Y)\\) sea continua.\nSea \\(Z=g(X,Y)\\) una variable aleatoria unidimensional función de las variables \\(X\\) e \\(Y\\). Por ejemplo:\n\nSuma de las dos variables \\(g(x,y)=x+y\\): \\(Z=X+Y\\).\nProducto de las dos variables \\(g(x,y)=x\\cdot y\\): \\(Z=X\\cdot Y\\).\nSuma de los cuadrados de las variables \\(g(x,y)=x^2+y^2\\): \\(Z=X^2+Y^2\\).\n\nHay que tener en cuenta que \\(Z\\), como variable aleatoria unidimensional tiene una función de probabilidad \\(P_Z\\) en el caso en que \\((X,Y)\\) sea discreta y una función de densidad \\(f_Z\\) en el caso en que \\((X,Y)\\) sea continua.\nEl siguiente resultado nos dice cómo calcular el valor esperado de \\(Z\\) sin tener que calcular \\(P_Z\\) o \\(f_Z\\), sólo usando la información de la variable aleatoria conjunta \\((X,Y)\\):\nProposición.  El valor esperado de \\(Z\\) se puede hallar usando la expresión siguiente:\n\nen el caso en que \\((X,Y)\\) sea discreta con \\((X,Y)(\\Omega)=\\{(x_i,y_j),\\ i=1,2,\\ldots, j=1,2,\\ldots\\}\\), \\[\nE(Z)  = E(g(X,Y))  =\\sum_{x_i}\\sum_{y_j}g(x_i,y_j)\\cdot P(x_i,y_j),\n\\]\nen el caso en que \\((X,Y)\\) sea continua: \\[\nE(Z)=E(g(X,Y))=\\int_{-\\infty}^\\infty \\int_{-\\infty}^\\infty g(x,y)\\cdot f_{XY}(x,y)\\, dx\\, dy.\n\\]\n\n\n6.6.1.1 Ejemplos\n\nEjemplo: suma y producto de dos dados (continuación)\nConsideremos el ejemplo de la variable \\((S,P)\\) que nos daba la suma y el producto de los resultados cuando lanzábamos dos veces un dado.\nVamos a calcular \\(E(S+P)\\).\n\nRecordemos que ya hemos calculado \\(P_{SP}\\). La expresión de \\(E(S+P)\\) es:\n\\[\n\\begin{array}{rl}\nE(S+P) & = (2+1)\\cdot P_{SP}(2,1)+(3+2)\\cdot P_{SP}(3,2)+(4+3)\\cdot P_{SP}(4,3)  \\\\ &\n\\quad +(4+4)\\cdot P_{SP}(4,4) + (5+4)\\cdot P_{SP}(5,4)+(5+6)\\cdot P_{SP}(5,6)\\\\ &\n\\quad +(6+5)\\cdot P_{SP}(6,5)+(6+8)\\cdot P_{SP}(6,8)+ (6+9)\\cdot P_{SP}(6,9) \\\\ &\n\\quad + (7+6)\\cdot P_{SP}(7,6)+(7+10)\\cdot P_{SP}(7,10)+(7+12)\\cdot P_{SP}(7,12)\\\\ &\n\\quad + (8+12)\\cdot P_{SP}(8,12)+(8+15)\\cdot P_{SP}(8,15)+(8+16)\\cdot P_{SP}(8,16)\\\\ &\n\\quad +(9+18)\\cdot P_{SP}(9,18)+ (9+20)\\cdot P_{SP}(9,20)\\\\ &\n\\quad +(10+24)\\cdot P_{SP}(10,24) +(10+25)\\cdot P_{SP}(10,25)\\\\ &\n\\quad +(11+30)\\cdot P_{SP}(11,30)  + (12+36)\\cdot P_{SP}(12,36) \\\\ &\n=  3\\cdot \\frac{1}{36}+5\\cdot\\frac{2}{36}+7\\cdot \\frac{2}{36}+8\\cdot \\frac{1}{36}+9\\cdot \\frac{2}{36}+11\\cdot\\frac{2}{36}+11\\cdot \\frac{2}{36}+14\\cdot\\frac{2}{36}\n\\\\ &  \\quad  +15\\cdot\\frac{1}{36} + 13\\cdot\\frac{2}{36}+17\\cdot\\frac{2}{36}+19\\cdot\\frac{2}{36}+20\\cdot\\frac{2}{36}+23\\cdot\\frac{2}{36}+24\\cdot\\frac{1}{36}\n\\\\ & \\quad+27\\cdot\\frac{2}{36}+29\\cdot\\frac{2}{36} + 34\\cdot\\frac{2}{36}+35\\cdot\\frac{1}{36}+41\\cdot\\frac{2}{36}+48\\cdot\\frac{1}{36}\\\\\n& =\\frac{693}{36}= 19.25.\n\\end{array}\n\\]\nHallar el valor esperado de la suma \\(E(S+P)\\) una vez hallada la tabla de la función de probabilidad conjunta, en R es bastante sencillo usando la función outer:\n\nvalores.suma = as.integer(rownames(tabla.func.prob.conjunta))\nvalores.producto = as.integer(colnames(tabla.func.prob.conjunta))\nsuma.valores = outer(valores.suma,valores.producto,\"+\")\n(valor.esperado.suma = sum(suma.valores*tabla.func.prob.conjunta))\n\n[1] 19.25\n\n\n\n\nObservación: En R para hallar el valor esperado de una función \\(g(X,Y)\\), \\(E(g(X,Y))\\) de las variables aleatorias \\(X\\) e \\(Y\\), basta sustituir el valor + en el script anterior por FUN=g, definiendo previamente la función g.\n\nEjemplo: otra densidad (continuación)\nRecordemos el ejemplo donde \\((X,Y)\\) era una variable aleatoria bidimensional continua con función de densidad conjunta: \\[\nf_{XY}(x,y)=\\begin{cases}\n2\\cdot \\mathrm{e}^{-x}\\cdot\\mathrm{e}^{-y}, & 0\\leq y\\leq x &lt; \\infty,\\\\\n0, & \\mbox{ en caso contrario.}\n\\end{cases}\n\\] Calculemos \\(E(X\\cdot Y)\\):\n\n\\[\n\\begin{array}{rl}\nE(X\\cdot Y) & =\\displaystyle \\int_{x=0}^{x=\\infty} \\int_{y=0}^{y=x} 2\\cdot x\\cdot y \\mathrm{e}^{-x}\\cdot\\mathrm{e}^{-y}\\, dy\\, dx\\\\\n& =\\displaystyle  2\\cdot\\int_{x=0}^{x=\\infty} x \\cdot\\mathrm{e}^{-x} \\cdot\\int_{y=0}^{y=x}  y \\cdot\\mathrm{e}^{-y}\\, dy\\, dx\\\\\n& =\\displaystyle  2\\int_{x=0}^{x=\\infty}x\\cdot \\mathrm{e}^{-x} \\left[-\\mathrm{e}^{-y}\\cdot (y+1)\\right]_{y=0}^{y=x}\\, dx\\\\\n& =\\displaystyle  2\\cdot\\int_{x=0}^{x=\\infty}x \\cdot\\mathrm{e}^{-x} \\cdot\\left(1-\\mathrm{e}^{-x}(x+1)\\right)\\, dx \\\\\n&= \\displaystyle 2\\cdot\\int_{x=0}^{x=\\infty}x\\cdot\\left( \\mathrm{e}^{-x}-\\mathrm{e}^{-2x}\\right)-x^2\\cdot\\mathrm{e}^{-2x}\\, dx \\\\\n& =\\displaystyle  2\\cdot\\left[-\\mathrm{e}^{-x}(x+1)+\\frac{1}{4}\\cdot\\mathrm{e}^{-2 x}(1+2x)+\\frac{1}{4} \\cdot\\mathrm{e}^{-2 x} \\left(2 x^2+2\n   x+1\\right)\\right]_{x=0}^{x=\\infty} \\\\\n   &= \\displaystyle 2\\cdot \\left(1-\\frac{1}{4}-\\frac{1}{4}\\right)=1.\n\\end{array}\n\\] En el último cálculo hemos usado integración por partes para integrar \\(\\int x\\mathrm{e}^{-x}\\,dx\\), \\(\\int x\\mathrm{e}^{-2x}\\,dx\\) y \\(\\int x^2\\mathrm{e}^{-2x}\\, dx\\).\n\n\n\nEjercicio\nHallar \\(E(X+Y)\\) para el ejemplo anterior.\n\n\n\n\n6.6.2 Valor esperado de una función de dos variables aleatorias independientes\nEl siguiente resultado nos simplifica el cálculo del valor esperado de una función de dos variables aleatorias en el caso en que sean independientes:\nProposición: cálculo del valor esperado de una función de dos variables aleatorias en el caso de independencia.  Sea \\((X,Y)\\) una variable aleatoria bidimensionaltal que \\(X\\) e \\(Y\\) son independientes. Sea \\(Z=g(X,Y)\\) una variable aleatoria unidimensional función de \\(X\\) e \\(Y\\) en la que podemos “separar” las variables \\(x\\) e \\(y\\) en la función \\(g\\). Es decir, existen dos funciones \\(g_x\\) y \\(g_y\\) tal que \\(g(x,y)=g_x(x)\\cdot g_y(y)\\) para todo valor \\(x,y\\in\\mathbb{R}\\). En este caso, el valor esperado de \\(Z\\) se puede calcular como: \\[\nE(Z)=E(g(X,Y))=E_X(g_x(X))\\cdot E_Y(g_y(Y)).\n\\]\nEs decir, el cálculo de \\(E(g(X,Y))\\) que es una suma doble en el caso de que \\((X,Y)\\) sea discreta o una integral doble en el caso en que \\((X,Y)\\) sea continua se transforma en el producto de dos sumas simples (caso discreto) o el producto de dos integrales simples (caso continuo):\n\\[\n\\begin{array}{rl}\nE(Z) & =E(g(X,Y))=\\displaystyle\\left(\\sum_{x_i} g_x(x_i)\\cdot P_X(x_i)\\right)\\cdot \\left(\\sum_{y_j} g_y(y_j)\\cdot P_Y(y_j)\\right),\\\\ &\\ \\quad \\mbox{caso discreto},\\\\\nE(Z) & =E(g(X,Y))=\\displaystyle\\left(\\int_{-\\infty}^\\infty g_x(x)\\cdot f_X(x)\\, dx\\right)\\cdot \\left(\\int_{-\\infty}^\\infty g_y(y)\\cdot f_Y(y)\\right), \\\\  &\\ \\quad \\mbox{caso continuo}.\n\\end{array}\n\\]\nUn caso particular de aplicación de la proposición anterior es el calculo de \\(E(X\\cdot Y)\\) cuando \\(X\\) e \\(Y\\) son independientes. En este caso \\(g(x,y)=x\\cdot y\\), \\(g_x(x)=x\\), y \\(g_y(y)=y\\).\nPodemos escribir, por tanto: \\[\nE(X\\cdot Y)=E_X(X)\\cdot E_Y(Y).\n\\]\n\n6.6.2.1 Ejemplos\n\nEjemplo: lanzar dos veces un dado (continuación)\nRecordemos el experimento aleatorio que consiste en lanzar un dado dos veces. Sea \\(X\\) el resultado del primer lanzamiento e \\(Y\\), el resultado del segundo lanzamiento.\nHemos visto que \\(X\\) e \\(Y\\) son independientes.\nLas marginales de \\(X\\) e \\(Y\\) recordemos que son las siguientes:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(X\\) o \\(Y\\)\n1\n2\n3\n4\n5\n6\n\n\n\n\n\\(P_X(i)\\) o \\(P_Y(i)\\)\n\\(\\frac{1}{6}\\)\n\\(\\frac{1}{6}\\)\n\\(\\frac{1}{6}\\)\n\\(\\frac{1}{6}\\)\n\\(\\frac{1}{6}\\)\n\\(\\frac{1}{6}\\)\n\n\n\n\nCalculemos \\(E(X\\cdot Y)\\) usando la proposición anterior:\n\n\\[\nE(X\\cdot Y)=\\displaystyle E_X(X)\\cdot E_Y(Y)=\\left(\\sum_{i=1}^6 i\\cdot \\frac{1}{6}\\right)\\cdot \\left(\\sum_{i=1}^6 i\\cdot \\frac{1}{6}\\right)=\\left(\\frac{21}{6}\\right)^2 = 12.25.\n\\] Dejamos como ejercicio el cálculo de \\(E(X\\cdot Y)\\) usando la función de probabilidad conjunta \\(P_{XY}\\) y comprobar que da el mismo resultado.\n\n\n\nEjemplo: densidad uniforme cuadrado unidad\nRecordemos la variable aleatoria bidimensional continua con función de densidad conjunta: \\[\nf_{XY}(x,y)=\\displaystyle \\begin{cases}\n1, & \\mbox{ si }0\\leq x\\leq 1,\\ 0\\leq y\\leq 1, \\\\\n0, & \\mbox{en caso contrario.}\n\\end{cases}\n\\] donde vimos que \\(X\\) e \\(Y\\) eran independientes y de distribución uniforme en el intervalo \\([0,1]\\).\nCalculemos \\(E(X\\cdot Y)\\) usando la proposición:\n\n\\[\n\\begin{array}{rl}\nE(X\\cdot Y)= & \\displaystyle E_X(X)\\cdot E_Y(Y)=\\int_0^1 x\\cdot 1\\, dx\\cdot \\int_0^1 y\\cdot 1\\, dy\\\\\n& =\\left[\\frac{x^2}{2}\\right]_{x=0}^{x=1}\\cdot \\left[\\frac{y^2}{2}\\right]_{y=0}^{y=1}=\\frac{1}{2}\\cdot \\frac{1}{2}=\\frac{1}{4}.\n\\end{array}\n\\] Dejamos como ejercicio el cálculo de \\(E(X\\cdot Y)\\) usando la función de densidad conjunta \\(f_{XY}\\) y comprobar que da el mismo resultado.\n\n\n\n\n\n6.6.3 Momentos conjuntos\nA continuación vamos a definir el momento de orden \\((k,l)\\) para una variable aleatoria bidimensional \\((X,Y)\\) para intentar obtener información de su comportamiento conjunto:\nDefinición de momento conjunto.  Sean \\((X,Y)\\) una variable aleatoria bidimensional con función de probabilidad conjunta \\(P_{XY}\\) en el caso discreto y función de densidad conjunta \\(f_{XY}\\) en el caso continuo. Dados \\(k\\) y \\(l\\) números enteros positivos, definimos el momento conjunto de orden \\((k,l)\\) para la variable \\((X,Y)\\) como: \\[\nE\\left(X^k Y^l\\right)=\\begin{cases}\n\\displaystyle \\sum_{x_i}\\sum_{y_j} x_i^k y_j^l P_{XY}(x_i,y_j), & \\mbox{ caso discreto,} \\\\\\displaystyle\n\\int_{-\\infty}^\\infty\\int_{-\\infty}^\\infty x^k y^l f_{XY}(x,y)\\, dx\\, dy. & \\mbox{ caso continuo.}\n\\end{cases}\n\\]\nObservación. Si consideramos \\(l=0\\), los momentos conjuntos de orden \\((k,0)\\) coinciden con los momentos de orden \\(k\\) de la variable aleatoria \\(X\\).\nDe la misma forma, considerando \\(k=0\\), los momentos conjuntos de orden \\((0,l)\\) coinciden con los momentos de orden \\(l\\) de la variable aleatoria \\(Y\\).\nPara \\(l=1\\) y \\(k=1\\) obtenemos el momento de orden \\((1,1)\\) ya visto anteriormente: \\(E(X\\cdot Y)\\), denominado correlación entre las variables \\(X\\) e \\(Y\\). Si dicha correlación es cero, \\(E(X\\cdot Y)=0\\), se dice que las variables \\(X\\) e \\(Y\\) son ortogonales.\n\n\n6.6.4 Momentos conjuntos centrados en las medias\nA continuación definamos los momentos conjuntos centrados en las medias:\nDefinición de momento conjunto.  Sean \\((X,Y)\\) una variable aleatoria bidimensional con función de probabilidad conjunta \\(P_{XY}\\) en el caso discreto y función de densidad conjunta \\(f_{XY}\\) en el caso continuo. Sean \\(\\mu_X=E(X)\\) y \\(\\mu_Y=E(Y)\\) los valores esperados de las variables \\(X\\) e \\(Y\\), respectivamente. Dados \\(k\\) y \\(l\\) números enteros positivos, definimos el momento conjunto de orden \\((k,l)\\) centrado en las medias para la variable \\((X,Y)\\) como: \\[\nE\\left((X-\\mu_X)^k\\cdot  (Y-\\mu_Y)^l\\right)=\\begin{cases}\n\\sum_{x_i}\\sum_{y_j} (x_i-\\mu_X)^k \\cdot (y_j-\\mu_Y)^l\\cdot  P_{XY}(x_i,y_j), & \\\\\\ \\qquad \\mbox{ caso discreto,}& \\\\\n\\int_{-\\infty}^\\infty\\int_{-\\infty}^\\infty (x-\\mu_X)^k\\cdot  (y-\\mu_Y)^l\\cdot  f_{XY}(x,y)\\, dx\\, dy. & \\\\ \\ \\qquad\\mbox{ caso continuo.} &\n\\end{cases}\n\\]\n\n\n6.6.5 Covariancia entre las variables\nEl momento conjunto centrado en las medias para \\(k=1\\) y \\(l=1\\) se denomina covariancia entre las variables \\(X\\) e \\(Y\\): \\[\n\\mathrm{Cov}(X,Y)=E((X-\\mu_X)\\cdot (Y-\\mu_Y)).\n\\] La covariancia puede calcularse a partir de la correlación entre las variables: \\[\n\\mathrm{Cov}(X,Y)=E((X-\\mu_X) \\cdot (Y-\\mu_Y))=E(X\\cdot Y)-\\mu_X\\cdot \\mu_Y,\n\\]\nya que, usando las propiedades de la esperanza, tenemos: \\[\n\\begin{array}{rl}\nE((X-\\mu_X)\\cdot (Y-\\mu_Y)) & =E(X\\cdot Y-\\mu_Y \\cdot X-\\mu_X \\cdot Y+\\mu_X\\cdot \\mu_Y)\\\\ & =E(X\\cdot Y)-\\mu_Y\\cdot E(X)-\\mu_X \\cdot E(Y)+\\mu_X\\cdot \\mu_Y \\\\ &  = E(X\\cdot Y)-\\mu_Y\\cdot \\mu_X-\\mu_X \\cdot \\mu_Y+\\mu_X\\cdot \\mu_Y \\\\ & = E(X\\cdot Y)-\\mu_X\\cdot \\mu_Y.\n\\end{array}\n\\]\nObservación.  Si las variables \\(X\\) e \\(Y\\) son independientes, su covarianza es nula ya que vimos que \\(E(X\\cdot Y)=\\mu_X\\cdot \\mu_y\\).\nLa covarianza es una medida de lo relacionadas están las variables \\(X\\) e \\(Y\\):\n\nSi cuando \\(X\\geq \\mu_X\\), también ocurre que \\(Y\\geq \\mu_Y\\) o viceversa, cuando \\(X\\leq \\mu_X\\), también ocurre que \\(Y\\leq \\mu_Y\\), el valor \\((X-\\mu_X)(Y-\\mu_Y)\\) será positivo y la covarianza será positiva.\nSi por el contrario, cuando \\(X\\geq \\mu_X\\), también ocurre que \\(Y\\leq \\mu_Y\\) o viceversa, cuando \\(X\\leq \\mu_X\\), también ocurre que \\(Y\\geq \\mu_Y\\), el valor \\((X-\\mu_X)(Y-\\mu_Y)\\) será negativo y la covarianza será negativa.\nEn cambio, si a veces ocurre una cosa y a veces ocurre otra, la covarianza va cambiando de signo y puede tener un valor cercano a 0.\n\n\n6.6.5.1 Propiedades de la covarianza\n\nSea \\((X,Y)\\) una variable aleatoria bidimensional. Entonces la varianza de la suma/resta se calcula usando la expresión siguiente: \\[\n\\mathrm{Var}(X\\pm Y)=\\mathrm{Var}(X)+\\mathrm{Var}(Y)\\pm 2 \\mathrm{Cov}(X,Y).\n\\]\n\n\nDemostración\nLa varianza de la suma/resta de las variables es, usando la propiedad de la varianza: \\[\n\\mathrm{Var}(X\\pm Y)=E\\left((X\\pm Y)^2\\right)-\\left(E(X\\pm Y)\\right)^2.\n\\] Desarrollando las expresiones anteriores, obtenemos: \\[\n\\begin{array}{rl}\n\\mathrm{Var}(X\\pm Y) & =E\\left(X^2+Y^2\\pm 2XY\\right)-\\left(E(X)\\pm E(Y)\\right)^2 \\\\ & =\nE(X^2)+E(Y^2)\\pm 2\\cdot E(X\\cdot Y) \\\\ &\\qquad\\qquad - \\left(E(X)^2+E(Y)^2\\pm 2\\cdot E(X)\\cdot E(Y)\\right)\n\\\\ & = E(X^2)-E(X)^2+E(Y^2)-E(Y)^2\\\\ &\\qquad\\qquad \\pm 2\\cdot (E(X\\cdot Y)-E(X)\\cdot E(Y)) \\\\ & = \\mathrm{Var}(X)+\\mathrm{Var}(Y)\\pm 2\\cdot \\mathrm{Cov}(X,Y),\n\\end{array}\n\\] tal como queríamos ver.\n\nUna consecuencia de la propiedad anterior es el resultado siguiente:\nProposición: si las variables son independientes, la varianza de la suma es la suma de varianzas.  Sea \\((X,Y)\\) una variable aleatoria bidimensional donde las variables \\(X\\) e \\(Y\\) son independientes. Entonces: \\[\n\\mathrm{Var}(X+Y)=\\mathrm{Var}(X)+\\mathrm{Var}(Y).\n\\]\n\nDemostración\nLa demostración es muy sencilla: basta aplicar la fórmula de la varianza de la suma y tener en cuenta que, como \\(X\\) e \\(Y\\) son independientes, su covarianza es cero: \\(\\mathrm{Cov}(X,Y)=0\\).\n\n\n\n\n6.6.6 Coeficiente de correlación entre las variables\nLa covarianza depende de las unidades en las que están las variables \\(X\\) e \\(Y\\) ya que si \\(a&gt;0\\) y \\(b&gt;0\\), entonces: \\[\n\\mathrm{Cov}(a\\cdot X,b\\cdot Y)=a\\cdot b\\cdot \\mathrm{Cov}(X,Y).\n\\] Por tanto, si queremos “medir” la relación que existe entre las variables \\(X\\) e \\(Y\\) tendremos que “normalizar” la covarianza definiendo el coeficiente de correlación entre las variables \\(X\\) e \\(Y\\):\nDefinición del coeficiente de correlación.  Sea \\((X,Y)\\) una variable aleatoria bidimensional. Se define el coeficiente de correlación entre las variables \\(X\\) e \\(Y\\) como: \\[\n\\rho_{XY}=\\frac{\\mathrm{Cov}(X,Y)}{\\sqrt{\\mathrm{Var}(X)}\\cdot\\sqrt{\\mathrm{Var}(Y)}}=\\frac{E(X\\cdot Y)-\\mu_X\\cdot \\mu_Y}{\\sqrt{E\\left(X^2\\right)-\\mu_X^2}\\cdot \\sqrt{E\\left(Y^2\\right)-\\mu_Y^2}}.\n\\]\nObservación.  Si las variables \\(X\\) e \\(Y\\) son independientes, su coeficiente de correlación \\(\\rho_{XY}=0\\) es nulo ya que su covarianza lo es.\nNotemos también que la correlación no tiene unidades y es invariante a cambios de escala.\nAdemás, la covarianza de las variables tipificadas \\(\\frac{X-\\mu_X}{\\sigma_X}\\) y \\(\\frac{Y-\\mu_Y}{\\sigma_Y}\\) coincide con la correlación de \\(X\\) e \\(Y\\).\nEl coeficiente de correlación es un valor normalizado ya que siempre está entre -1 y 1: \\(-1\\leq\\rho_{XY}\\leq 1\\).\n\nPara demostrar de este hecho, sean \\(\\mu_X=E(X)\\), \\(\\mu_Y=E(Y)\\), \\(\\sigma_X=\\sqrt{\\mathrm{Var}(X)}\\) y \\(\\sigma_Y=\\sqrt{\\mathrm{Var}(Y)}\\).\nConsideremos la variable \\(Z=\\left(\\frac{X-\\mu_X}{\\sigma_X}\\pm \\frac{Y-\\mu_Y}{\\sigma_Y}\\right)^2\\). Como \\(Z\\geq 0\\), tenemos que \\(E(Z)\\geq 0\\). Desarrollemos el valor de \\(E(Z)\\):\n\\[\n\\begin{array}{rl}\nE(Z) & = E\\left(\\frac{X-\\mu_X}{\\sigma_X}\\pm \\frac{Y-\\mu_Y}{\\sigma_Y}\\right)^2 = E\\left(\\left(\\frac{X-\\mu_X}{\\sigma_X}\\right)^2+\\left(\\frac{Y-\\mu_Y}{\\sigma_Y}\\right)^2\\pm 2\\left(\\frac{X-\\mu_X}{\\sigma_X}\\right)\\cdot  \\left(\\frac{Y-\\mu_Y}{\\sigma_Y}\\right)\\right) \\\\ & =\nE\\left(\\left(\\frac{X-\\mu_X}{\\sigma_X}\\right)^2\\right)+E\\left(\\left(\\frac{Y-\\mu_Y}{\\sigma_Y}\\right)^2\\right)\\pm 2\\cdot  E\\left(\\left(\\frac{X-\\mu_X}{\\sigma_X}\\right) \\cdot \\left(\\frac{Y-\\mu_Y}{\\sigma_Y}\\right)\\right) \\\\ & =\n\\frac{1}{\\sigma_X^2}E\\left(\\left(X-\\mu_X\\right)^2\\right)+\\frac{1}{\\sigma_Y^2}E\\left(\\left(Y-\\mu_Y\\right)^2\\right)\\pm \\frac{2}{\\sigma_X\\cdot \\sigma_Y}E\\left(\\left(X-\\mu_X\\right) \\left(Y-\\mu_Y\\right)\\right) \\\\ & = \\frac{1}{\\sigma_X^2}\\sigma_X^2+\n\\frac{1}{\\sigma_Y^2}\\sigma_Y^2 \\pm\\frac{2}{\\sigma_X\\cdot \\sigma_Y} \\mathrm{Cov}(X,Y) = 1+1\\pm 2\\cdot \\frac{\\mathrm{Cov}(X,Y)}{\\sigma_X\\sigma_Y}=2\\cdot (1\\pm\\rho_{XY})\n\\end{array}\n\\]\nAhora, como \\(E(Z)\\geq 0\\), tenemos que \\(1\\pm \\rho_{XY}\\geq 0\\), lo que significa que, por un lado \\(1+\\rho_{XY}\\geq 0\\) y, por otro, \\(1-\\rho_{XY}\\geq 0\\). De la primera inecuación, deducimos que \\(\\rho_{XY}\\geq -1\\) y de la segunda, \\(\\rho_{XY}\\leq 1\\).\nEn resumen, \\(-1\\leq\\rho_{XY}\\leq 1\\), tal como queríamos ver.\n\n\n6.6.6.1 Ejemplos\n\nEjemplo: otra densidad (continuación)\nHallemos el coeficiente de correlación para el ejemplo de la variable aleatoria bidimensional continua con función de densidad conjunta:\n\\[\nf_{XY}(x,y)=\\begin{cases}\n2\\cdot  \\mathrm{e}^{-x}\\cdot \\mathrm{e}^{-y}, & 0\\leq y\\leq x &lt; \\infty,\\\\\n0, & \\mbox{ en caso contrario,}\n\\end{cases}\n\\] Recordemos los cálculos realizados anteriormente:\n\n\n\\(E(X\\cdot Y)=1.\\)\n\\(f_X(x)=2\\cdot \\left(\\mathrm{e}^{-x}-\\mathrm{e}^{-2x}\\right)\\), si \\(x\\geq 0\\). Su esperanza es:\n\n\\[\n\\begin{array}{rl}\nE(X)&=\\int_0^\\infty x\\cdot 2\\left(\\mathrm{e}^{-x}-\\mathrm{e}^{-2x}\\right)\\, dx=2 \\left[\\frac{1}{4} \\mathrm{e}^{-2 x} (2 x+1)-\\mathrm{e}^{-x}(x+1)\\right]_0^\\infty \\\\\n& = 2\\left(1-\\frac{1}{4}\\right)=\\frac{3}{2}.\n\\end{array}\n\\]\nCalculemos a continuación su varianza: \\(\\mathrm{Var}(X)=E\\left(X^2\\right)-\\mu_X^2\\). El valor de \\(E\\left(X^2\\right)\\) es: \\[\n\\begin{array}{rl}\nE\\left(X^2\\right) & =\\displaystyle \\int_0^\\infty x^2 \\cdot 2\\cdot \\left(\\mathrm{e}^{-x}-\\mathrm{e}^{-2\\cdot x}\\right)\\, dx\\\\\n&=\\displaystyle 2 \\cdot  \\left[\\frac{1}{4} \\cdot \\mathrm{e}^{-2 \\cdot x} \\cdot  (2\\cdot x^2+2\\cdot x+1)- \\mathrm{e}^{-x} \\cdot (x^2+2\\cdot x+2)\\right]_0^\\infty \\\\ & = 2\\cdot \\left(2-\\frac{1}{4}\\right)=\\frac{7}{2}.\n\\end{array}\n\\]\nEl valor de la varianza de \\(X\\) es: \\(\\mathrm{Var}(X)=\\frac{7}{2}-\\left(\\frac{3}{2}\\right)^2 = \\frac{5}{4}.\\)\n\nLa variable \\(Y\\) era exponencial de parámetro \\(\\lambda =2\\). Por tanto, \\(E(Y)=\\frac{1}{2}\\), \\(\\mathrm{Var}(Y)=\\frac{1}{4}\\).\n\nEl coeficiente de correlación entre las variables \\(X\\) e \\(Y\\) es: \\[\n\\rho_{XY}=\\frac{E(X\\cdot Y)-\\mu_X\\cdot \\mu_Y}{\\sqrt{\\mathrm{Var}(X)}\\cdot\\sqrt{\\mathrm{Var}(Y)}}=\\frac{1-\\frac{3}{2}\\cdot \\frac{1}{2}}{\\sqrt{\\frac{5}{4}}\\cdot\\sqrt{\\frac{1}{4}}}=\\frac{\\sqrt{5}}{5}\\approx 0.447.\n\\] Vemos que la correlación entre las variables \\(X\\) e \\(Y\\) es positiva pero no demasiado ya que su valor no está cercano a 1.\n\n\n\nEjemplo: normal bidimensional\nRecordemos que la función de densidad de la variable aleatoria normal bidimensional es: \\(f_{XY}(x,y)=\\frac{1}{2\\cdot \\pi\\cdot \\sqrt{1-\\rho^2}}\\mathrm{e}^{-\\frac{(x^2-2\\cdot \\rho \\cdot x\\cdot y+y^2)}{2\\cdot (1-\\rho^2)}},\\ -\\infty &lt;x,y&lt;\\infty.\\)\nLas variables aleatorias marginales son normales estándar o \\(N(0,1)\\).\nHallemos el coeficiente de correlación \\(\\rho_{XY}\\) en este caso.\n\nCalculemos \\(E(X\\cdot Y)\\):\n\\[\n\\begin{array}{rl}\nE(X\\cdot Y) & = \\displaystyle\\int_{-\\infty}^\\infty x y \\frac{1}{2\\cdot\\cdot\\pi\\cdot\\sqrt{1-\\rho^2}}\\mathrm{e}^{-\\frac{(x^2-2\\rho\\cdot x\\cdot y+y^2)}{2(1-\\rho^2)}}\\, dy\\, dx \\\\\n& = \\displaystyle\\frac{1}{2\\pi\\sqrt{1-\\rho^2}}\\int_{x=-\\infty}^{x=\\infty}x\\cdot  \\mathrm{e}^{-\\frac{x^2}{2\\cdot(1-\\rho^2)}}\\int_{y=-\\infty}^{y=\\infty}y \\mathrm{e}^{-\\frac{(-2\\cdot\\rho\\cdot  x\\cdot y+y^2)}{2\\cdot(1-\\rho^2)}}\\, dy\\, dx \\\\ & = \\displaystyle\\frac{1}{2\\cdot\\pi\\cdot\\sqrt{1-\\rho^2}}\\int_{x=-\\infty}^{x=\\infty}x  \\mathrm{e}^{-\\frac{x^2}\\cdot{2(1-\\rho^2)}}  \\mathrm{e}^{\\frac{\\rho^2\\cdot x^2}{2\\cdot(1-\\rho^2)}} \\int_{y=-\\infty}^{y=\\infty}y \\mathrm{e}^{-\\frac{(y-\\rho y)^2}{2\\cdot(1-\\rho^2)}}\\, dy\\, dx,\\\\ &\\ \\qquad\\mbox{ cambio de variable en la segunda integral } z=\\frac{y-\\rho x}{\\sqrt{1-\\rho^2}},\\\\\n& = \\displaystyle\\frac{1}{2\\pi\\sqrt{1-\\rho^2}}\\int_{x=-\\infty}^{x=\\infty}x  \\mathrm{e}^{-\\frac{x^2}{2}}  \\int_{z=-\\infty}^{z=\\infty} \\left(z \\sqrt{1-\\rho^2}+\\rho x\\right) \\sqrt{1-\\rho^2}\\mathrm{e}^{-\\frac{z^2}{2}}\\, dz\\, \\\\\n& = \\displaystyle\\frac{1}{2\\pi} \\int_{x=-\\infty}^{x=\\infty}x  \\mathrm{e}^{-\\frac{x^2}{2}}\\left(\\sqrt{1-\\rho^2}\\int_{z=-\\infty}^{z=\\infty} z\\cdot \\mathrm{e}^{-\\frac{z^2}{2}}\\, dz +\\rho\\cdot x \\int_{z=-\\infty}^{z=\\infty}\\mathrm{e}^{-\\frac{z^2}{2}}\\, dz \\right)\\, dx\n\\end{array}\n\\]\nAhora, usando que el valor esperado de una variable \\(N(0,1)\\) es cero tenemos que: \\(\\int_{z=-\\infty}^{z=\\infty} z \\mathrm{e}^{-\\frac{z^2}{2}}\\, dz =0,\\) y usando que la integral de la función de densidad de la \\(N(0,1)\\) (\\(\\frac{1}{\\sqrt{2\\pi}}\\mathrm{e}^{-\\frac{z^2}{2}}\\)) sobre todo \\(\\mathbb{R}\\) es 1, tenemos que: \\(\\int_{z=-\\infty}^{z=\\infty} \\mathrm{e}^{-\\frac{z^2}{2}}\\, dz =\\sqrt{2\\pi}.\\)\nPor tanto, \\[\nE(X\\cdot Y)=\\frac{\\rho}{2\\pi} \\int_{x=-\\infty}^{x=\\infty} x^2  \\mathrm{e}^{-\\frac{x^2}{2}}\\sqrt{2\\pi}\\, dx=\\frac{\\rho}{\\sqrt{2\\pi}}\\int_{x=-\\infty}^{x=\\infty} x^2  \\mathrm{e}^{-\\frac{x^2}{2}}\\, dx.\n\\] Por último, usando que la varianza de la distribución \\(Z=N(0,1)\\) es 1, tenemos que \\(\\mathrm{Var}(Z)=E\\left(Z^2\\right)-E(Z)^2\\). Como \\(E(Z)=0\\), deducimos que \\(E\\left(Z^2\\right)=1\\): \\[\n\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^\\infty x^2\\mathrm{e}^{-\\frac{x^2}{2}}\\, dx=1,\\ \\Rightarrow \\int_{-\\infty}^\\infty x^2\\mathrm{e}^{-\\frac{x^2}{2}}\\, dx=\\sqrt{2\\pi}.\n\\] El valor de \\(E(X\\cdot Y)\\) es: \\[\nE(X\\cdot Y)=\\frac{\\rho}{\\sqrt{2\\pi}}\\sqrt{2\\pi}=\\rho.\n\\]\nLa correlación entre las variables \\(X\\) e \\(Y\\) es precisamente \\(\\rho\\).\nAhora, usando que \\(\\mu_X=\\mu_Y=0\\) y \\(\\sigma_X=\\sigma_Y=1\\) ya que recordemos que las marginales son \\(N(0,1)\\), el coeficiente de correlación entre las variables \\(X\\) e \\(Y\\) es: \\[\n\\rho_{XY}=\\frac{E(X\\cdot Y)-\\mu_X\\cdot \\mu_Y}{\\sqrt{\\mathrm{Var}(X)}\\cdot\\sqrt{\\mathrm{Var}(Y)}}=\\frac{\\rho-0\\cdot 0}{1\\cdot 1}=\\rho.\n\\] Por tanto, \\(\\rho\\) es el coeficiente de correlación entre las variables \\(X\\) e \\(Y\\) y mide lo correlacionadas que están dichas variables.\n\n\n\n\n\n6.6.7 Incorrelación e independencia\nHemos visto que si dos variables \\(X\\) e \\(Y\\) son independientes, entonces son incorreladas, es decir, la covarianza es 0 (\\(E(X\\cdot Y)=E(X)\\cdot E(Y)\\)).\nEl recíproco, sin embargo, es falso. Veamos un ejemplo de variables incorreladas que no son independientes.\n\nEjemplo de variables aleatorias incorreladas pero no independientes\nConsideremos la variable aleatoria bidimensional continua con función de densidad: \\[\nf_{XY}(x,y)=\\begin{cases}\n\\frac{3}{8}(x^2+y^2), & \\mbox{si }(x,y)\\in [-1,1]\\times [-1,1],\\\\\n0, & \\mbox{en caso contrario.}\n\\end{cases}\n\\]\nDejamos como ejercicio comprobar que es una función de densidad. Es decir, que es positiva y que la integral sobre todo el plano vale 1.\n\nCalculemos las densidades marginales:\n\\[\n\\begin{array}{rl}\nf_X(x) & = \\int_{-1}^{1} \\frac{3}{8}\\cdot (x^2+y^2)\\, dy = \\frac{3}{8}\\cdot\\left[x^2\\cdot y+\\frac{y^3}{3}\\right]_{-1}^1 =\\frac{3}{8}\\cdot\\left(2 \\cdot x^2+\\frac{2}{3}\\right)=\\frac{3}{4}\n\\cdot x^2+\\frac{1}{4}, \\\\\nf_Y(y) & = \\int_{-1}^{1} \\frac{3}{8}\\cdot(x^2+y^2)\\, dx = \\frac{3}{8}\\cdot\\left[\\frac{x^3}{3}+y^2 x\\right]_{-1}^1 =\\frac{3}{8}\\cdot\\left(\\frac{2}{3}+2 y^2+\\right)=\\frac{3}{4}\\cdot y^2+\\frac{1}{4}.\n\\end{array}\n\\]\nLos valores esperados de cada variable \\(X\\) e \\(Y\\) son:\n\\[\n\\begin{array}{rl}\nE(X) & =\\int_{-1}^1 x \\cdot\\left(\\frac{3}{4} \\cdot x^2+\\frac{1}{4}\\right)\\, dx =0, \\mbox{al integrar una función impar,}\\\\\nE(Y) & =\\int_{-1}^1 x \\left(\\frac{3}{4}\\cdot y^2+\\frac{1}{4}\\right)\\, dx =0, \\mbox{al integrar una función impar.}\n\\end{array}\n\\]\nEl valor de la correlación entre \\(X\\) e \\(Y\\) es:\n\\[\n\\begin{array}{rl}\nE(X\\cdot Y) & =\\int_{-1}^1\\int_{-1}^1 x \\cdot y \\cdot \\frac{3}{8}\\cdot (x^2+y^2)\\, dy\\, dx\\\\ & =\\frac{3}{8}\\cdot\\left(\\int_{-1}^1\\int_{-1}^1 x^3 \\cdot y\\, dy \\, dx+\\int_{-1}^1\\int_{-1}^1 x\\cdot y^3\\, dy \\, dx\\right) \\\\ & = \\frac{3}{8} \\left(\\int_{x=-1}^{x=1}x^3 \\left[\\frac{y^2}{2}\\right]_{y=-1}^{y=1}\\, dx + \\int_{y=-1}^{y=1}y^3 \\left[\\frac{x^2}{2}\\right]_{x=-1}^{x=1}\\right)=0.\n\\end{array}\n\\]\nEl coeficiente de correlación entre \\(X\\) e \\(Y\\) es: \\(\\rho_{XY}=E(X\\cdot Y)-E(X)\\cdot E(Y)=0-0\\cdot 0=0\\). Por tanto, son incorreladas.\nEn cambio no son independientes ya que claramente si \\((x,y)\\in [-1,1]\\times [-1,1]\\),\n\\[\nf_{XY}(x,y)=\\frac{3}{8}(x^2+y^2) \\neq f_X(x)\\cdot f_Y(y)=\\left(\\frac{3}{4} x^2+\\frac{1}{4}\\right)\\cdot \\left(\\frac{3}{4} y^2+\\frac{1}{4}\\right).\n\\]"
  },
  {
    "objectID": "5.html#variables-aleatorias-condicionales-y-valor-esperado-condicional",
    "href": "5.html#variables-aleatorias-condicionales-y-valor-esperado-condicional",
    "title": "6  Vectores aleatorios bidimensionales",
    "section": "6.7 Variables aleatorias condicionales y valor esperado condicional",
    "text": "6.7 Variables aleatorias condicionales y valor esperado condicional\nMuchas variables aleatorias bidimensionales de interés práctico no son independientes.\nPor ejemplo, la salida \\(Y\\) de un canal de comunicación debe depender de la entrada \\(X\\) para transmitir información.\nEn esta sección vamos a introducir variables aleatorias \\(Y\\) cuya distribución depende de otras \\(X\\). Dichas variables se denominan variables aleatorias condicionales.\nTambién nos interesa el valor esperado de la variable condicional \\(Y\\) suponiendo que conocemos \\(X=x\\).\n\n6.7.1 Variables aleatorias condicionales discretas\nSea \\((X,Y)\\) una variable aleatoria bidimensional. Sea \\(B\\) un subconjunto de los números reales \\(\\mathbb{R}\\). Recordemos que la probabilidad condicional del suceso \\(\\{Y\\in B\\}\\) suponiendo que \\(X=x\\) se definía de la forma siguiente: \\[\nP(Y\\in B|X=x)=\\frac{P(Y\\in B,\\ X=x)}{P(X=x)}, \\mbox{ siempre que }P(X=x)&gt;0.\n\\]\nLa definición anterior motiva la definición siguiente de variable aleatoria condicional discreta:\nDefinición de variable aleatoria condicional discreta.  Sea \\((X,Y)\\) una variable aleatoria bidimensional discreta con conjunto de valores \\((X,Y)(\\Omega)=\\{(x_i,y_j)\\ i=1,2,\\ldots, j=1,2,\\ldots\\}\\) y función de probabilidad conjunta \\(P_{XY}\\). Sean \\(x_i\\) un valor de \\(X(\\Omega)\\) con \\(P(X=x_i)&gt;0\\). Entonces definimos la función de probabilidad de la variable aleatoria condicional discreta \\(Y|X=x_i\\) como: \\[\nP_{Y|X=x_i}(y_j)=P(Y=y_j|X=x_i)=\\frac{P(X=x_i,\\ Y=y_j)}{P(X=x_i)}=\\frac{P_{XY}(x_i,y_j)}{P_X(x_i)}.\n\\]\n¡Observación.  La función de probabilidad de la variable aleatoria condicional \\(Y|X=x_i\\) depende únicamente de la función de probabilidad conjunta de la variable aleatoria bidimensional \\((X,Y)\\).\nObservación.  Al ser \\(Y|X=x_i\\) una variable aleatoria unidimensional, su función de probabilidad tiene que verificar que la suma de todos sus valores tiene que dar 1. Es decir: \\[\n\\sum_{y_j} P(Y=y_j|X=x_i)=1.\n\\] Veámoslo:\n\n\\[\n\\begin{array}{rl}\n\\sum_{y_j} P(Y=y_j|X=x_i) &=\\displaystyle \\sum_{y_j} \\frac{P_{XY}(x_i,y_j)}{P_X(x_i)}=\\frac{1}{P_X(x_i)}\\sum_{y_j} P_{XY}(x_i,y_j) \\\\\n&=\\frac{1}{P_X(x_i)}\\cdot P_X(x_i)=1.\n\\end{array}\n\\]\n\nObservación.  Si \\(X\\) e \\(Y\\) son independientes, la distribución de \\(Y|X=x_iY\\) es la misma que la de \\(Y\\), es decir, la variable aleatoria condicional \\(Y|X=x_i\\) coincide con \\(Y\\). Es decir, condicionar con \\(X=x_i\\) no tiene ningún efecto sobre \\(Y\\).\n\nEfectivamente, veamos que \\(P_{Y|X=x_i}(y_j)=P_Y(y_j)\\) para todo valor \\(y_j\\) de \\(Y(\\Omega)\\):\n\\[\n\\begin{array}{rl}\nP_{Y|X=x_i}(y_j) &=\\frac{P_{XY}(x_i,y_j)}{P_X(x_i)} \\stackrel{\\mbox{Por ser independientes}}{=}\\frac{P_Y(y_j)\\cdot P_X(x_i)}{P_X(x_i)}\\\\\n& =P_Y(y_j).\n\\end{array}\n\\]\n\nObservación.  La definición de la función de probabilidad de la variable aleatoria condicional \\(X|Y=y_j\\) se definiría de forma similar:\n\\[\nP_{X|Y=y_j}(x_i)=P(X=x_i|Y=y_j)=\\frac{P(X=x_i,\\ Y=y_j)}{P(Y=y_j)}=\\frac{P_{XY}(x_i,y_j)}{P_Y(y_j)},\n\\] para todo \\(x_i\\in X(\\Omega)\\).\nObservación. Si tenemos la tabla de la función de probabilidad conjunta \\(P_{XY}\\), para hallar la función de distribución de la variable \\(Y|X=x_i\\) es equivalente a considerar la fila del valor \\(x_i\\) a la tabla y dividir todos los valores de la fila por la suma de los valores en dicha fila:\n\n\n\n\n\n\n\n\n\n\n\n\\(Y|X=x_i\\)\n\\(y_1\\)\n\\(y_2\\)\n\\(\\ldots\\)\n\\(y_N\\)\n\n\n\n\n\\(P_{Y|X=x_i}\\)\n\\(\\frac{P_{XY}(x_i,y_1)}{P_X(x_i)}\\)\n\\(\\frac{P_{XY}(x_i,y_2)}{P_X(x_i)}\\)\n\\(\\ldots\\)\n\\(\\frac{P_{XY}(x_i,y_N)}{P_X(x_i)}\\)\n\n\n\n\nObservación. De la misma manera, si tenemos la tabla de la función de probabilidad conjunta \\(P_{XY}\\), para hallar la función de distribución de la variable \\(X|Y=y_j\\) es equivalente a considerar la columna del valor \\(y=y_j\\) a la tabla y dividir todos los valores de la columna por la suma de los valores en dicha columna:\n\n\n\n\n\\(X|Y=y_j\\)\n\\(P_{X|Y=y_j}\\)\n\n\n\n\n\\(x_1\\)\n\\(\\frac{P_{XY}(x_1,y_j)}{P_Y(y_j)}\\)\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n\\(x_M\\)\n\\(\\frac{P_{XY}(x_M,y_j)}{P_Y(y_j)}\\)\n\n\n\n\n\n6.7.1.1 Ejemplos\n\nEjemplo de la suma y el producto de los resultados de dos lanzamientos de un dado\nVamos a hallar la variable aleatoria condicional \\(S|P=12\\).\nTenemos calculada la tabla de la función de probabilidad conjunta \\(P_{SP}\\).\nSi \\(P=12\\), los únicos valores \\(x_i\\) de \\(S(\\Omega)\\) para los que se verifica \\(P_{SP}(x_i,12)\\neq 0\\) son 7 y 8.\nAdemás si calculamos \\(P_P(12)\\), obtenemos \\(P(P=12)=\\frac{4}{36}\\) ya que hay 4 casos en que el producto da 12: \\((3,4), (4,3), (2,6)\\) y \\((6,2)\\).\nPor tanto, la tabla de la función de probabilidad condicional de la variable \\(S|P=12\\) es:\n\n\n\n\n\\(S|P=12\\)\n\\(P_{S|P=12}\\)\n\n\n\n\n\n\\(7\\)\n\\(\\frac{\\frac{2}{36}}{\\frac{4}{36}}=\\frac{1}{2}\\)\n\n\n\n\\(8\\)\n\\(\\frac{\\frac{2}{36}}{\\frac{4}{36}}=\\frac{1}{2}\\)\n\n\n\n\n\n\n\nEjemplo de la suma y el producto de los resultados de dos lanzamientos de un dado\nVamos a hallar la variable aleatoria condicional \\(P|S=8\\).\n\nSi \\(S=8\\), los únicos valores \\(y_j\\) de \\(P(\\Omega)\\) para los que se verifica \\(P_{SP}(8,y_j)\\neq 0\\) son 12 y 15 y 16.\nEl valor de \\(P_S(8)\\) recordemos que valía: \\(P_S(8)=\\frac{5}{36}\\).\nPor tanto, la tabla de la función de probabilidad condicional de la variable \\(P|S=8\\) es:\n\n\n\n\n\n\n\n\n\n\n\\(P|S=8\\)\n\\(12\\)\n\\(15\\)\n\\(16\\)\n\n\n\n\n\\(P_{P|S=8}\\)\n\\(\\frac{\\frac{2}{36}}{\\frac{5}{36}}=\\frac{2}{5}\\)\n\\(\\frac{\\frac{2}{36}}{\\frac{5}{36}}=\\frac{2}{5}\\)\n\\(\\frac{\\frac{1}{36}}{\\frac{5}{36}}=\\frac{1}{5}\\)\n\n\n\n\nPara hallar la variable aleatoria condicional \\(S|P=12\\) hemos de condicionar por la columna \\(P=12\\) en la tabla de la función de probabilidad conjunta:\n\nprob.cond.p12=tabla.func.prob.conjunta[,valores.producto==12]/\n  sum(tabla.func.prob.conjunta[,valores.producto==12])\nprob.cond.p12\n\n  2   3   4   5   6   7   8   9  10  11  12 \n0.0 0.0 0.0 0.0 0.0 0.5 0.5 0.0 0.0 0.0 0.0 \n\n\nEl problema es que aparecen valores con función de probabilidad marginal nulos. Para eliminarlos hacemos lo siguiente:\n\nprob.cond.p12.buena = prob.cond.p12[prob.cond.p12!=0]\nprob.cond.p12.buena\n\n  7   8 \n0.5 0.5 \n\n\nPara hallar la función de probabilidad marginal \\(P|S=8\\), haríamos lo siguiente:\n\nprob.cond.s8=tabla.func.prob.conjunta[valores.suma==8,]/\n  sum(tabla.func.prob.conjunta[valores.suma==8,])\n(prob.cond.s8.buena = prob.cond.s8[prob.cond.s8!=0])\n\n 12  15  16 \n0.4 0.4 0.2 \n\n\n\n\n\n\n\n6.7.2 Variables aleatorias condicionales continuas\nLa definición en el caso continua se hace cambiando la función de probabilidad por la función de densidad:\nDefinición de variable aleatoria condicional discreta.  Sea \\((X,Y)\\) una variable aleatoria bidimensional continua con función de densidad conjunta \\(f_{XY}\\). Sean \\(x\\in\\mathbb{R}\\) con \\(f_X(x)&gt;0\\). Entonces definimos la función de densidad de la variable aleatoria condicional continua \\(Y|X=x\\) como: \\[\nf_{Y|X=x}(y)=\\frac{f_{XY}(x,y)}{f_X(x)}.\n\\]\nObservación.  La función de densidad de la variable aleatoria condicional continua\\(Y|X\\) depende únicamente de la función de densidad conjunta de la variable aleatoria bidimensional \\((X,Y)\\).\nObservación.  Al ser \\(Y|X=x\\) una variable aleatoria unidimensional, su función de densidad tiene que verificar que la integral de dicha función sobre todo \\(\\mathbb{R}\\) tiene que ser 1. Es decir:\n\\[\n\\int_{-\\infty}^\\infty f_{Y|X=x}(y)\\, dy=1.\n\\]\nVeámoslo:\n\n\\[\n\\begin{array}{rl}\n\\int_{-\\infty}^\\infty f_{Y|X=x}(y)\\, dy & =\\int_{-\\infty}^\\infty \\frac{f_{XY}(x,y)}{f_X(x)}\\, dy=\\frac{1}{f_X(x)}\\int_{-\\infty}^\\infty f_{XY}(x,y)\\, dy\\\\\n& = \\frac{1}{f_X(x)}\\cdot f_X(x) =1.\n\\end{array}\n\\]\n\nObservación.  Si \\(X\\) e \\(Y\\) son independientes, \\(Y|X=x =Y\\), es decir, la variable aleatoria condicional \\(Y|X=x\\) coincide con \\(Y\\). Es decir, condicionar con \\(X=x\\) no tiene ningún efecto sobre \\(Y\\).\n\nEfectivamente, veamos que \\(f_{Y|X=x}(y)=f_Y(y)\\) para todo valor \\(y\\in\\mathbb{R}.\\)\n\\[\nf_{Y|X=x}(y) =\\displaystyle \\frac{f_{XY}(x,y)}{f_X(x)} \\stackrel{\\mbox{Al ser independientes}}{=}\\frac{f_Y(y)\\cdot f_X(x)}{f_X(x)}=f_Y(y).\n\\]\n\nObservación.  La definición de la función de densidad de la variable aleatoria condicional \\(X|Y=y\\) se definiría de forma similar: \\[\nf_{X|Y=y}(x)=\\frac{f_{XY}(x,y)}{f_Y(y)},\n\\] para todo \\(x\\in\\mathbb{R}\\).\n\n6.7.2.1 Ejemplos\n\nEjemplo: otra función de densidad (continuación)\nRecordemos el ejemplo de la variable aleatoria bidimensional continua con función de densidad: \\[\nf_{XY}(x,y)=\\begin{cases}\n2 \\cdot \\mathrm{e}^{-x}\\cdot \\mathrm{e}^{-y}, & 0\\leq y\\leq x &lt; \\infty,\\\\\n0, & \\mbox{ en caso contrario,}\n\\end{cases}\n\\] Dado un valor \\(x_0\\geq 0\\) cualquiera, vamos a hallar la función de densidad de la variable aleatoria condicional \\(Y|X=x_0\\).\n\nFijémonos que, fijado un valor \\(x_0\\), los valores \\(y\\) para los cuales \\(f_{XY}(x_0,y)\\neq 0\\) cumplen \\(0\\leq y\\leq x_0\\). Por tanto,\n\\[\nf_{Y|X=x_0}(y)=\\frac{f_{XY}(x_0,y)}{f_X(x_0)}=\\frac{2\\cdot \\mathrm{e}^{-x_0}\\cdot \\mathrm{e}^{-y}}{f_X(x_0)},\n\\] si \\(0\\leq y\\leq x_0\\), y \\(f_{Y|X=x_0}(y)=0\\), en caso contrario.\nRecordemos que la densidad marginal de la variable \\(X\\) era: \\(f_X(x_0)=2\\left(\\mathrm{e}^{-x_0}-\\mathrm{e}^{-2x_0}\\right)\\).\nLa función de densidad marginal de la variable \\(Y|X=x_0\\) es:\n\\[\nf_{Y|X=x_0}(y)=\\frac{2\\cdot \\mathrm{e}^{-x_0}\\cdot \\mathrm{e}^{-y}}{2\\cdot \\left(\\mathrm{e}^{-x_0}-\\mathrm{e}^{-2\\cdot x_0}\\right)}=\\frac{e^{-y}}{1-\\mathrm{e}^{-x_0}},\n\\]\nsi \\(0\\leq y\\leq x_0\\), y \\(f_{Y|X=x_0}(y)=0\\), en caso contrario.\nSea ahora \\(y_0&gt;0\\). Calculemos ahora la densidad marginal de la variable \\(X|Y=y_0\\).\nFijémonos que, fijado un valor \\(y_0\\), los valores \\(x\\) para los cuales \\(f_{XY}(x,y_0)\\neq 0\\) cumplen \\(y_0\\leq x\\leq \\infty\\). Por tanto,\n\\[\nf_{X|Y=y_0}(x)=\\frac{f_{XY}(x,y_0)}{f_Y(y_0)}=\\frac{2\\cdot \\mathrm{e}^{-x}\\cdot \\mathrm{e}^{-y_0}}{f_Y(y_0)},\n\\] si \\(y_0\\leq x\\leq \\infty\\), y \\(f_{X|Y=y_0}(x)=0\\), en caso contrario.\nRecordemos que la variable \\(Y\\) era exponencial de parámetro \\(\\lambda=2\\). Por tanto, \\(f_Y(y_0)=2\\mathrm{e}^{-2y_0}\\).\nLa función de densidad marginal de la variable \\(X|Y=y_0\\) es:\n\\[\nf_{X|Y=y_0}(x)=\\frac{2\\cdot \\mathrm{e}^{-x}\\cdot \\mathrm{e}^{-y_0}}{2\\cdot \\mathrm{e}^{-2\\cdot y_0}}=\\frac{\\mathrm{e}^{-x}}{\\mathrm{e}^{-y_0}},\n\\] si \\(y_0\\leq x\\leq \\infty\\), y \\(f_{X|Y=y_0}(x)=0\\), en caso contrario.\n\n\n\nEjemplo: normal bidimensional\nSea \\((X,Y)\\) una variable aleatoria bidimensional normal bidimensional con densidad conjunta:\n\\[\nf_{XY}(x,y)=\\frac{1}{2\\pi\\sqrt{1-\\rho^2}}\\mathrm{e}^{-\\frac{(x^2-2\\rho xy+y^2)}{2(1-\\rho^2)}},\\ -\\infty &lt;x,y&lt;\\infty.\n\\] Sea \\(x\\in\\mathbb{R}\\). Hallemos la función de densidad de la variable aleatoria condicionada \\(Y|X=x\\).\n\nRecordemos que las marginales eran \\(N(0,1)\\). Por tanto, \\(f_X(x)=\\frac{1}{\\sqrt{2\\pi}}\\mathrm{e}^{-\\frac{x^2}{2}}.\\)\nLa función de densidad de la variable condicional \\(Y|X=x\\) es:\n\\[\n\\begin{array}{rl}\nf_{Y|X=x}(y) & =  \\frac{f_{XY}(x,y)}{f_X(x)}=\\frac{\\frac{1}{2\\cdot \\pi\\cdot \\sqrt{1-\\rho^2}}\\mathrm{e}^{-\\frac{(x^2-2\\cdot \\rho \\cdot x\\cdot y+y^2)}{2(1-\\rho^2)}}}{\\frac{1}{\\sqrt{2\\pi}}\\mathrm{e}^{-\\frac{x^2}{2}}}\\\\\n& =\\frac{1}{\\sqrt{2\\cdot \\pi\\cdot  (1-\\rho^2)}}\\mathrm{e}^{-\\frac{(y-\\rho\\cdot  x)^2}{2\\cdot (1-\\rho^2)}},\\ y\\in\\mathbb{R}.\n\\end{array}\n\\] Concluimos que la variable aleatoria condicional \\(Y|X=x\\) es una normal de parámetros \\(\\mu_{Y|X=x}=\\rho x\\) y \\(\\sigma_{Y|X=x}^2 =1-\\rho^2\\).\n Observaciones\nTenemos dos observaciones con respecto al resultado obtenido:\n\nLa varianza de la variable aleatoria condicional no depende de la \\(x\\) que se ha fijado. Sólo depende del parámetro \\(\\rho\\). La \\(x\\) sólo influye en la media de dicha variable.\nEn el caso en que \\(\\rho=0\\), que significa que \\(X\\) e \\(Y\\) son independientes, la distribución condicional de \\(Y|X=x\\) es una \\(N(0,1)\\), distribución que coincide con la distribución de la variable aleatoria marginal \\(Y\\).\n\n\n\n\n\n\n6.7.3 Valores esperados condicionales\nDefinición de valor esperado condicional. Dada una variable aleatoria bidimensional \\((X,Y)\\), definimos el valor esperado de la variable \\(Y\\) dado que \\(X=x\\) como \\(E(Y|x)\\), es decir, el valor esperado de la variable aleatoria condicional \\(Y|X=x\\): \\[\nE(Y|x)=\\begin{cases}\n\\displaystyle\\sum_{y_j} y_j \\cdot P_{Y|X=x}(y_j), & \\mbox{ caso discreto,}\\\\\n\\displaystyle\\int_{-\\infty}^\\infty y \\cdot f_{Y|X=x}(y)\\,dy, & \\mbox{ caso continuo.}\n\\end{cases}\n\\]\nTenemos el siguiente resultado relacionado con los valores esperados: el valor esperado respecto \\(x\\) del valor esperado de la variable condicional \\(Y|X=x\\) coincide con el valor esperado de la variable \\(Y\\):\nProposición.  Sea \\((X,Y)\\) una variable aleatoria bidimensional. Sean \\(E(Y|x)\\) el valor esperado condicional de \\(Y\\) respecto \\(x\\). Entonces el valor esperado de la variable aleatoria \\(E(Y|X)\\) como función de la variable \\(X\\) es el valor esperado de la variable \\(Y\\): \\[\nE_X(E(Y|X))=E(Y).\n\\]\n\nDemostración\nHaremos la demostración en el caso continuo. Dejamos como ejercicio la demostración para el caso discreto.\nSea \\(f_{XY}\\) la función de densidad conjunta y \\(f_X\\) y \\(f_Y\\) las funciones de densidad marginales.\nEl valor de \\(E_X(E(Y|X))\\) es: \\[\n\\begin{array}{rl}\nE_X(E(Y|X)) & =\\int_{x=-\\infty}^{x=\\infty} E(Y|x)f_X(x)\\, dx=\\int_{x=-\\infty}^{x=\\infty}\\int_{y=-\\infty}^{y=\\infty} y f_{Y|X=x}(y)\\, dy f_X(x)\\, dx \\\\ & = \\int_{x=-\\infty}^{x=\\infty}\\int_{y=-\\infty}^{y=\\infty} y \\frac{f_{XY}(x,y)}{f_X(x)}f_X(x)\\, dy\\, dx = \\int_{y=-\\infty}^{y=\\infty} y \\int_{x=-\\infty}^{x=\\infty}f_{XY}(x,y)\\, dx\\, dy \\\\ &  = \\int_{y=-\\infty}^{y=\\infty} y f_Y(y)\\, dy = E(Y),\n\\end{array}\n\\] tal como queríamos ver.\n\n\n\n6.7.4 Relación con el problema de la regresión general\nEl problema de la regresión general es el siguiente:\nSea \\((X,Y)\\) una variable aleatoria bidimensional. Queremos hallar una función \\(g\\) tal que la variable \\(\\hat{Y}=g(X)\\) explique mejor la variable \\(Y\\).\nDicho de forma más explícita, queremos hallar una función \\(g\\) tal que minimice el error cometido al aproximar \\(Y\\) por \\(\\hat{Y}=g(X)\\). Dicho error se definede forma natural como el valor esperado de la variable \\((Y-g(X))^2\\):\n\\[\n\\min_g E\\left((Y-g(X))^2\\right).\n\\]\nEl siguiente resultado nos dice cuál es la función \\(g\\):\nProposición:  La función \\(g\\) solución del problema de regresión general es la siguiente: \\(g(x)=E(Y|X=x)\\).\nEs decir, la función \\(g\\) asigna a cada valor \\(x\\) de la variable aleatoria \\(X\\), el valor esperado de la variable condicional \\(Y|X=x\\).\nEn resumen, la función \\(g(x)=E(Y|X=x)\\) es la función que minimiza el error. A la curva \\(y=g(x)\\) se la denomina curva general de regresión de \\(Y\\) sobre \\(X\\).\n\n\n6.7.5 Valores esperados condicionales. Caso general\nPodemos generalizar los valores esperados condicionales en el sentido que en lugar de hallar \\(E(Y|X=x)\\), hallar \\(E(g(Y)|X=x)\\), donde \\(g\\) es una función de la variable aleatoria \\(Y\\):\nDefinición de valor esperado condicional.\nDada una variable aleatoria bidimensional \\((X,Y)\\) y una función \\(g\\), definimos el valor esperado de la variable \\(g(Y)\\) dado que \\(X=x\\) como \\(E(g(Y)|x)\\), es decir, el valor esperado de la variable aleatoria condicional \\(g(Y)|X=x\\):\n\\[\nE(g(Y)|x)=\\begin{cases}\n\\sum_{y_j} g(y_j) P_{Y|X=x}(y_j), & \\mbox{ caso discreto,}\\\\\n\\int_{-\\infty}^\\infty g(y) f_{Y|X=x}(y)\\,dy, & \\mbox{ caso continuo.}\n\\end{cases}\n\\]\nObservación: cuando \\(g(y)=y^k\\), tenemos definidos los momentos condicionados de orden \\(k\\) de la variable \\(Y|X=x\\).\n\n6.7.5.1 Ejemplos\n\nEjemplo de la suma y el producto de los resultados de dos lanzamientos de un dado\nVamos a hallar el valor esperado de la variable aleatoria condicional \\(P|S=8\\).\n\nRecordemos su función de probabilidad:\n\n\n\n\n\n\n\n\n\n\n\\(P|S=8\\)\n\\(12\\)\n\\(15\\)\n\\(16\\)\n\n\n\n\n\\(P_{P|S=8}\\)\n\\(\\frac{\\frac{2}{36}}{\\frac{5}{36}}=\\frac{2}{5}\\)\n\\(\\frac{\\frac{2}{36}}{\\frac{5}{36}}=\\frac{2}{5}\\)\n\\(\\frac{\\frac{1}{36}}{\\frac{5}{36}}=\\frac{1}{5}\\)\n\n\n\n\nSu valor esperado es: \\[\nE(P|S=8)=12\\cdot \\frac{2}{5}+15\\cdot \\frac{2}{5}+16\\cdot \\frac{1}{5}=\\frac{70}{5}=14.\n\\] El valor medio del producto de los resultados al lanzar un dado dos veces cuando la suma de dichos resultados es 8 vale 14.\nEl valor esperado de la variable \\(E(P|S=8)\\) es:\n\nvalores.cond.s8=as.integer(names(prob.cond.s8.buena))\nsum(valores.cond.s8*prob.cond.s8.buena)\n\n[1] 14\n\n\n\n\n\nEjemplo: otra densidad (continuación)\nRecordemos el ejemplo de la variable aleatoria bidimensional continua con función de densidad:\n\\[\nf_{XY}(x,y)=\\begin{cases}\n2 \\mathrm{e}^{-x}\\mathrm{e}^{-y}, & 0\\leq y\\leq x &lt; \\infty,\\\\\n0, & \\mbox{ en caso contrario,}\n\\end{cases}\n\\]\nYa sebemos que si fijamos \\(x_0&gt;0\\), la función de densidad de la variable aleatoria condicionada \\(Y|X=x_0\\) es:\n\\[\nf_{Y|X=x_0}(y)=\\begin{cases}\n\\frac{e^{-y}}{1-\\mathrm{e}^{-x_0}}, & \\mbox{ si }0\\leq y\\leq x_0, \\\\\n0, & \\mbox{en caso contrario.}\n\\end{cases}\n\\]\nHallemos su valor esperado:\n\n\\[\nE(Y|X=x_0)=\\int_0^{x_0} y \\frac{e^{-y}}{(1-\\mathrm{e}^{-x_0})}\\, dy=\\frac{1}{(1-\\mathrm{e}^{-x_0})}\\left[-\\mathrm{e}^{-y} (y+1)\\right]_0^{x_0} = \\frac{1-\\mathrm{e}^{-x_0}(1+x_0)}{1-\\mathrm{e}^{-x_0}}.\n\\]\nVerifiquemos la propiedad vista anteriormente \\(E_X(E(Y|x))=E(Y)\\). Recordemos que la función de densidad marginal de la variable \\(X\\) era: \\(f_X(x)=2\\left(\\mathrm{e}^{-x}-\\mathrm{e}^{-2x}\\right)\\), para \\(x&gt;0\\):\n\\[\n\\begin{array}{rl}\nE_X(E(Y|x)) & =\\int_0^\\infty E(Y|x)\\cdot f_X(x)\\, dx = \\int_0^\\infty \\frac{1-\\mathrm{e}^{-x}(1+x)}{1-\\mathrm{e}^{-x}}\\cdot 2\\left(\\mathrm{e}^{-x}-\\mathrm{e}^{-2x}\\right)\\, dx\n\\\\ & =  2\\int_0^\\infty \\frac{1-\\mathrm{e}^{-x}(1+x)}{1-\\mathrm{e}^{-x}} \\mathrm{e}^{-x}\\left(1-\\mathrm{e}^{-x}\\right)\\, dx = 2 \\int_0^\\infty \\left(\\mathrm{e}^{-x}-\\mathrm{e}^{-2x}(1+x)\\right)\\, dx \\\\ & = 2\\left[-\\mathrm{e}^{-x}+\\mathrm{e}^{-2 x}\n   \\left(\\frac{x}{2}+\\frac{3}{4}\\right)\\right]_0^\\infty = 2 \\left(1-\\frac{3}{4}\\right)=\\frac{1}{2}.\n\\end{array}\n\\]\nRecordemos que la variable \\(Y\\) era exponencial de parámetro \\(\\lambda=2\\). Por tanto \\(E(Y)=\\frac{1}{\\lambda}=\\frac{1}{2}\\), valor que coincide con el hallado, tal como queríamos ver."
  },
  {
    "objectID": "5.html#variables-aleatorias-definidas-como-función-de-dos-variables-aleatorias-conjuntas",
    "href": "5.html#variables-aleatorias-definidas-como-función-de-dos-variables-aleatorias-conjuntas",
    "title": "6  Vectores aleatorios bidimensionales",
    "section": "6.8 Variables aleatorias definidas como función de dos variables aleatorias conjuntas",
    "text": "6.8 Variables aleatorias definidas como función de dos variables aleatorias conjuntas\nDado un experimento aleatorio, a veces estaremos interesados en una o más funciones de las variables asociadas con el experimento.\nPor ejemplo, si consideramos el experimento aleatorio de lanzar un dado dos veces y definimos la variable aleatoria bidimensional \\((X_1,X_2)\\) como la variable que nos da el resultado de cada lanzamiento, podemos expresar la suma y el producto como \\(S=X_1+X_2\\), \\(P=X_1\\cdot X_2\\).\nOtros ejemplos podrían ser considerar el experimento aleatoria de realizar mediciones repetidas de la misma cantidad aleatoria. Entonces, podríamos estar interesados en el valor máximo y mínimo en el conjunto, así como la media muestral y la varianza muestral.\nEn esta sección presentamos métodos para determinar las probabilidades de eventos que involucran funciones de dos variables aleatorias.\nDaremos métodos de cómo hallar la función de distribución y la función de probabilidad (caso discreto) o la función de densidad (caso continuo) de la variable aleatoria definida como función de la variable aleatoria bidimensional.\n\n6.8.1 Variable aleatoria función de la variable aleatoria bidimensional\nProposición.\nSea \\((X,Y)\\) una variable aleatoria bidimensional con función de probabilidad \\(P_{XY}\\) (caso discreto) o función de densidad (caso continuo). Sea \\(g\\) una función y definimos la variable aleatoria unidimensional \\(Z\\) como \\(Z=g(X,Y)\\). Entonces la función de distribución de \\(Z\\) es: \\[\n\\begin{array}{rl}\nF_Z(z) & = \\displaystyle P(Z\\leq z)=\\sum\\sum_{(x_i,y_j),\\ |\\ g(x_i,y_j)\\leq z} P_{XY}(x_i,y_j),\\ z\\in\\mathbb{R},\\\\ &\\ \\qquad\\mbox{ (caso discreto),}\\\\\nF_Z(z) & = \\displaystyle  P(Z\\leq z)=\\int\\int_{(x,y)\\in\\mathbb{R}^2,\\ |\\ g(x,y)\\leq z} f_{XY}(x,y)\\,dy\\, dx, \\ z\\in\\mathbb{R},\\\\ &\\ \\qquad\\mbox{ (caso continuo).}\n\\end{array}\n\\]\n\nObservación.\nEn el caso discreto, la variable aleatoria será discreta con valores \\(Z(\\Omega)=\\{z_{ij}=g(x_i,y_j),\\ |\\ (x_i,y_j)\\in (X,Y)(\\Omega)\\}\\). Hay que tener en cuenta que en dicho conjunto puede haber repeticiones, es decir, pueden existir dos parejas \\((i,j)\\) y \\((i',j')\\) tal que \\(z_{ij}=z_{i'j'}\\).\nLa expresión de la función de probabilidad en el caso discreto se complica mucho debido a dichas repeticiones y es mejor hallarla en cada caso concreto.\nLa última observación se puede aplicar también en el caso continuo: la expresión de la función de densidad se halla en cada caso concreto.\n\nEjemplo del lanzamiento de un dado dos veces.\nConsideremos el experimento aleatorio de lanzar dos veces un dado.\nSea \\((X,Y)\\) la variable aleatoria bidimensional discreta ya estudiada anteriormente donde \\(X\\) nos da el resultado del primer lanzamiento e \\(Y\\), el resultado del segundo lanzamiento.\nVimos que \\((X,Y)(\\Omega)=\\{(i,j),\\ i=1,2,3,4,5,6,\\ j=1,2,3,4,5,6\\}\\) con función de probabilidad conjunta \\(P_{XY}(i,j)=\\frac{1}{36}\\), \\(i=1,2,3,4,5,6,\\ j=1,2,3,4,5,6.\\)\nAnteriormente hemos estudiado la suma \\(S\\) de los resultados. En este caso podemos interpretar \\(S=g(X,Y)\\) donde \\(g(x,y)=x+y\\).\nComo la función \\(S\\) ya ha sido estudiada y el producto se ha dejado como ejercicio, estudiaremos la siguiente variable aleatoria función de \\(X\\) e \\(Y\\): \\(Z=X^2+Y^2\\).\nRealizaremos los cálculos con ayuda de R ya que hacerlos a mano es bastante tedioso.\nLos valores de \\(Z(\\Omega)\\) son: \\(Z(\\Omega)=\\{z_{ij}=i^2+j^2,\\ i=1,2,3,4,5,6,\\ j=1,2,3,4,5,6\\}\\). Observad que hay parejas \\((i,j)\\) que dan lugar a los mismos valores, por ejemplo \\(1^2+2^2 = 2^2+1^2\\), y, en general, si \\(i\\neq j\\), \\(z_{ij}=i^2+j^2=z_{ji}=j^2+i^2\\).\n\nPara hallar el conjunto \\(Z(\\Omega)\\) usamos la función outer de R:\n\ng=function(x,y){x^2+y^2}  ## definimos la función g\nsort(unique(as.vector(outer(1:6,1:6,g))))\n\n [1]  2  5  8 10 13 17 18 20 25 26 29 32 34 37 40 41 45 50 52 61 72\n\n\nVemos que hay 21 valores distintos de la variable \\(Z\\).\nPara hallar la función de probabilidad de \\(Z\\) hemos de calcular para cada valor \\(z_k\\), las parejas \\((i,j)\\) tal que \\(i^2+j^2=z_k\\):\n\nvalores.variable.Z = sort(unique(as.vector(outer(1:6,1:6,g))))  \nmatriz.valores = outer(1:6,1:6,g) ## aplicamos la función g a \n##  todas las parejas (i,j), i,j=1,2,3,4,5,6\nfrecuencias = c()  ## vector donde guardaremos las frecuencias de los valores de Z\nfor (i in 1:length(valores.variable.Z)){\n  z=valores.variable.Z[i]\n  frecuencias=c(frecuencias,length(matriz.valores[matriz.valores==z]))\n}\nfrecuencias\n\n [1] 1 2 1 2 2 2 1 2 2 2 2 1 2 2 2 2 2 1 2 2 1\n\n\nLa función de probabilidad de \\(Z\\) es:\n\nfunción.probabilidad.Z=data.frame(rbind(valores.variable.Z,round(frecuencias/36,3)))\nrownames(función.probabilidad.Z)=c(\"Z\",\"P_Z\")\nfunción.probabilidad.Z\n\n       X1    X2    X3     X4     X5     X6     X7     X8     X9    X10    X11\nZ   2.000 5.000 8.000 10.000 13.000 17.000 18.000 20.000 25.000 26.000 29.000\nP_Z 0.028 0.056 0.028  0.056  0.056  0.056  0.028  0.056  0.056  0.056  0.056\n       X12    X13    X14    X15    X16    X17    X18    X19    X20    X21\nZ   32.000 34.000 37.000 40.000 41.000 45.000 50.000 52.000 61.000 72.000\nP_Z  0.028  0.056  0.056  0.056  0.056  0.056  0.028  0.056  0.056  0.028\n\n\n\n\n\nEjemplo variables aleatorias continuas\nRecordemos la variable aleatoria bidimensional \\((X,Y)\\) con función de densidad: \\[\nf_{XY}(x,y)=\\begin{cases}\n2\\cdot  \\mathrm{e}^{-x}\\cdot \\mathrm{e}^{-y}, & 0\\leq y\\leq x &lt; \\infty,\\\\\n0, & \\mbox{ en caso contrario,}\n\\end{cases}\n\\] Consideremos la variable aleatoria \\(Z=X+Y\\). Vamos a calcular la función de densidad de \\(Z\\).\nEn primer lugar, los valores de \\(Z\\) para los que \\(f_Z(z)\\neq 0\\) cumplen \\(z\\geq 0\\) ya que \\(X\\geq 0\\) e \\(Y\\geq 0\\).\nCalculemos la función de distribución de la variable \\(Z\\). Sean \\(z\\in\\mathbb{R}\\) con \\(z\\geq 0\\):\n\n\\[\n\\begin{array}{rl}\nF_Z(z) & =  P(Z\\leq z)=P(X+Y\\leq z)\\\\\n& =\\displaystyle \\int\\int_{\\{(x,y)\\mathbb{R}^2,\\ |\\ x+y\\leq z\\}\\cap \\{(x,y)\\in \\mathbb{R}^2,\\ |\\ 0\\leq y\\leq x&lt;\\infty\\}} 2\\cdot  \\mathrm{e}^{-x}\\cdot \\mathrm{e}^{-y}\\, dy\\, dx\n\\end{array}\n\\]\nEl gráfico siguiente muestra en color violeta la región de integración para hallar \\(F_Z(z)\\) dado un \\(z\\geq 0\\).\n\n\n\n\n\n\n\nEl valor de \\(F_Z(z)\\) es: (fijémonos que primero fijamos la \\(y\\) y para cada \\(y\\) la \\(x\\) va desde la recta \\(x=y\\) hasta la recta \\(x=z-y\\))\n\\[\n\\begin{array}{rl}\nF_Z(z) & =\\displaystyle\\int_{y=0}^{y=\\frac{z}{2}}\\int_{x=y}^{x=z-y}2 \\cdot\\mathrm{e}^{-x}\\cdot\\mathrm{e}^{-y}\\, dx\\, dy = 2 \\cdot\\int_{y=0}^{y=\\frac{z}{2}} \\mathrm{e}^{-y} \\cdot\\left[-\\mathrm{e}^{-x}\\right]_{x=y}^{x=z-y}\\, dy \\\\ &\n\\displaystyle = 2 \\cdot\\cdot\\int_{y=0}^{y=\\frac{z}{2}} \\mathrm{e}^{-y}\\cdot \\left(\\mathrm{e}^{-y}-\\mathrm{e}^{y-z}\\right)\\, dy = 2 \\cdot\\int_{y=0}^{y=\\frac{z}{2}} \\left(\\mathrm{e}^{-2y}-\\mathrm{e}^{-z} \\right)\\, dy  \\\\\n& = 2\\cdot\\left[-\\frac{1}{2}\\mathrm{e}^{-2y}-\\mathrm{e}^{-z} \\cdot y\\right]_{y=0}^{y=\\frac{z}{2}}  = \\displaystyle 2\\cdot\\left(\\frac{1}{2}-\\frac{1}{2}\\mathrm{e}^{-z}-\\frac{z}{2}\\mathrm{e}^{-z}\\right) \\\\\n& = 1-\\mathrm{e}^{-z}\\cdot(1+z),\\ z\\geq 0.\n\\end{array}\n\\] La función de densidad de \\(Z\\) es:\n\\[\nf_Z(z)=F'_Z(z)=z\\cdot\\mathrm{e}^{-z},\\ z\\geq 0,\n\\] y \\(f_Z(z)=0\\) en caso contrario.\n\n\n\nEjemplo de la suma de dos normales\nConsideremos el caso en que la variable aleatoria \\((X,Y)\\) tenga distribución normal bidimensional.\nRecordemos que su función de densidad conjunta era: \\[\nf_{XY}(x,y)=\\frac{1}{2\\cdot \\pi\\cdot \\sqrt{1-\\rho^2}}\\mathrm{e}^{-\\frac{(x^2-2\\cdot \\rho\\cdot  x\\cdot y+y^2)}{2\\cdot (1-\\rho^2)}},\\ -\\infty &lt;x,y&lt;\\infty.\n\\] Consideremos \\(S=X+Y\\). Estudiemos qué distribución tiene \\(S\\).\nDado un valor \\(z\\in\\mathbb{R}\\), la función de distribución de \\(S\\) en \\(s\\) es:\n\n\\[\n\\begin{array}{rl}\nF_S(s) & =P(S\\leq s)=\\displaystyle \\int\\int_{\\{(x,y)\\in\\mathbb{R}^2,\\ |\\ x+y\\leq s\\}}\\frac{1}{2\\pi\\sqrt{1-\\rho^2}}\\mathrm{e}^{-\\frac{(x^2-2\\rho xy+y^2)}{2(1-\\rho^2)}}\\, dy\\, dx \\\\\n& =\\displaystyle  \\frac{1}{2\\pi\\sqrt{1-\\rho^2}} \\int_{x=-\\infty}^{x=\\infty}\\int_{y=-\\infty}^{y=s-x}\\mathrm{e}^{-\\frac{(x^2-2\\rho xy+y^2)}{2(1-\\rho^2)}}\\, dy\\, dx \\\\\n& = \\displaystyle  \\frac{1}{2\\pi\\sqrt{1-\\rho^2}} \\int_{x=-\\infty}^{x=\\infty} \\mathrm{e}^{-\\frac{x^2}{2(1-\\rho^2)}} \\int_{y=-\\infty}^{y=s-x}\\mathrm{e}^{-\\frac{(-2\\rho xy+y^2)}{2(1-\\rho^2)}}\\, dy\\, dx  \\\\ &\\ \\qquad\\mbox{hacemos el siguiente cambio  en la segunda integral $t=y+x$}\\\\\n& = \\displaystyle  \\frac{1}{2\\pi\\sqrt{1-\\rho^2}} \\int_{x=-\\infty}^{x=\\infty} \\mathrm{e}^{-\\frac{x^2}{2(1-\\rho^2)}} \\int_{t=-\\infty}^{t=s}\\mathrm{e}^{-\\frac{(-2\\rho x(t-x)+(t-x)^2)}{2(1-\\rho^2)}}\\, dt\\, dx\n\\end{array}\n\\]\n\\[\n\\begin{array}{rl}\nF_S(s)\n& =  \\displaystyle \\frac{1}{2\\pi\\sqrt{1-\\rho^2}} \\int_{x=-\\infty}^{x=\\infty} \\mathrm{e}^{-\\frac{(1+\\rho)x^2}{1-\\rho^2}}\\int_{t=-\\infty}^{t=s} \\mathrm{e}^{-\\frac{(t^2-2(1+\\rho) t x)}{2(1-\\rho^2)}}\\, dt\\, dx \\\\\n& = \\frac{1}{2\\pi\\sqrt{1-\\rho^2}} \\int_{x=-\\infty}^{x=\\infty} \\mathrm{e}^{-\\frac{(1+\\rho)x^2}{1-\\rho^2}}\\int_{t=-\\infty}^{t=s} \\mathrm{e}^{-\\frac{(t-(1+\\rho)x)^2}{2(1-\\rho^2)}} \\mathrm{e}^{\\frac{(\\rho+1)^2 x^2}{2(1-\\rho^2)}}\\, dt\\, dx  \\\\\n& = \\frac{1}{2\\pi\\sqrt{1-\\rho^2}} \\int_{x=-\\infty}^{x=\\infty} \\mathrm{e}^{-\\frac{x^2}{2}}\\cdot \\sqrt{2\\pi (1-\\rho^2)} F_X(s)\\, dx, \\\\\n& \\mbox{ donde $F_X(s)$ es la función de distribución}\\\\\n& \\mbox{de una variable $X$ normal de parámetros} \\\\\n& \\mbox{ $\\mu =(1+\\rho)x$ y $\\sigma^2=1-\\rho^2$.} \\\\\n& = \\frac{1}{\\sqrt{2\\pi}}\\int_{x=-\\infty}^{x=\\infty} \\mathrm{e}^{-\\frac{x^2}{2}}\\cdot F_X(s)\\, dx.\n\\end{array}\n\\]\nPara calcular la función de densidad \\(f_S(s)\\) aplicamos la expresión \\(f_S(s)=F'_S(s)\\) y la derivación bajo el signo integral:\n\\[\n\\begin{array}{rl}\nf_S(s) & = \\frac{1}{\\sqrt{2\\pi}}\\int_{x=-\\infty}^{x=\\infty} \\mathrm{e}^{-\\frac{x^2}{2}}\\cdot f_X(s)\\, dx = \\frac{1}{\\sqrt{2\\pi}}\\int_{x=-\\infty}^{x=\\infty} \\mathrm{e}^{-\\frac{x^2}{2}}\\cdot \\frac{1}{\\sqrt{2\\pi (1-\\rho^2)}}\\mathrm{e}^{-\\frac{(s-(1+\\rho)x)^2}{2(1-\\rho^2)}}\\, dx \\\\ & = \\frac{\\mathrm{e}^{-\\frac{s^2}{2(1-\\rho^2)}}}{2\\pi\\sqrt{1-\\rho^2}} \\int_{x=-\\infty}^{x=\\infty} \\mathrm{e}^{-\\frac{(2(1+\\rho) x^2-2(1+\\rho)xs)}{2(1-\\rho^2)}}\\, dx= \\frac{\\mathrm{e}^{-\\frac{s^2}{2(1-\\rho^2)}}}{2\\pi\\sqrt{1-\\rho^2}} \\int_{x=-\\infty}^{x=\\infty} \\mathrm{e}^{-\\frac{\\left(x-\\frac{s}{2}\\right)^2}{1-\\rho}}\\mathrm{e}^{\\frac{s^2}{4(1-\\rho)}}\\, dx\n\\end{array}\n\\]\nEn la última integral hacemos el cambio \\(u=x-\\frac{z}{2}\\): \\[\nf_S(s)  =\\frac{\\mathrm{e}^{-\\frac{s^2}{4(1+\\rho)}}}{2\\pi\\sqrt{1-\\rho^2}} \\int_{u=-\\infty}^{u=\\infty} \\mathrm{e}^{-\\frac{u^2}{1-\\rho}}\\, du.\n\\] A continuación usando que \\(f_Z(z)=\\frac{1}{\\sqrt{2\\pi}}\\mathrm{e}^{-\\frac{x^2}{2}}\\) es la función de densidad de la distribución \\(Z=N(0,1)\\), podemos escribir: \\(\\int_{-\\infty}^\\infty \\frac{1}{\\sqrt{2\\pi}}\\mathrm{e}^{-\\frac{x^2}{2}}=1,\\ \\Rightarrow \\int_{-\\infty}^\\infty \\mathrm{e}^{-\\frac{x^2}{2}}=\\sqrt{2\\pi}.\\)\nSi en la última integral hacemos el cambio \\(v=\\sqrt{\\frac{2}{1-\\rho}}u\\), obtenemos:\n\\[\n\\begin{array}{rl}\nf_S(s)  & = \\frac{\\mathrm{e}^{-\\frac{s^2}{4(1+\\rho)}}}{2\\pi\\sqrt{1-\\rho^2}}\\int_{v=-\\infty}^{v=\\infty}\\mathrm{e}^{-\\frac{v^2}{2}} \\sqrt{\\frac{1-\\rho}{2}}\\, dv= \\frac{\\mathrm{e}^{-\\frac{s^2}{4(1+\\rho)}}}{2\\pi\\sqrt{2(1+\\rho)}}\\int_{v=-\\infty}^{v=\\infty}\\mathrm{e}^{-\\frac{v^2}{2}} \\, dv \\\\ & = \\frac{\\mathrm{e}^{-\\frac{s^2}{4(1+\\rho)}}}{2\\pi\\sqrt{2(1+\\rho)}} \\sqrt{2\\pi}= \\frac{1}{\\sqrt{2\\pi 2(1+\\rho)}}\\mathrm{e}^{-\\frac{s^2}{4(1+\\rho)}},\\ s\\in\\mathbb{R}.\n\\end{array}\n\\]\nDicha función de densidad corresponde a una distribución normal de parámetros \\(\\mu =0\\) y \\(\\sigma = \\sqrt{2(1+\\rho)}\\).\nEn resumen, la distribución de la suma de dos normales es una normal de parámetros \\(S=N(\\mu=0,\\sigma = \\sqrt{2(1+\\rho)})\\).\n\n\n\n\n6.8.2 Transformaciones lineales de variables aleatorias\nConsideremos una variable aleatoria bidimensional continua \\((X,Y)\\) con función de densidad conjunta \\(f_{XY}\\).\nDefinimos la variable aleatoria bidimensional continua \\((U,V)\\) a partir de una transformación lineal de la variable \\((X,Y)\\). Es decir, existe una matriz \\(\\mathbf{M}=\\begin{pmatrix}a & b\\\\ c& d\\end{pmatrix}\\) y un vector \\(\\mathbf{n}=\\begin{pmatrix}\\alpha\\\\\\beta \\end{pmatrix}\\) tal que:\n\\[\n\\begin{array}{rl}\n\\begin{pmatrix}U\\\\ V\\end{pmatrix} & =\\mathbf{M}\\cdot \\begin{pmatrix}X\\\\ Y\\end{pmatrix}+\\mathbf{n}=\\begin{pmatrix}a & b\\\\ c& d\\end{pmatrix}\\cdot\\begin{pmatrix}X\\\\ Y\\end{pmatrix}+\\begin{pmatrix}\\alpha\\\\\\beta \\end{pmatrix},\\\\  & \\Rightarrow \\left.\\begin{array}{rl}U & = aX+bY+\\alpha,\\\\ V & =cX+dY+\\beta.\\end{array}\\right\\}\n\\end{array}\n\\]\nPara que \\((U,V)\\) sea una variable aleatoria bidimensional, necesitamos que la matriz \\(\\mathbf{M}\\) sea no singular, o \\(\\mathrm{det}(\\mathbf{M})\\neq 0\\).\nNos preguntamos cuál es la relación entre la función de densidad de la variable \\((U,V)\\), \\(f_{UV}\\) y la función de densidad de la variable \\((X,Y)\\), \\(f_{XY}\\). La expresión siguiente nos da dicha relación:\n\\[\nf_{UV}(u,v)=\\frac{1}{|\\mathrm{det}(\\mathbf{M})|}f_{XY}\\left(\\mathbf{M}^{-1}\\begin{pmatrix}u-\\alpha\\\\ v-\\beta\\end{pmatrix}\\right), \\ (u,v)\\in\\mathbb{R}^2.\n\\]\nObservación.  Si la variable \\((X,Y)\\) tiene una región \\(D\\) donde \\(f_{XY}(x,y)\\neq 0\\), para todo \\((x,y)\\in D\\), antes de aplicar la expresión anterior para hallar la función de densidad de la variable \\((U,V)\\) hemos de calcular cómo se transforma \\(D\\) con la matriz \\(\\mathbf{M}\\). Es decir, hay que hallar la región\n\\[\nD'=\\mathbf{M}(D)=\\{(u,v)\\in\\mathbb{R}^2,\\ \\mbox{existe $(x,y)\\in D$ con } (u,v)=\\mathbf{M}(x,y)+\\mathbf{n}\\}.\n\\]\n\nEjemplo: transformación lineal\nConsideremos la variable \\((X,Y)\\) continua con función de densidad: \\[\nf_{XY}(x,y)=\\begin{cases}\n\\frac{1}{2}(1+x+y), & \\mbox{ si }(x,y)\\in R, \\\\\n0, & \\mbox{en caso contrario.}\n\\end{cases}\n\\] donde \\(R\\) es el rombo de vértices \\((1,0)\\), \\((0,1)\\), \\((-1,0)\\) y \\((0,-1)\\), ver figura adjunta.\nOtra forma de definir la función anterior es:\n\\[\nf_{XY}(x,y)=\\begin{cases}\n\\frac{1}{2}(1+x+y), & -1\\leq x\\leq 0,\\ -1-x\\leq y\\leq x+1, \\\\\n\\frac{1}{2}(1+x+y), & 0\\leq x\\leq 0,\\ x-1\\leq y\\leq 1-x, \\\\\n0, & \\mbox{en caso contrario.}\n\\end{cases}\n\\] Dejamos como ejercicio al lector comprobar que la función anterior es una función de densidad.\n\n\n\n\n\n\n\n\nConsideramos la variable aleatoria bidimensional \\((U,V)\\) definida a partir de la variable \\((X,Y)\\):\n\\[\n\\begin{pmatrix}U\\\\ V\\end{pmatrix}=\\begin{pmatrix}1 & -1\\\\ 1& 1\\end{pmatrix}\\cdot\\begin{pmatrix}X\\\\ Y\\end{pmatrix},\\ \\Rightarrow \\left.\\begin{array}{rl}U & = X-Y,\\\\ V & =X+Y.\\end{array}\\right\\}\n\\] La región \\(R\\) se transforma en el cuadrado \\(C\\) de vértices \\((1,1)\\), \\((-1,1)\\), \\((-1,-1)\\) y \\((1,-1)\\) ya que si aplicamos la matriz a los vértices del rombo, obtenemos los vértices de cuadrado: \\[\n\\begin{array}{rl}\n\\begin{pmatrix}1 & -1\\\\ 1& 1\\end{pmatrix}\\cdot \\begin{pmatrix}1\\\\ 0\\end{pmatrix} & =\\begin{pmatrix}1\\\\ 1\\end{pmatrix},\\qquad\n\\begin{pmatrix}1 & -1\\\\ 1& 1\\end{pmatrix}\\cdot \\begin{pmatrix}0\\\\ 1\\end{pmatrix}=\\begin{pmatrix}-1\\\\ 1\\end{pmatrix},\\\\\n\\begin{pmatrix}1 & -1\\\\ 1& 1\\end{pmatrix}\\cdot \\begin{pmatrix}-1\\\\ 0\\end{pmatrix} & =\\begin{pmatrix}-1\\\\ -1\\end{pmatrix},\\qquad\n\\begin{pmatrix}1 & -1\\\\ 1& 1\\end{pmatrix}\\cdot \\begin{pmatrix}0\\\\ -1\\end{pmatrix}=\\begin{pmatrix}1\\\\ -1\\end{pmatrix}.\n\\end{array}\n\\] Ver la figura adjunta.\nPara hallar la función de densidad \\(f_{UV}\\) necesitamos escribir \\(X\\) e \\(Y\\) en función de \\(U\\) y \\(V\\): \\[\n\\begin{pmatrix}X\\\\ Y\\end{pmatrix}=\\begin{pmatrix}1 & -1\\\\ 1& 1\\end{pmatrix}^{-1}\\cdot\\begin{pmatrix}U\\\\ V\\end{pmatrix}=\\begin{pmatrix}\\frac{1}{2} & \\frac{1}{2}\\\\ -\\frac{1}{2}& \\frac{1}{2}\\end{pmatrix}\\cdot\\begin{pmatrix}U\\\\ V\\end{pmatrix},\\ \\Rightarrow \\left.\\begin{array}{rl}X & = \\frac{1}{2}(U+V),\\\\ Y & =\\frac{1}{2}(-U+V).\\end{array}\\right\\}\n\\]\n\n\n\n\n\n\n\nLa función de densidad \\(f_{UV}\\) es, \\[\n\\begin{array}{rl}\nf_{UV}(u,v) & =\\frac{1}{\\left|\\mathrm{det}\\begin{pmatrix}1 & -1\\\\ 1& 1\\end{pmatrix}\\right|}\\cdot f_{XY}\\left(\\frac{1}{2}(u+v),\\frac{1}{2}(-u+v)\\right) \\\\ & =\\frac{1}{2}\\cdot \\frac{1}{2}\\cdot \\left(1+\\frac{1}{2}(-u+v)+\\frac{1}{2}(u+v)\\right)=\\frac{1}{4}(1+v),\n\\end{array}\n\\] para \\((u,v)\\) perteneciente al cuadrado \\(C\\) de vértices \\((1,1)\\), \\((-1,1)\\), \\((-1,-1)\\) y \\((1,-1)\\), o si se quiere para \\(-1\\leq u\\leq 1\\), \\(-1\\leq v\\leq 1\\), y \\(f_{UV}(u,v)=0\\), en caso contrario.\nObservamos que es más cómodo trabajar con las variables \\((u,v)\\) en vez de trabajar con las variables \\((x,y)\\) por dos razones:\n\nLa región donde la función de densidad no es nula es más simple, ya que trabajar con un cuadrado simplifica mucho más los cálculos que trabajar con un rombo a la hora de hallar la función de distribución, densidades marginales, densidades condicionadas, valores esperados, etc.\nLa expresión de la función de densidad también es más simple, ya que sólo depende de la segunda variable \\(v\\); sin embargo, la función de densidad inicial \\(f_{XY}\\) dependía de las dos variables \\(x\\) e \\(y\\).\n\n\n\n\nEjemplo\nConsideremos el caso en que la variable aleatoria \\((X,Y)\\) tenga distribución normal bidimensional.\nRecordemos que su función de densidad conjunta era: \\[\nf_{XY}(x,y)=\\frac{1}{2\\pi\\sqrt{1-\\rho^2}}\\mathrm{e}^{-\\frac{(x^2-2\\rho xy+y^2)}{2(1-\\rho^2)}},\\ -\\infty &lt;x,y&lt;\\infty.\n\\] Recordemos que las distribuciones marginales son distribuciones \\(N(0,1)\\).\nLa idea es hallar la función de densidad conjunta de una distribución normal bidimensional para la que sus distribuciones marginales sean dos normales \\(N(\\mu_1,\\sigma_1)\\) y \\(N(\\mu_2,\\sigma_2)\\).\n\nRecordemos que si \\(Z=N(0,1)\\), entonces \\(\\sigma_1\\cdot Z+\\mu_1 =N(\\mu_1,\\sigma_1)\\). Este hecho, motiva que consideremos el cambio lineal siguiente a las variables \\(X\\) e \\(Y\\): \\[\n\\begin{pmatrix}U\\\\ V\\end{pmatrix}=\\begin{pmatrix}\\sigma_1 & 0\\\\ 0& \\sigma_2\\end{pmatrix}\\cdot\\begin{pmatrix}X\\\\ Y\\end{pmatrix}+\\begin{pmatrix}\\mu_1\\\\\\mu_2\\end{pmatrix},\\ \\Rightarrow \\left.\\begin{array}{rl}U & = \\sigma_1\\cdot X+\\mu_1,\\\\ V & =\\sigma_2\\cdot Y+\\mu_2.\\end{array}\\right\\}\n\\]\nLa función de densidad conjunta \\(f_{UV}\\) es: \\[\n\\begin{array}{rl}\nf_{UV}(u,v) & = \\frac{1}{\\left|\\begin{pmatrix}\\sigma_1 & 0\\\\ 0& \\sigma_2\\end{pmatrix}\\right|} f_{XY}\\left(\\frac{u-\\mu_1}{\\sigma_1},\\frac{v-\\mu_2}{\\sigma_2}\\right)\n=\\frac{1}{\\sigma_1\\cdot \\sigma_2}f_{XY}\\left(\\frac{u-\\mu_1}{\\sigma_1},\\frac{v-\\mu_2}{\\sigma_2}\\right)\\\\ & =\n\\frac{1}{2\\pi\\sigma_1\\sigma_2\\sqrt{1-\\rho^2}}\\mathrm{e}^{-\\frac{\\left(\\left(\\frac{u-\\mu_1}{\\sigma_1}\\right)^2-2\\rho \\left(\\frac{u-\\mu_1}{\\sigma_1}\\right)\\left(\\frac{v-\\mu_2}{\\sigma_2}\\right)+\\left(\\frac{v-\\mu_2}{\\sigma_2}\\right)^2\\right)}{2(1-\\rho^2)}},\n\\end{array}\n\\] para \\((u,v)\\in\\mathbb{R}^2\\).\nSi llamamos \\(\\mathbf{\\Sigma}\\) a la matriz \\(\\mathbf{\\Sigma}=\\begin{pmatrix}\\sigma_1^2 & \\rho\\sigma_1\\sigma_2\\\\ \\rho\\sigma_1\\sigma_2 & \\sigma_2^2\\end{pmatrix}\\), llamada matriz de covarianzas de la distribución normal \\((U,V)\\) la función de densidad anterior puede escribirse como: \\[\nf_{UV}(u,v)=\\frac{1}{2\\pi \\sqrt{\\left|\\mathrm{\\Sigma}\\right|}}\\mathrm{e}^{-\\frac{1}{2}(\\mathbf{u}-\\mathbf{\\mu})^\\top \\mathbf{\\Sigma}^{-1}(\\mathbf{u}-\\mathbf{\\mu})},\\ \\mbox{donde $\\mathbf{u}=\\begin{pmatrix}u \\\\ v\\end{pmatrix}$ y $\\mathbf{\\mu}=\\begin{pmatrix}\\mu_1\\\\\\mu_2\\end{pmatrix}$.}\n\\] La variable aleatoria bidimensional \\((X,Y)\\) es la variable aleatoria tipificada con respecto de la variable \\((U,V)\\).\n\n\n\n\n6.8.3 Transformaciones generales de variables aleatorias\nConsideremos una variable aleatoria bidimensional continua \\((X,Y)\\) con función de densidad conjunta \\(f_{XY}\\).\nDefinimos la variable aleatoria bidimensional continua \\((U,V)\\) a partir de una transformación general de la variable \\((X,Y)\\). Es decir, existen dos funciones de dos variables \\(g_1\\) y \\(g_2\\) tal que: \\[\nU  = g_1 (X,Y),\\quad\nV  = g_2 (X,Y).\n\\] Vamos a suponer que las funciones \\(g_1\\) y \\(g_2\\) son invertibles, es decir, dados \\((u,v)\\), podemos encontrar \\((x,y)\\) tal que \\(x=h_1(u,v)\\) e \\(y=h_2(u,v)\\). Las funciones \\(h_1\\) y \\(h_2\\) son las inversas de las funciones \\(g_1\\) y \\(g_2\\), respectivamente.\nLa función de densidad conjunta \\(f_{UV}\\) se puede expresar de la forma siguiente en función de la función de densidad conjunta \\(f_{XY}\\):\n\\[\n\\begin{array}{rl}\nf_{UV}(u,v) & =\\left|\\mathrm{det}\\begin{pmatrix}\\frac{\\partial h_1}{\\partial u} & \\frac{\\partial h_1}{\\partial v}\\\\ \\frac{\\partial h_2}{\\partial u} & \\frac{\\partial h_2}{\\partial v}\\end{pmatrix}\\right|f_{XY}(h_1(u,v),h_2(u,v))\\\\ & =\\frac{1}{\\left|\\mathrm{det}\\begin{pmatrix}\\frac{\\partial g_1}{\\partial x} & \\frac{\\partial g_1}{\\partial y}\\\\ \\frac{\\partial g_2}{\\partial x} & \\frac{\\partial g_2}{\\partial y}\\end{pmatrix}\\right|_{x=h_1(u,v),y=h_2(u,v)}}f_{XY}(h_1(u,v),h_2(u,v)).\n\\end{array}\n\\]\nA la matriz \\(\\begin{pmatrix}\\frac{\\partial g_1}{\\partial x} & \\frac{\\partial g_1}{\\partial y}\\\\ \\frac{\\partial g_2}{\\partial x} & \\frac{\\partial g_2}{\\partial y}\\end{pmatrix}\\) se le llama matriz jacobiana del cambio y a la matriz \\(\\begin{pmatrix}\\frac{\\partial h_1}{\\partial u} & \\frac{\\partial h_1}{\\partial v}\\\\ \\frac{\\partial h_2}{\\partial u} & \\frac{\\partial h_2}{\\partial v}\\end{pmatrix}\\), matriz jacobiana del cambio inverso.\n\nEjemplo: cambio a polares\nSea \\((X,Y)\\) una variable aleatoria bidimensional cuya función de densidad conjunta es:\n\\[\nf_{XY}(x,y)=\n\\begin{cases}\n\\frac{2}{\\pi}\\left(x^2 + y^2\\right), & \\mbox{si }(x,y)\\in D_1, \\\\\n0, & \\mbox{en caso contrario,}\n\\end{cases}\n\\] donde \\(D_1\\) es el disco de radio \\(1\\): \\[\nD_1 = \\{(x,y)\\in\\mathbb{R}^2,\\ | \\ x^2+y^2\\leq 1\\}.\n\\] El cambio a polares consiste en considerar las coordenadas polares \\((r,\\alpha)\\) de un punto cualquiera \\((x,y)\\) del plano, ver figura adjunta. El cambio que pasa de \\((r,\\alpha)\\) a \\((x,y)\\) (fijaos que es el cambio inverso, según nuestra notación o \\(h_1\\)y \\(h_2\\), respectivamente) es: \\[\nx=h_1(r,\\alpha)=r\\cdot \\cos\\alpha,\\quad y=h_2(r,\\alpha)=r\\cdot \\sin\\alpha.\n\\]\n\n\n\n\n\n\n\n\nFijémonos que, con el cambio a polares, el disco unidad \\(D_1\\) se transforma en el rectángulo \\([0,1]\\times [0,2\\pi]\\).\nHallemos el jacobiano del cambio inverso: \\[\n\\mathrm{det}\\begin{pmatrix}\\frac{\\partial h_1}{\\partial u} & \\frac{\\partial h_1}{\\partial v}\\\\ \\frac{\\partial h_2}{\\partial u} & \\frac{\\partial h_2}{\\partial v}\\end{pmatrix} =\\mathrm{det}\\begin{pmatrix}\\cos\\alpha & -r\\sin\\alpha\\\\ \\sin\\alpha & r\\cdot\\cos\\alpha\\end{pmatrix} = r.\n\\] La función de densidad conjunta \\(f_{r\\alpha}\\) en las nuevas variables (polares) es: \\[\nf_{r\\alpha}(r,\\alpha)=r\\cdot \\frac{2}{\\pi}\\left((r\\cos\\alpha)^2+(r\\sin\\alpha)^2\\right)=\\frac{2}{\\pi}\\cdot r^3,\n\\] si \\((r,\\alpha)\\in [0,1]\\times [0,2\\pi]\\).\nPodemos comentar que, gracias al cambio a polares, en este caso, es mucho más sencillo y cómodo trabajar con las variables \\((r,\\alpha)\\) en vez de trabajar con las variables \\((x,y)\\) por dos razones:\n\nLa región donde la función de densidad no es nula es más simple, ya que trabajar con un rectángulo simplifica mucho más los cálculos que trabajar con un disco a la hora de hallar la función de distribución, densidades marginales, densidades condicionadas, valores esperados, etc.\n\nPor ejemplo, comprobar que el área de la función de densidad conjunta \\(f_{r\\alpha}\\) da \\(1\\) es trivial: \\[\n\\int_{r=0}^{r=1}\\int_{\\alpha =0}^{\\alpha =2\\pi}\\frac{2}{\\pi} r^3\\, d\\alpha\\, dr = \\frac{2}{\\pi}\\cdot 2\\pi \\left[\\frac{r^4}{4}\\right]_{r=0}^{r=1}=4\\cdot \\frac{1}{4}=1.\n\\]\n\nLa expresión de la función de densidad también es más simple, ya que sólo depende de la primera variable \\(r\\); sin embargo, la función de densidad inicial \\(f_{XY}\\) dependía de las dos variables \\(x\\) e \\(y\\)."
  },
  {
    "objectID": "6.html#varias-variables-aleatorias",
    "href": "6.html#varias-variables-aleatorias",
    "title": "7  Vectores aleatorios",
    "section": "7.1 Varias variables aleatorias",
    "text": "7.1 Varias variables aleatorias\nEn el capítulo anterior trabajamos con variables aleatorias bidimensionales\nEn este capítulo vamos a generalizar los conceptos introducidos para variables aleatorias \\(n\\)-dimensionales, con \\(n\\geq 3\\).\nEl ejemplo que comentamos en el capítulo de variables aleatorias de medir la temperatura media un día determinado del año durante 10 años es un ejemplo de variable aleatoria 10-dimensional.\n\n7.1.1 Definición\nLa generalización de la noción de variable aleatoria \\(n\\)-dimensional a partir de la noción de variable aleatoria bidimensional es bastante obvia:\nDefinición de variable aleatoria \\(n\\)-dimensional: Dado un experimento aleatorio con espacio muestral \\(\\Omega\\), definimos variable aleatoria \\(n\\)-dimensional \\(\\mathbf{X}=(X_1,X_2,\\ldots,X_n)\\) a toda aplicación \\[\n\\begin{array}{rl}\n\\mathbf{X}=(X_1,X_2,\\ldots,X_n): \\Omega & \\longrightarrow \\mathbb{R}^n\\\\\nw & \\longrightarrow \\mathbf{X}(w)=(X_1(w),X_2(w),\\ldots,X_n(w)).\n\\end{array}\n\\]\n\nEjemplo: puertos entrada internet\nTenemos tres puertos de entrada de paquetes de internet.\nSupongamos que cada milisegundo llega un paquete y el switch lo asigna a cada uno de los puertos con probabilidad \\(\\frac{1}{3}\\).\nEstudiamos cómo se distribuyen los paquetes en 4 milisegundos.\n\nSea \\(\\mathbf{X}=(X_1,X_2,X_3)\\) la variable aleatoria 3-dimensional, donde \\(X_i\\) nos da el número de paquetes que ha recibido el puerto \\(i\\)-ésimo durante estos 4 milisegundos.\nPor ejemplo, el suceso \\(\\{X_1\\leq 1, X_2\\geq 3, X_3\\leq 1\\}\\) es \\(\\{(0,3,0),(0,3,1),(0,4,0),(0,4,1),(1,3,0),(1,3,1),(1,4,0),(1,4,1)\\}\\).\n\n\n\n\n7.1.2 Representación del dominio de una variable aleatoria \\(n\\)-dimensional\nLos sucesos que se derivan de una variable aleatoria \\(n\\)-dimensional estan especificados por regiones del espacio \\(n\\)-dimensional.\nVeamos algunos ejemplos:\nSuceso: \\(\\{X_1+X_2+X_3\\leq 1\\}\\). En el gráfico siguiente, el plano \\(x_1+x_2+x_3=1\\) separa el espacio en dos partes. Es la parte que corresponde al punto \\((0,0,0)\\).\nSi pensamos el plano anterior como un “espejo” es la parte de atrás del mismo.\n\n\n\n\n\n\n\nSuceso: \\(\\{X_1^2+X_2^2+X_3^2\\leq 1\\}\\). Es el interior de la esfera del gráfico siguiente:\n\n\n\n\n\n\n\nSuceso: \\(\\{0\\leq X_1\\leq 1,\\ 0\\leq X_2\\leq 1,\\ 0\\leq X_3\\leq 1\\}\\). Es el interior del cubo del gráfico siguiente:\n\n\n\n\n\n\n\nLa probabilidad de que la variable \\(n\\)-dimensional pertenezca a una cierta región del \\(n\\)-espacio \\(B\\subset \\mathbb{R}^n\\) se define de la forma siguiente: \\[\nP((X_1,X_2,\\ldots,X_n)\\in B)=P\\{w\\in \\Omega,\\ |\\ (X_1(w),X_2(w),\\ldots,X_n)\\in B\\},\n\\] dicho de otra manera, la probabilidad anterior es la probabilidad del suceso formado por los elementos de \\(w\\in\\Omega\\) que cumplen que su imagen por la variable aleatoria \\(n\\)-dimensional \\((X_1,X_2,\\ldots,X_n)\\) esté en \\(B\\).\nPor ejemplo, si consideramos \\(B=\\{X_1+X_2+\\cdots +X_n\\leq 1\\}\\), \\(P((X_1,X_2,\\ldots,X_n)\\in B)\\) es la probabilidad del suceso formado por los elementos \\(w\\) de \\(\\Omega\\) tal que la suma de las imágenes por \\(X_i\\) desde \\(i=1\\) hasta \\(n\\) sea menor o igual que 1: \\(X_1(w)+\\cdots +X_n\\leq 1\\)."
  },
  {
    "objectID": "6.html#función-de-distribución-conjunta",
    "href": "6.html#función-de-distribución-conjunta",
    "title": "7  Vectores aleatorios",
    "section": "7.2 Función de distribución conjunta",
    "text": "7.2 Función de distribución conjunta\n\n7.2.1 Definición\nDada una variable aleatoria \\(n\\)-dimensional \\((X_1,X_2,\\ldots,X_n)\\), queremos estudiar cómo se distribuye la probabilidad de sucesos cualesquiera de la forma \\(\\{(X_1,X_2,\\ldots,X_n)\\in B\\}\\), donde \\(B\\) es una región del espació \\(n\\)-dimensional \\(\\mathbb{R}^n\\).\nPara ello, definimos la función de distribución conjunta:\nDefinición de función de distribución conjunta: Dada una variable \\(n\\)-dimensional \\((X_1,X_2,\\ldots,X_n)\\), definimos su función de distribución conjunta \\(F_{X_1\\ldots X_n}\\) a la función definida sobre \\(\\mathbb{R}^n\\) de la manera siguiente: \\[\n\\begin{array}{rl}\nF_{X_1\\ldots X_n}: \\mathbb{R}^n & \\longrightarrow \\mathbb{R}\\\\\n(x_1,\\ldots,x_n) & \\longrightarrow F_{X_1\\ldots X_n}(x_1,\\ldots,x_n)=P(X_1\\leq x_1,\\ldots,X_n\\leq x_n).\n\\end{array}\n\\]\nEs decir, dado un valor \\((x_1,\\ldots,x_n)\\in \\mathbb{R}^n\\), consideramos la región del espacio \\(n\\)-dimensional \\((-\\infty,x_1]\\times\\cdots\\times (-\\infty,x_n]\\).\nEntonces la función de distribución conjunta en el valor \\((x_1,\\ldots,x_n)\\) es la probabilidad del suceso formado por aquellos elementos tal que la imagen por la variable aleatoria \\(n\\)-dimensional \\((X_1,X_2,\\ldots,X_n)\\) caen dentro de la región anterior:\n\\[\n\\begin{array}{rl}\nF_{X_1\\ldots X_n}(x_1,\\ldots,x_n) & =P\\{w\\in\\Omega,\\ |\\ (X_1(w),\\ldots,X_n(w)) \\\\ & \\qquad\\qquad\\in (-\\infty,x_1]\\times\\cdots\\times (-\\infty,x_n]\\} \\\\ & = P\\{w\\in\\Omega,\\ |\\ X_1(w)\\leq x_1,\\ldots, X_n(w)\\leq x_n\\}.\n\\end{array}\n\\]\nEl gráfico siguiente muestra el conjunto \\((-\\infty,x_1]\\times (-\\infty,x_2]\\times (-\\infty,x_3]\\) en \\(\\mathbb{R}^3\\) para un valor \\((x,y,z)\\):\n\n\n\n\n\n\n\n\n\n7.2.2 Propiedades\nSea \\((X_1,X_2,\\ldots,X_n)\\) una variable \\(n\\)-dimensional. Sean \\(F_{X_1\\ldots X_n}\\) su función de distribución conjunta. Dicha función satisface las propiedades siguientes:\n\nLa función de distribución conjunta es no decreciente en cada una de las variables: \\[\n\\mbox{Si }x_i\\leq x_i', \\mbox{ para todo $i$, }\\mbox{ entonces, }F_{X_1\\ldots X_n}(x_1,\\ldots,x_n)\\leq F_{X_1\\ldots X_n}(x_1',\\ldots,x_n').\n\\]\n\\(F_{X_1\\ldots X_n}(x_1,\\ldots,x_{i-1},\\stackrel{(i)}{-\\infty},x_{i+1},\\ldots,x_n)=0,\\) para todo \\(i\\) y \\(F_{X_1\\ldots X_n}(\\infty,\\ldots,\\infty)=1\\), para todo \\(x_1,\\ldots,x_n\\in\\mathbb{R}\\).\nLas variables aleatorias \\(X_1,\\ldots, X_n\\) se llaman variables aleatorias marginales y sus funciones de distribución \\(F_{X_1},\\ldots, F_{X_n}\\) pueden hallarse de la forma siguiente como función de la función de distribución conjunta \\(F_{X_1\\ldots X_n}\\): \\[\nF_{X_i}(x_i)=F_{X_1\\ldots X_n}(\\infty,\\ldots,\\infty,\\stackrel{(i)}{x_i},\\infty,\\ldots,\\infty),\n\\] para todo \\(x_1,\\ldots,x_n\\in\\mathbb{R}\\) y para todo \\(i=1,\\ldots,n\\).\nLa función de distribución conjunta es continua por la derecha en todas las variables \\(x_i\\): \\[\n\\begin{array}{rl}\n& \\lim\\limits_{x_i\\to a^+}F_{X_1\\ldots X_n}(x_1,\\ldots,x_{i-1},\\stackrel{(i)}{x_i},x_{i+1},\\ldots,x_n) \\\\ &\\qquad =\\lim\\limits_{x_i\\to a, x_i&gt; a}F_{X_1\\ldots X_n}(x_1,\\ldots,x_{i-1},\\stackrel{(i)}{x_i},x_{i+1},\\ldots,x_n)\\\\ &\\qquad =F_{X_1\\ldots X_n}(x_1,\\ldots,x_{i-1},\\stackrel{(i)}{a},x_{i+1},\\ldots,x_n),\n\\end{array}\n\\] para todo \\(a\\in\\mathbb{R}\\) y para todo \\(i=1,\\ldots,n\\).\n\n\nEjemplo: densidad tridimensional\nConsideremos una variable aleatoria \\(3\\)-dimensional \\((X_1,X_2,X_3)\\) con función de distribución conjunta:\n\\[\nF_{X_1X_2X_3}(x_1,x_2,x_3)=\\begin{cases}\n0, & \\mbox{si }x_1&lt;0,\\mbox{ o }x_2&lt;0,\\mbox{ o }x_3 &lt;0\\\\\nx_1^2\\cdot x_2^2\\cdot x_3^2, & \\mbox{si }0\\leq x_1\\leq 1,\\ 0\\leq x_2\\leq 1,\\ 0\\leq x_3\\leq 1, \\\\\nx_2^2\\cdot x_3^2, & \\mbox{si }x_1&gt; 1,\\ 0\\leq x_2\\leq  1,\\ 0\\leq x_3\\leq  1, \\\\\nx_1^2\\cdot x_3^2, & \\mbox{si }0\\leq x_1\\leq  1,\\ x_2&gt; 1,\\ \\ 0\\leq x_3\\leq  1, \\\\\nx_3^2, & \\mbox{si }x_1&gt; 1,\\ x_2&gt; 1,\\ \\ 0\\leq x_3\\leq  1, \\\\\nx_1^2\\cdot x_2^2, & \\mbox{si }0\\leq x_1\\leq  1,\\ 0\\leq x_2\\leq  1,\\ x_3&gt; 1,\\\\\nx_1^2, & \\mbox{si }0\\leq x_1\\leq  1,\\ x_2 &gt;  1,\\ x_3&gt; 1,\\\\\nx_2^2, & \\mbox{si }x_1&gt;1,\\ 0\\leq x_2\\leq  1,\\ x_3&gt; 1,\\\\\n1, & \\mbox{si }x_1\\geq 1,\\ x_2\\geq 1,\\ x_3\\geq 1.\n\\end{cases}\n\\]\nEn las figuras siguientes, hemos representado por zonas cómo está definida \\(F_{X_1X_2X_3}\\).\nLa primera figura muestra las zonas en la “planta baja” o para \\(0\\leq x_3\\leq 1\\). En color marrón, está representada la región \\(0\\leq x_1\\leq 1,\\ 0\\leq x_2\\leq 1,\\ 0\\leq x_3\\leq 1\\), en color amarillo, la región \\(x_1&gt; 1,\\ 0\\leq x_2\\leq 1,\\ 0\\leq x_3\\leq 1\\), en color verde, la región \\(x_1&gt;1,\\ x_2&gt;1,\\ 0\\leq x_3\\leq 1\\) y en color violeta, la región \\(0\\leq x_1\\leq 1,\\ x_2&gt;1,\\ 0\\leq x_3\\leq 1\\).\nLa segunda figura muestra las zonas del “primer piso” o para \\(x_3&gt;1\\). Los colores tienen un significado similar a los de la primera figura: en color marrón, está representada la región \\(0\\leq x_1\\leq 1,\\ 0\\leq x_2\\leq 1,\\  x_3&gt; 1\\), en color amarillo u ocre, la región \\(x_1&gt; 1,\\ 0\\leq x_2\\leq 1,\\ x_3&gt; 1\\), en color verde, la región \\(x_1&gt;1,\\ x_2&gt;1,\\ x_3&gt; 1\\) y en color violeta, la región \\(0\\leq x_1\\leq 1,\\ x_2&gt;1,\\ x_3&gt; 1\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComprobemos algunas de las propiedades que hemos enunciado anteriormente:\n\nClaramente \\(F_{X_1X_2X_3}(x_1,x_2,-\\infty)=F_{X_1X_2X_3}(x_1,-\\infty,x_3)=F_{X_1X_2X_3}(-\\infty,x_2,x_3)=0\\) ya que \\(F_{X_1X_2X_3}(x_1,x_2,x_3)=0\\) si \\(x_1&lt;0\\) o \\(x_2&lt;0\\) o \\(x_3&lt;0\\). Por tanto, si hacemos tender \\(x_1\\) o \\(x_2\\) o \\(x_3\\) hacia \\(-\\infty\\), obtendremos que \\(F_{X_1X_2X_3}(x_1,x_2,-\\infty)=F_{X_1X_2X_3}(x_1,-\\infty,x_3)=F_{X_1X_2X_3}(-\\infty,x_2,x_3)=0\\).\nDe la misma manera \\(F_{X_1X_2X_3}(\\infty,\\infty,\\infty)=1\\) ya que \\(F_{X_1X_2X_3}(x_1,x_2,x_3)=1\\) para \\(x_1&gt;1\\), \\(x_2&gt;1\\) y \\(x_3&gt;1\\). Por tanto, si hacemos tender \\(x_1\\), \\(x_2\\) y \\(x_3\\) hacia \\(\\infty\\), obtendremos \\(F_{X_1X_2X_3}(\\infty,\\infty,\\infty)=1\\).\nHallemos las marginales: \\[\nF_{X_1}(x_1)=F_{X_1X_2X_3}(x_1,\\infty,\\infty)=\\begin{cases}\n0, & \\mbox{ si }x_1 &lt; 0,\\\\\nx_1, & \\mbox{ si } 0\\leq x_1\\leq 1,\\\\\n1, & \\mbox{ si } x_1&gt;1.\n\\end{cases}\n\\] Para ver la expresión anterior basta trazar el plano \\(X_1=x_1\\) en el gráfico anterior y ver hacia dónde tiende a medida que las variables \\(x_2\\) y \\(x_3\\) se van hacia \\(\\infty\\).\n\n¿Habéis averiguado cuál es la distribución de \\(X_1\\)?\n¡Efectivamente!, \\(X_1\\) es la uniforme en el intervalo \\((0,1)\\).\nDejamos como ejercicio hallar la distribución marginal para las variables \\(X_2\\) e \\(X_3\\).\n\nComprobemos que \\(F_{X_1X_2X_3}\\) es continua por la derecha para las variables \\(x_1\\), \\(x_2\\) y \\(x_3\\) en el punto \\((1,1,1)\\) que es un punto problemático:\n\n\\[\n\\begin{array}{rl}\n\\lim_{x_1\\to 1,x_1&gt; 1} F_{X_1X_2X_3}(x_1,1,1) & =\\lim_{x_1\\to 1,x_1&gt; 1} 1  = F_{X_1X_2X_3}(1,1,1),\\\\\n\\lim_{x_2\\to 1,x_2&gt; 1} F_{X_1X_2X_3}(1,x_2,1) & =\\lim_{x_2\\to 1,x_2&gt; 1} 1 = F_{X_1X_2X_3}(1,1,1),\\\\  \n\\lim_{x_3\\to 1,x_3&gt; 1} F_{X_1X_2X_3}(1,1,x_3) & =\\lim_{x_3\\to 1,x_3&gt; 1} 1  = F_{X_1X_2X_3}(1,1,1).\n\\end{array}\n\\]\n\n\nEjemplo: cálculo con R distribución conjunta\nRealizar un gráfico 3D de la función de distribución conjunta no es posible ya que deberíamos pasar a \\(\\mathbb{R}^4\\).\nLo que sí es posible es dibujar las curvas de nivel de dicha función para un valor de \\(x_3\\) fijado.\nEl los gráficos siguientes dibujamos las curvas de nivel para \\(x_3=0,0.5,1\\) i \\(x_3=1.5\\).\n\nPrimero definimos la función y luego la dibujamos para \\(x_1\\) y \\(x_2\\) entre \\(-1\\) y \\(3\\):\n\nf.dist.con = function(x1,x2,x3){ifelse(x1&lt;0 | x2&lt;0 | x3 &lt;0,0,\n          ifelse(x1&gt;=0 & x1&lt;=1 & x2&gt;=0 & x2&lt;=1 & x3&gt;=0 & x3&lt;=1,\n                 x1^2*x2^2*x3^2,\n          ifelse(x1&gt;1 & x2&gt;=0 & x2&lt;=1 & x3&gt;=0 & x3&lt;=1,\n                 x2^2*x3^2,\n          ifelse(x1&gt;=0 & x1&lt;=1 & x2&gt;1 & x3&gt;=0 & x3&lt;=1,\n                 x1^2*x3^2,\n          ifelse(x1&gt;=0 & x1&lt;=1 & x2&gt;=0 & x2&lt;=1 & x3&gt;1,\n                 x1^2*x2^2,\n          ifelse(x1&gt;=0 & x1&lt;=1 & x2 &gt;1 & x3 &gt;1,x1^2,\n          ifelse(x1&gt;1 & x2 &gt;=0 & x2&lt;=1 & x3 &gt;1,x2^2,\n          ifelse(x1&gt;=0 & x1&lt;=1 & x2&gt;=0 & x2&lt;=1 & x3 &gt;1,\n                 x3^2,1))))))))}\nx1=seq(from=-1,to=3,by=0.05)\nx2=seq(from=-1,to=3,by=0.05)\ncurva.nivel.0=outer(x1,x2,f.dist.con,x3=0)\ncurva.nivel.0.5=outer(x1,x2,f.dist.con,x3=0.5)\ncurva.nivel.1=outer(x1,x2,f.dist.con,x3=1)\ncurva.nivel.1.5=outer(x1,x2,f.dist.con,x3=1.5)\nimage(x1,x2,curva.nivel.0)\n\n\n\n\n\n\n\nimage(x1,x2,curva.nivel.0.5)\n\n\n\n\n\n\n\nimage(x1,x2,curva.nivel.1)\n\n\n\n\n\n\n\nimage(x1,x2,curva.nivel.1.5)\n\n\n\n\nEjemplo del lanzamiento de un dado tres veces\nConsideremos el experimento aleatorio que consiste en lanzar un dado tres veces.\nEl espacio \\(\\Omega\\) de resultados es: \\[\n\\Omega =\\{(i,j,k),\\ | i,j,k=1,2,3,4,5,6\\}.\n\\] En total tendremos \\(6\\cdot 6\\cdot 6=6^3=216\\) resultados posibles.\nConsideremos la variable 3-dimensional \\(\\mathbf{X}=(X_1,X_2,X_3)\\), donde \\(X_1\\) nos da el número de 1’s obtenidos, \\(X_2\\), el número de 2’s y \\(X_3\\), el número de 3’s.\n\nEl conjunto \\(\\mathbf{X}(\\Omega)\\) tiene en total 64 elementos ya que cada componente \\(X_i\\) puede tener en total 4 resultados: 0, 1, 2 o 3. Por tanto el conjunto total de resultados es: \\(4\\cdot 4\\cdot 4=4^3=64\\).\nEl valor de función de distribución conjunta en el resultado \\((0,0,0)\\) es:\n\\[\nF_{X_1X_2X_3}(0,0,0)=p(X_1\\leq 0,\\ X_2\\leq 0,\\ X_3\\leq 0)=\\frac{3^3}{6^3}=\\left(\\frac{1}{2}\\right)^3 =0.125,\n\\] ya que si \\(X_1\\leq 0\\), \\(X_2\\leq 0\\) y \\(X_3\\leq 0\\), significa que no ha salido ni ningún 1, ni ningún 2 ni ningún 3. Sólo pueden salir 4’s, 5’s o 6’s y existen \\(3\\cdot 3\\cdot 3=3^3=27\\) posibilidades de que esto pase entre \\(6^3=216\\) posibilidades posibles."
  },
  {
    "objectID": "6.html#variables-aleatorias-n-dimensionales-discretas",
    "href": "6.html#variables-aleatorias-n-dimensionales-discretas",
    "title": "7  Vectores aleatorios",
    "section": "7.3 Variables aleatorias \\(n\\)-dimensionales discretas",
    "text": "7.3 Variables aleatorias \\(n\\)-dimensionales discretas\nDefinición de variable aleatoria \\(n\\)-dimensional discreta: Sea \\((X_1,\\ldots,X_n)\\) una variable aleatoria \\(n\\)-dimensional. Diremos que es discreta cuando su conjunto de valores en \\(\\mathbb{R}^n\\), \\((X_1,\\ldots,X_n)(\\Omega)\\) es un conjunto finito o numerable.\nEn la mayoría de los casos, dicho conjunto es un subconjunto de los enteros naturales.\n\nEjemplo\nLa variable aleatoria 3-dimensional anterior que nos daba el número de 1’s obtenidos, el número de 2’s y el número de 3’s es discreta ya que\n\n\\[\n\\begin{array}{rl}\n\\mathbf{X}(\\Omega) =& \\{\n  (0,0,0),(1,0,0),(0,1,0),(0,0,1),(2,0,0),(0,2,0),\\\\\n& (0,0,2),(3,0,0),(0,3,0),(0,0,3),(0,1,1),(1,0,1),\\\\\n&  (1,1,0),(0,1,2),(0,2,1),(1,0,2),(2,0,1),(1,2,0),\\\\\n&  (2,1,0),(0,1,3),(0,3,1),(1,0,3),(3,0,1),(1,3,0),\\\\\n&  (3,1,0),(0,2,2),(2,0,2),(2,2,0),(0,2,3),(3,2,0),\\\\\n&  (2,0,3),(3,0,2),(2,3,0),(3,2,0),(0,3,3),(3,0,3),\\\\\n&  (3,3,0),(1,1,1),(1,1,2),(1,2,1),(2,1,1),(1,1,3),\\\\\n& (1,3,1),(3,1,1),(1,2,2),(2,1,2),(2,2,1),(1,2,3),\\\\\n& (2,1,3),(1,3,2),(3,1,2),(2,3,1),(3,2,1),(1,3,3),\\\\\n& (3,1,3),(3,3,1),(2,2,2),(2,2,3),(2,3,2),(3,2,2),\\\\\n& (2,3,3),(3,2,3),(3,3,2),(3,3,3)\\}.\n\\end{array}\n\\]\n\n\n\n7.3.1 Función de probabilidad conjunta\nDefinición de función de probabilidad conjunta: Dada una variable aleatoria \\(n\\)-dimensional discreta \\((X_1\\ldots,X_n)\\) con \\((X_1\\ldots,X_n)(\\Omega)=\\{(x_{i_1},x_{i_2},\\ldots,x_{i_n}),\\ i_1=1,2,\\ldots,\\ i_n=1,2,\\ldots,\\}\\), definimos la función de probabilidad discreta \\(P_{X_1\\ldots X_n}\\) para un valor \\((x_{i_1},x_{i_2},\\ldots,x_{i_n})\\in\\mathbb{R}^n\\) de la siguiente forma:\n\\[\n\\begin{array}{rl}\nP_{X_1\\ldots X_n}: \\mathbb{R}^n & \\longrightarrow \\mathbb{R}\\\\\n(x_{i_1},x_{i_2},\\ldots,x_{i_n}) & \\longrightarrow P_{X_1\\ldots X_n}(x_{i_1},x_{i_2},\\ldots,x_{i_n})=P(X= x_{i_1},\\ldots X_n= x_{i_n}).\n\\end{array}\n\\]\nObservación: Si \\((x_{i_1},x_{i_2},\\ldots,x_{i_n})\\not\\in (X_1\\ldots,X_n)(\\Omega)\\), el valor de la función de probabilidad conjunta en \\((x_{i_1},x_{i_2},\\ldots,x_{i_n})\\) en nulo: \\(P_{X_1\\ldots X_n}(x_{i_1},x_{i_2},\\ldots,x_{i_n})=0\\), ya que, en este caso, el conjunto \\(\\{w\\in\\Omega,\\ | (X_1(w),\\ldots,X_n(w))=(x_{i_1},x_{i_2},\\ldots,x_{i_n})\\}=\\emptyset\\) ya que recordemos \\((x_{i_1},x_{i_2},\\ldots,x_{i_n})\\not\\in (X_1\\ldots,X_n)(\\Omega)\\).\nPor tanto, de cara a calcular \\(P_{X_1\\ldots X_n}\\) basta calcular \\(P_{X_1\\ldots X_n}(x_{i_1},\\ldots,x_{i_n})\\) para \\((x_{i_1},\\ldots,x_{i_n})\\in (X_1\\ldots,X_n)(\\Omega)\\).\nLos valores de \\(P_{X_1\\ldots X_n}(x_{i_1},\\ldots,x_{i_n})\\) estarían organizados en una tabla \\(n\\)-dimensional.\n\nEjemplo de la variable 3-dimensional que nos da el número de 1’s, 2’s y 3’s en el lanzamiento de un dado tres veces\nPara mostrar la función de probabilidad conjunta haremos una tabla bidimensional para cada valor de \\(X_3\\).\nComo \\(X_3(\\Omega)=\\{0,1,2,3\\}\\), en total mostraremos 4 tablas bidimensionales.\n\nTabla para \\(X_3=0\\):\n\n\n\n\n\\(X_1/X_2\\)\n0\n1\n2\n3\n\n\n\n\n0\n\\(\\frac{1}{8}\\)\n\\(\\frac{1}{8}\\)\n\\(\\frac{1}{24}\\)\n\\(\\frac{1}{216}\\)\n\n\n1\n\\(\\frac{1}{8}\\)\n\\(\\frac{1}{12}\\)\n\\(\\frac{1}{72}\\)\n\\(0\\)\n\n\n2\n\\(\\frac{1}{24}\\)\n\\(\\frac{1}{72}\\)\n\\(0\\)\n\\(0\\)\n\n\n3\n\\(\\frac{1}{216}\\)\n\\(0\\)\n\\(0\\)\n\\(0\\)\n\n\n\n\nTabla para \\(X_3=1\\):\n\n\n\n\n\\(X_1/X_2\\)\n0\n1\n2\n3\n\n\n\n\n0\n\\(\\frac{1}{8}\\)\n\\(\\frac{1}{12}\\)\n\\(\\frac{1}{72}\\)\n\\(0\\)\n\n\n1\n\\(\\frac{1}{12}\\)\n\\(\\frac{1}{36}\\)\n\\(0\\)\n\\(0\\)\n\n\n2\n\\(\\frac{1}{72}\\)\n\\(0\\)\n\\(0\\)\n\\(0\\)\n\n\n3\n\\(0\\)\n\\(0\\)\n\\(0\\)\n\\(0\\)\n\n\n\n\nTabla para \\(X_3=2\\):\n\n\n\n\n\\(X_1/X_2\\)\n0\n1\n2\n3\n\n\n\n\n0\n\\(\\frac{1}{24}\\)\n\\(\\frac{1}{72}\\)\n\\(0\\)\n\\(0\\)\n\n\n1\n\\(\\frac{1}{72}\\)\n\\(0\\)\n\\(0\\)\n\\(0\\)\n\n\n2\n\\(0\\)\n\\(0\\)\n\\(0\\)\n\\(0\\)\n\n\n3\n\\(0\\)\n\\(0\\)\n\\(0\\)\n\\(0\\)\n\n\n\n\nTabla para \\(X_3=3\\):\n\n\n\n\n\\(X_1/X_2\\)\n0\n1\n2\n3\n\n\n\n\n0\n\\(\\frac{1}{216}\\)\n\\(0\\)\n\\(0\\)\n\\(0\\)\n\n\n1\n\\(0\\)\n\\(0\\)\n\\(0\\)\n\\(0\\)\n\n\n2\n\\(0\\)\n\\(0\\)\n\\(0\\)\n\\(0\\)\n\n\n3\n\\(0\\)\n\\(0\\)\n\\(0\\)\n\\(0\\)\n\n\n\n\n\nLa función fun.prod.con nos da la función de probabilidad conjunta de la variable aleatoria \\(\\mathbf{X}\\) cuando lanzamos un dado tres veces:\n\nfun.prob.con=function(x1,x2,x3){\n  n=6\n  cuenta.1 =function(x){length(x[x==1])}\n  cuenta.2 =function(x){length(x[x==2])}\n  cuenta.3 =function(x){length(x[x==3])}\n  Dxyz=data.frame(d1=rep(1:n,each=n),d2=rep(1:n,times=n),\n                  d3=rep(1:n,each=n*n))\n  X1=apply(Dxyz,1,cuenta.1)\n  X2=apply(Dxyz,1,cuenta.2)\n  X3=apply(Dxyz,1,cuenta.3)\n  frecuencia = table(X1==x1 & X2==x2 & X3==x3)\n  res=ifelse(length(frecuencia)==2,frecuencia[2],0)\n  return(res/6^3)\n}\n\nPara construir la tabla de la función de probabilidad conjunta para la variable \\(\\mathbf{X}=(X_1,X_2,X_3)\\) con \\(X_3=0\\) hacemos lo siguiente:\n\nvalores.variables=0:3\ntabla.0 = c()\nfor (i in 1:length(valores.variables)){\n  for (j in 1:length(valores.variables)){\n  tabla.0=c(tabla.0,\n            fun.prob.con(valores.variables[i],\n                         valores.variables[j],0));\n  }\n  }\ntabla.0 = matrix(tabla.0,length(valores.variables),\n                 length(valores.variables))\nrownames(tabla.0)=valores.variables\ncolnames(tabla.0)=valores.variables\nknitr::kable(tabla.0)\n\nCon los demás valores de \\(X_3\\), lo haríamos de forma similar.\nTabla con \\(X_3=0\\):\n\n\n\n\n\n\n0\n1\n2\n3\n\n\n\n\n0\n0.1250000\n0.1250000\n0.0416667\n0.0046296\n\n\n1\n0.1250000\n0.0833333\n0.0138889\n0.0000000\n\n\n2\n0.0416667\n0.0138889\n0.0000000\n0.0000000\n\n\n3\n0.0046296\n0.0000000\n0.0000000\n0.0000000\n\n\n\n\n\nTabla con \\(X_3=1\\):\n\n\n\n\n\n\n0\n1\n2\n3\n\n\n\n\n0\n0.1250000\n0.0833333\n0.0138889\n0\n\n\n1\n0.0833333\n0.0277778\n0.0000000\n0\n\n\n2\n0.0138889\n0.0000000\n0.0000000\n0\n\n\n3\n0.0000000\n0.0000000\n0.0000000\n0\n\n\n\n\n\nTabla con \\(X_3=2\\):\n\n\n\n\n\n\n0\n1\n2\n3\n\n\n\n\n0\n0.0416667\n0.0138889\n0\n0\n\n\n1\n0.0138889\n0.0000000\n0\n0\n\n\n2\n0.0000000\n0.0000000\n0\n0\n\n\n3\n0.0000000\n0.0000000\n0\n0\n\n\n\n\n\nTabla con \\(X_3=3\\):\n\n\n\n\n\n\n0\n1\n2\n3\n\n\n\n\n0\n0.0046296\n0\n0\n0\n\n\n1\n0.0000000\n0\n0\n0\n\n\n2\n0.0000000\n0\n0\n0\n\n\n3\n0.0000000\n0\n0\n0"
  },
  {
    "objectID": "6.html#variables-aleatorias-n-dimensionales-continuas",
    "href": "6.html#variables-aleatorias-n-dimensionales-continuas",
    "title": "7  Vectores aleatorios",
    "section": "7.4 Variables aleatorias \\(n\\)-dimensionales continuas",
    "text": "7.4 Variables aleatorias \\(n\\)-dimensionales continuas\n\n7.4.1 Definición\nRecordemos la definición de variable continua bidimensional: \\((X,Y)\\) es continua si existe una función \\(f_{XY}:\\mathbb{R}^2\\longrightarrow \\mathbb{R}\\), llamada función de densidad conjunta no negativa \\(f_{XY}(x,y)\\geq 0\\), para todo \\((x,y)\\in\\mathbb{R}^2\\) tal que para cualquier región \\(B\\) del plano, la probabilidad de que \\((X,Y)\\) esté en \\(B\\) se calcula de la forma siguiente: \\[\nP((X,Y)\\in B)=\\int\\int_B f_{XY}(x,y)\\,dx\\, dy.\n\\]\nLa generalización natural es, entonces:\nDefinición de variable aleatoria \\(n\\)-dimensional continua.  Sea \\((X_1\\ldots,X_n)\\) una variable aleatoria \\(n\\)-dimensional. Diremos que \\((X_1\\ldots,X_n)\\) es continua si existe una función \\(f_{X_1\\ldots X_n}:\\mathbb{R}^n\\longrightarrow \\mathbb{R}\\) llamada función de densidad conjunta no negativa \\(f_{X_1\\ldots X_n}(x_1,\\ldots,x_n)\\geq 0\\) para todo \\((x_1,\\ldots,x_n)\\in\\mathbb{R}^n\\) tal que dado cualquier región \\(B\\) del espacio \\(n\\)-dimensional, la probabilidad de que \\((X_1\\ldots,X_n)\\) esté en \\(B\\) se calcula de la forma siguiente: \\[\nP((X_1\\ldots,X_n)\\in B)=\\int\\cdots\\int_B f_{X_1\\ldots X_n}(x_1,\\ldots,x_n)\\,dx_1\\cdots\\,dx_n.\n\\]\n\n7.4.1.1 Ejemplos\n\nEjemplo\nConsideremos una variable aleatoria \\(3\\)-dimensional \\((X_1,X_2,X_3)\\) con función de densidad conjunta: \\[\nf_{X_1X_2X_3}(x_1,x_2,x_3)=\\begin{cases}\n8 x_1\\cdot x_2\\cdot x_3, & \\mbox{si }0\\leq x_1\\leq 1,\\ 0\\leq x_2\\leq 1,\\ 0\\leq x_3\\leq 1, \\\\\n0, & \\mbox{en caso contrario.}\\\\\n\\end{cases}\n\\]\n\nEn la figura siguiente hemos dibujado en rosa la región donde \\(f_{X_1X_2X_3}\\) no es cero, es decir \\([0,1]\\times [0,1]\\times [0,1]\\).\n\n\n\n\n\n\n\n\n\n\n\n\n7.4.2 Propiedades de la función de densidad\nConsideremos \\((X_1\\ldots,X_n)\\) una variable aleatoria \\(n\\)-dimensional continua con función de densidad conjunta \\(f_{X_1\\ldots X_n}\\). Esta función verifica las propiedades siguientes:\n\nLa integral de dicha función sobre todo el espacio \\(n\\)-dimensional vale 1: \\[\n\\int\\int_{\\mathbb{R}^n} f_{X_1\\ldots X_n}(x_1,\\ldots,x_n)\\,dx_1\\cdots dx_n =1.\n\\] Para ver dicha propiedad, basta considerar \\(B=\\mathbb{R}^n\\), tener en cuenta que el suceso \\((X_1\\ldots,X_n)\\in \\mathbb{R}^n\\) es el total \\(\\Omega\\) y aplicar la definición de \\(f_{X_1\\ldots X_n}\\): \\[\nP((X_1\\ldots,X_n)\\in \\mathbb{R}^n)=1= \\int\\cdots\\int_{\\mathbb{R}^n} f_{X_1\\ldots X_n}(x_1,\\ldots,x_n)\\,dx_1\\cdots dx_n.\n\\]\nLa relación que hay entre la función de distribución conjunta \\(F_{X_1\\ldots X_n}\\) y la función de densidad conjunta \\(f_{X_1\\ldots X_n}\\) es la siguiente: \\[\nF_{X_1\\ldots X_n}(x_1,\\ldots,x_n)=\\int_{-\\infty}^{x_1}\\cdots\\int_{-\\infty}^{x_n} f_{X_1\\ldots X_n}(u_1,\\ldots,u_n)\\,du_1\\cdots du_n.\n\\] Para ver dicha propiedad, basta considerar \\(B=(-\\infty,x_1]\\times\\cdots\\times (-\\infty,x_n]\\) y aplicar la definición de función de distribución conjunta: \\[\n\\begin{array}{rl}\n& F_{X_1\\ldots X_n}(x_1,\\ldots,x_n)=P((X_1\\ldots,X_n)\\in (-\\infty,x_1]\\times\\cdots (-\\infty,x_n])\\\\ &\\qquad =\\int_{-\\infty}^{x_1}\\cdots\\int_{-\\infty}^{x_n} f_{X_1\\ldots X_n}(u_1,\\ldots,u_n)\\,du_1\\cdots du_n.\n\\end{array}\n\\]\nLa relación que hay entre la función de densidad \\(F_{X_1\\ldots X_n}\\) y la función de distribución \\(f_{X_1\\ldots X_n}\\) es la siguiente: \\[\nf_{X_1\\ldots X_n}(x_1,\\ldots,x_n)=\\frac{\\partial^n F_{X_1\\ldots X_n}(x_1,\\ldots,x_n)}{\\partial x_1\\cdots\\partial x_n}.\n\\] Dicha propiedad se deduce de la anterior, derivando primero respecto a \\(x_1\\), después respecto a \\(x_2\\) y sucesivamente hasta llegar a \\(x_n\\) para eliminar las \\(n\\) integrales.\nLa función de densidad marginal de la variable \\(k\\) dimensional \\((X_{s_1},\\ldots,X_{s_k})\\) con \\(\\{s_1,\\ldots, s_k\\}\\) un subconjunto de \\(\\{1,\\ldots,n\\}\\), \\(f_{X_{s_1}\\ldots,X_{s_k}}\\) se calculan de la forma siguiente: \\[\nf_{X_{s_1}\\ldots,X_{s_k}}(x_{s_1},\\ldots,x_{s_k})=\\int_{x_{t_1}=-\\infty}^{x_{t_1}=\\infty}\\cdots \\int_{x_{t_{n-k}}=-\\infty}^{x_{t_{n-k}}=\\infty} f_{X_1\\ldots X_n}(x_1,\\ldots,x_n)\\, dx_{t_1}\\cdots dx_{t_{n-k}},\n\\] con \\(\\{t_1,\\ldots,t_{n-k}\\}=\\{1,\\ldots,n\\}\\setminus \\{s_1,\\ldots,s_k\\}.\\) Es decir, las variables \\(t\\)’s son las que no aparecen en la definición de la variable aleatoria \\(k\\) dimensional \\((X_{s_1},\\ldots,X_{s_k})\\).\n\n\n7.4.2.1 Ejemplos\n\nEjemplo anterior\nComprobemos las propiedades usando la función de densidad del ejemplo anterior:\n\\[\nf_{X_1X_2X_3}(x_1,x_2,x_3)=\\begin{cases}\n8 x_1\\cdot x_2\\cdot x_3, & \\mbox{si }0\\leq x_1\\leq 1,\\ 0\\leq x_2\\leq 1,\\ 0\\leq x_3\\leq 1, \\\\\n0, & \\mbox{en caso contrario.}\\\\\n\\end{cases}\n\\]\n\nLa integral de \\(f_{X_1X_2X_3}\\) sobre todo el espacio 3D vale 1:\n\\[\n\\begin{array}{rl}\n& \\int\\int\\int_{\\mathbb{R}^3} f_{X_1X_2X_3}(x_1,x_2,x_3)\\,dx\\, dy=\\int_0^1\\int_0^1\\int_0^1 8 x_1\\cdot x_2\\cdot x_3\\, dx_1\\,dx_2\\,dx_3\\\\ & \\qquad=8\\int_0^1 x_1\\, dx_1\\int_0^1 x_2\\, dx_2\\int_0^1 x_3\\,dx_3=8\\left[\\frac{x_1^2}{2}\\right]_0^1\\cdot\\left[\\frac{x_2^2}{2}\\right]_0^1\\cdot \\left[\\frac{x_3^2}{2}\\right]_0^1\\\\\n& \\qquad = 8\\cdot\\left(\\frac{1}{2}\\right)^3 =1.\n\\end{array}\n\\]\nVamos a calcular la función de distribución \\(F_{X_1X_2X_3}\\).\nRecordemos que la expresión de la función de distribución en función de la función de densidad era:\n\\[\nF_{X_1X_2X_3}(x_1,x_2,x_3)=\\int_{-\\infty}^{x_1}\\int_{-\\infty}^{x_2}\\int_{-\\infty}^{x_3}f_{X_1X_2X_3}(u_1,u_2,u_3)\\,du_1\\, du_2\\, du_3.\n\\] Como la región del espacio 3D donde \\(f_{X_1X_2X_3}(x_1,x_2,x_3)\\) es no nula es el cubo unidad \\([0,1]\\times [0,1]\\times [0,1]\\), fijado un punto del espacio \\((x_1,x_2,x_3)\\) es fundamental calcular la intersección de dicho cubo unidad con la región \\((-\\infty,x_1]\\times (-\\infty,x_2]\\times (-\\infty,x_3]\\).\nDicha intersección \\(([0,1]\\times [0,1]\\times [0,1])\\cap ((-\\infty,x_1]\\times (-\\infty,x_2]\\times (-\\infty,x_3])\\) es la región donde tendremos que integrar la función de densidad para hallar la función de distribución en el punto \\((x_1,x_2,x_3)\\).\nPara hallar la región anterior, vamos a dividir el espacio 3D en tres “pisos”:\n\n“Sótano” o zona donde \\(x_3&lt;0\\).\n“Planta baja” o zona donde \\(0\\leq x_3\\leq 1\\).\n“Primer piso” o zona donde \\(x_3&gt;1\\).\nSi \\((x_1,x_2,x_3)\\) está en el “sótano” o \\(x_3&lt;0\\), claramente, \\(([0,1]\\times [0,1]\\times [0,1])\\cap ((-\\infty,x_1]\\times (-\\infty,x_2]\\times (-\\infty,x_3])=\\emptyset\\). Por tanto, \\(F_{X_1X_2X_3}(x_1,x_2,x_3)=0\\).\nSi \\((x_1,x_2,x_3)\\) está en la planta baja o \\(0\\leq x_3\\leq 1\\), vamos a distinguir cuatro casos dependiendo de los valores de \\(x_1\\) y \\(x_2\\):\n\n\\(x_1 &lt;0\\) o \\(x_2 &lt;0\\). En este caso, \\(([0,1]\\times [0,1]\\times [0,1])\\cap ((-\\infty,x_1]\\times (-\\infty,x_2]\\times (-\\infty,x_3])=\\emptyset\\). Por tanto, \\(F_{X_1X_2X_3}(x_1,x_2,x_3)=0\\).\n\\(0\\leq x_1\\leq 1\\) y \\(0\\leq x_2\\leq 1\\). En este caso: \\(([0,1]\\times [0,1]\\times [0,1])\\cap ((-\\infty,x_1]\\times (-\\infty,x_2]\\times (-\\infty,x_3])\\) que es igual a \\([0,x_1]\\times [0,x_2]\\times [0,x_3]\\), ver figura adjunta.\n\n\nPor tanto, \\[\n\\begin{array}{rl}\nF_{X_1X_2X_3}(x_1,x_2,x_3) & =\\int_{0}^{x_1}\\int_{0}^{x_2}\\int_{0}^{x_3} 8 x_1 x_2 x_3 dx_1\\, dx_2\\ dx_3\\\\\n& = 8\\left[\\frac{x_1^2}{2}\\right]_0^{x_1}\\left[\\frac{x_2^2}{2}\\right]_0^{x_2}\\left[\\frac{x_3^2}{2}\\right]_0^{x_3} = x_1^2 x_2^2 x_3^2.\n\\end{array}\n\\]\n\n\n\n\n\n\n\n\n\nSeguimos en la planta “baja”,\n\nSi \\(x_1 &gt;1\\) y \\(0\\leq x_2\\leq 1\\), \\(([0,1]\\times [0,1]\\times [0,1])\\cap ((-\\infty,x_1]\\times (-\\infty,x_2]\\times (-\\infty,x_3])\\) que es igual a \\([0,1]\\times [0,x_2]\\times [0,x_3]\\), ver figura adjunta. Hemos dibujado sólo la parte “positiva” de la región \\((-\\infty,x_1]\\times (-\\infty,x_2]\\times (-\\infty,x_3]\\) ya que la parte “negativa” claramente no interseca con \\([0,1]\\times [0,1]\\times [0,1]\\) para no complicar demasiado la figura.\n\nEn este caso,\n\\[\n\\begin{array}{rl}\nF_{X_1X_2X_3}(x_1,x_2,x_3) & =\\int_{0}^{1}\\int_{0}^{x_2}\\int_{0}^{x_3} 8 x_1 x_2 x_3 dx_1\\, dx_2\\ dx_3 \\\\\n& =\n8\\left[\\frac{x_1^2}{2}\\right]_0^{1}\\left[\\frac{x_2^2}{2}\\right]_0^{x_2}\\left[\\frac{x_3^2}{2}\\right]_0^{x_3} = x_2^2 x_3^2.\n\\end{array}\n\\]\n\nSi \\(0\\leq x_1\\) y \\(x_2&gt;1\\), es un caso parecido al caso anterior pero “cambiando los papeles” de \\(x_1\\) y \\(x_2\\). Por tanto,\n\n\\[\nF_{X_1X_2X_3}(x_1,x_2,x_3)=x_1^2 x_3^2.\n\\]\n\n\n\n\n\n\n\n\n\n\n\nSeguimos en la planta “baja”,\n\nSi \\(x_1&gt;1\\) y \\(x_2&gt;1\\), entonces \\(([0,1]\\times [0,1]\\times [0,1])\\cap ((-\\infty,x_1]\\times (-\\infty,x_2]\\times (-\\infty,x_3])\\) que es igual a \\([0,1]\\times [0,1]\\times [0,x_3]\\), ver figura adjunta. También hemos dibujado sólo la parte “positiva” de la región \\((-\\infty,x_1]\\times (-\\infty,x_2]\\times (-\\infty,x_3]\\) ya que la parte “negativa” claramente no interseca con \\([0,1]\\times [0,1]\\times [0,1]\\) para no complicar demasiado la figura.\n\nEn este caso, \\[\nF_{X_1X_2X_3}(x_1,x_2,x_3)=\\int_{0}^{1}\\int_{0}^{1}\\int_{0}^{x_3} 8 x_1 x_2 x_3 dx_1\\, dx_2\\ dx_3 =\n8\\left[\\frac{x_1^2}{2}\\right]_0^{1}\\left[\\frac{x_2^2}{2}\\right]_0^{1}\\left[\\frac{x_3^2}{2}\\right]_0^{x_3} = x_3^2.\n\\]\n\n\n\n\n\n\n\nSupongamos ahora que \\((x_1,x_2,x_3)\\) está en el “primer piso” o \\(x_3&gt;1\\). Aquí también vamos a distinguir 4 casos:\n\n\\(x_1 &lt;0\\) o \\(x_2 &lt;0\\). En este caso, \\(([0,1]\\times [0,1]\\times [0,1])\\cap ((-\\infty,x_1]\\times (-\\infty,x_2]\\times (-\\infty,x_3])=\\emptyset\\). Por tanto, \\(F_{X_1X_2X_3}(x_1,x_2,x_3)=0\\).\n\\(0\\leq x_1\\leq 1\\) y \\(0\\leq x_2\\leq 1\\). En este caso: \\(([0,1]\\times [0,1]\\times [0,1])\\cap ((-\\infty,x_1]\\times (-\\infty,x_2]\\times (-\\infty,x_3])=[0,x_1]\\times [0,x_2]\\times [0,1]\\), ver figura adjunta.\n\nEn este caso,\n\\[\n\\begin{array}{rl}\nF_{X_1X_2X_3}(x_1,x_2,x_3) & =\\int_{0}^{x_1}\\int_{0}^{x_2}\\int_{0}^{1} 8 x_1 x_2 x_3 dx_1\\, dx_2\\ dx_3\\\\\n& = 8\\left[\\frac{x_1^2}{2}\\right]_0^{x_1}\\left[\\frac{x_2^2}{2}\\right]_0^{x_2}\\left[\\frac{x_3^2}{2}\\right]_0^{1} = x_1^2 x_2^2.\n\\end{array}\n\\]\n\n\n\n\n\n\ncenter\n\n\n\n\n\nSeguimos en el “primer piso”,\n\nSi \\(x_1 &gt;1\\) y \\(0\\leq x_2\\leq 1\\), \\(([0,1]\\times [0,1]\\times [0,1])\\cap ((-\\infty,x_1]\\times (-\\infty,x_2]\\times (-\\infty,x_3])\\) que es igual a \\([0,1]\\times [0,x_2]\\times [0,1]\\), ver figura adjunta.\n\nEn este caso,\n\\[\nF_{X_1X_2X_3}(x_1,x_2,x_3)=\\int_{0}^{1}\\int_{0}^{x_2}\\int_{0}^{1} 8 x_1 x_2 x_3 dx_1\\, dx_2\\ dx_3 =\n8\\left[\\frac{x_1^2}{2}\\right]_0^{1}\\left[\\frac{x_2^2}{2}\\right]_0^{x_2}\\left[\\frac{x_3^2}{2}\\right]_0^{1} = x_2^2.\n\\]\n\nSi \\(0\\leq x_1 \\leq 1\\) y $ x_2&gt; 1$. Este caso es parecido al caso anterior pero “cambiando los papeles” de \\(x_1\\) y \\(x_2\\). Por tanto,\n\n\\[\nF_{X_1X_2X_3}(x_1,x_2,x_3)=x_1^2.\n\\]\n\n\n\n\n\n\n\n\n\n\n\nSeguimos en el “primer piso”,\n\nSi \\(x_1&gt;1\\) y \\(x_2&gt;1\\), \\(([0,1]\\times [0,1]\\times [0,1])\\cap ((-\\infty,x_1]\\times (-\\infty,x_2]\\times (-\\infty,x_3])\\) que es iguala a \\([0,1]\\times [0,1]\\times [0,1]\\), ver figura adjunta.\n\nEn este caso, \\[\n\\begin{array}{rl}\nF_{X_1X_2X_3}(x_1,x_2,x_3)& =\\int_{0}^{1}\\int_{0}^{1}\\int_{0}^{1} 8 x_1 x_2 x_3 dx_1\\, dx_2\\ dx_3 \\\\\n& = 8\\left[\\frac{x_1^2}{2}\\right]_0^{1}\\left[\\frac{x_2^2}{2}\\right]_0^{1}\\left[\\frac{x_3^2}{2}\\right]_0^{1} = 1.\n\\end{array}\n\\]\n\n\n\n\n\n\n\n\n\n\n\nEn resumen: \\[\nF_{X_1X_2X_3}(x_1,x_2,x_3)=\\begin{cases}\n0, & \\mbox{si }x_1&lt;0,\\mbox{ o }x_2&lt;0,\\mbox{ o }x_3 &lt;0\\\\\nx_1^2\\cdot x_2^2\\cdot x_3^2, & \\mbox{si }0\\leq x_1\\leq 1,\\ 0\\leq x_2\\leq 1,\\ 0\\leq x_3\\leq 1, \\\\\nx_2^2\\cdot x_3^2, & \\mbox{si }x_1&gt; 1,\\ 0\\leq x_2\\leq  1,\\ 0\\leq x_3\\leq  1, \\\\\nx_1^2\\cdot x_3^2, & \\mbox{si }0\\leq x_1\\leq  1,\\ x_2&gt; 1,\\ \\ 0\\leq x_3\\leq  1, \\\\\nx_3^2, & \\mbox{si }x_1&gt; 1,\\ x_2&gt; 1,\\ \\ 0\\leq x_3\\leq  1, \\\\\nx_1^2\\cdot x_2^2, & \\mbox{si }0\\leq x_1\\leq  1,\\ 0\\leq x_2\\leq  1,\\ x_3&gt; 1,\\\\\nx_1^2, & \\mbox{si }0\\leq x_1\\leq  1,\\ x_2 &gt;  1,\\ x_3&gt; 1,\\\\\nx_2^2, & \\mbox{si }x_1&gt;1,\\ 0\\leq x_2\\leq  1,\\ x_3&gt; 1,\\\\\n1, & \\mbox{si }x_1\\geq 1,\\ x_2\\geq 1,\\ x_3\\geq 1.\n\\end{cases}\n\\]\nDicha función era la función que nos sirvió como ejemplo a la hora de introducir la variables aleatorias \\(n\\)-dimensionales. Ahora sabemos que es continua y conocemos su función de densidad.\nComprobemos seguidamente que si derivamos tres veces la expresión de \\(F_{X_1X_2X_3}\\), primero respecto \\(x_1\\), luego respecto \\(x_2\\) y finalmente respecto \\(x_3\\), obtendremos la función de densidad \\(f_{X_1X_2X_3}\\).\nSi derivamos respecto \\(x_1\\) obtenemos: \\[\n\\frac{\\partial F_{X_1X_2X_3}(x_1,x_2,x_3)}{\\partial x_1}=\\begin{cases}\n0, & \\mbox{si }x_1&lt;0,\\mbox{ o }x_2&lt;0,\\mbox{ o }x_3 &lt;0\\\\\n2 x_1\\cdot x_2^2\\cdot x_3^2, & \\mbox{si }0\\leq x_1\\leq 1,\\ 0\\leq x_2\\leq 1,\\ 0\\leq x_3\\leq 1, \\\\\n0, & \\mbox{si }x_1&gt; 1,\\ 0\\leq x_2\\leq  1,\\ 0\\leq x_3\\leq  1, \\\\\n2 x_1 \\cdot x_3^2, & \\mbox{si }0\\leq x_1\\leq  1,\\ x_2&gt; 1,\\ \\ 0\\leq x_3\\leq  1, \\\\\n0, & \\mbox{si }x_1&gt; 1,\\ x_2&gt; 1,\\ \\ 0\\leq x_3\\leq  1, \\\\\n2 x_1\\cdot x_2^2, & \\mbox{si }0\\leq x_1\\leq  1,\\ 0\\leq x_2\\leq  1,\\ x_3&gt; 1,\\\\\n2 x_1, & \\mbox{si }0\\leq x_1\\leq  1,\\ x_2 &gt;  1,\\ x_3&gt; 1,\\\\\n0, & \\mbox{si }x_1&gt;1,\\ 0\\leq x_2\\leq  1,\\ x_3&gt; 1,\\\\\n0, & \\mbox{si }x_1\\geq 1,\\ x_2\\geq 1,\\ x_3\\geq 1.\n\\end{cases}\n\\]\nSi ahora derivamos respecto \\(x_2\\) obtenemos: \\[\n\\frac{\\partial^2 F_{X_1X_2X_3}(x_1,x_2,x_3)}{\\partial x_2\\partial x_1}=\\begin{cases}\n0, & \\mbox{si }x_1&lt;0,\\mbox{ o }x_2&lt;0,\\mbox{ o }x_3 &lt;0\\\\\n4 x_1\\cdot x_2\\cdot x_3^2, & \\mbox{si }0\\leq x_1\\leq 1,\\ 0\\leq x_2\\leq 1,\\ 0\\leq x_3\\leq 1, \\\\\n0, & \\mbox{si }x_1&gt; 1,\\ 0\\leq x_2\\leq  1,\\ 0\\leq x_3\\leq  1, \\\\\n0, & \\mbox{si }0\\leq x_1\\leq  1,\\ x_2&gt; 1,\\ \\ 0\\leq x_3\\leq  1, \\\\\n0, & \\mbox{si }x_1&gt; 1,\\ x_2&gt; 1,\\ \\ 0\\leq x_3\\leq  1, \\\\\n4 x_1\\cdot x_2, & \\mbox{si }0\\leq x_1\\leq  1,\\ 0\\leq x_2\\leq  1,\\ x_3&gt; 1,\\\\\n0, & \\mbox{si }0\\leq x_1\\leq  1,\\ x_2 &gt;  1,\\ x_3&gt; 1,\\\\\n0, & \\mbox{si }x_1&gt;1,\\ 0\\leq x_2\\leq  1,\\ x_3&gt; 1,\\\\\n0, & \\mbox{si }x_1\\geq 1,\\ x_2\\geq 1,\\ x_3\\geq 1.\n\\end{cases}\n\\]\nPor último, si derivamos respecto \\(x_3\\), obtenemos: \\[\n\\frac{\\partial^3 F_{X_1X_2X_3}(x_1,x_2,x_3)}{\\partial x_3\\partial x_2\\partial x_1}=\\begin{cases}\n0, & \\mbox{si }x_1&lt;0,\\mbox{ o }x_2&lt;0,\\mbox{ o }x_3 &lt;0\\\\\n8 x_1\\cdot x_2\\cdot x_3, & \\mbox{si }0\\leq x_1\\leq 1,\\ 0\\leq x_2\\leq 1,\\ 0\\leq x_3\\leq 1, \\\\\n0, & \\mbox{si }x_1&gt; 1,\\ 0\\leq x_2\\leq  1,\\ 0\\leq x_3\\leq  1, \\\\\n0, & \\mbox{si }0\\leq x_1\\leq  1,\\ x_2&gt; 1,\\ \\ 0\\leq x_3\\leq  1, \\\\\n0, & \\mbox{si }x_1&gt; 1,\\ x_2&gt; 1,\\ \\ 0\\leq x_3\\leq  1, \\\\\n0, & \\mbox{si }0\\leq x_1\\leq  1,\\ 0\\leq x_2\\leq  1,\\ x_3&gt; 1,\\\\\n0, & \\mbox{si }0\\leq x_1\\leq  1,\\ x_2 &gt;  1,\\ x_3&gt; 1,\\\\\n0, & \\mbox{si }x_1&gt;1,\\ 0\\leq x_2\\leq  1,\\ x_3&gt; 1,\\\\\n0, & \\mbox{si }x_1\\geq 1,\\ x_2\\geq 1,\\ x_3\\geq 1,\n\\end{cases}\n\\] expresión que coincide con la función de densidad \\(f_{X_1X_2X_3}(x_1,x_2,x_3)\\).\nAcabemos el ejemplo calculando las funciones de densidad marginales de las variables \\(X_1\\), \\(X_2\\), \\(X_3\\), \\((X_1,X_2)\\), \\((X_1,X_3)\\), \\((X_2,X_3)\\).\nDebido a la simetría de la región donde \\(f_{X_1X_2X_3}(x_1,x_2,x_3)\\) no se anula, es suficiente calcular la función de densidad marginal para las variables \\(X_1\\) y \\((X_1,X_2)\\). Para ver las demás, basta cambiar los “papeles” de las variables correspondientes. Por ejemplo, la función de densidad de la variable \\(X_2\\) es la misma que la de la variable \\(X_1\\) cambiando \\(x_1\\) por \\(x_2\\).\nPara hallar la función de densidad de la variable \\(X_1\\), aplicamos la fórmula vista anteriormente: \\[\nf_{X_1}(x_1)=\\int_{-\\infty}^\\infty\\int_{-\\infty}^\\infty  f_{X_1X_2X_3}(x_1,x_2,x_3)\\, dx_2\\, dx_3.\n\\] Recordemos que la región donde no se anulaba la función de densidad conjunta \\(f_{X_1X_2X_3}\\) era el cubo \\([0,1]\\times [0,1]\\times [0,1]\\). Por tanto, fijado \\(x_1\\), el valor de \\(f_{X_1}(x_1)\\) es no nulo si el plano “vertical” \\(X_1=x_1\\) interseca dicho cubo. Y esto ocurre siempre que \\(x_1\\in (0,1)\\). Por tanto, \\[\nf_{X_1}(x_1)=\\begin{cases}\n\\int_{0}^1\\int_0^1 8 x_1x_2 x_3  \\, dx_2\\, dx_3=8x_1\\left[\\frac{x_2^2}{2}\\right]_0^1 \\left[\\frac{x_3^2}{2}\\right]_0^1 =2 x_1, & \\mbox{ si }x_1\\in (0,1),\\\\\n0, & \\mbox{en caso contrario.}\n\\end{cases}\n\\]\nPara hallar la función de densidad conjunta de la variable \\((X_1,X_2)\\), aplicamos la expresión siguiente: \\[\nf_{X_1X_2}(x_1,x_2)=\\int_{-\\infty}^\\infty  f_{X_1X_2X_3}(x_1,x_2,x_3)\\, dx_3.\n\\] En este caso, fijado \\(x_1\\) y \\(x_2\\), tenemos que ver cuando la recta “vertical” \\(X_1=x_1\\), \\(X_2=x_2\\) intersecta el cubo \\([0,1]\\times [0,1]\\times [0,1]\\) y esto ocurre siempre que \\((x_1,x_2)\\in [0,1]\\times [0,1]\\). Por tanto,\n\\[\nf_{X_1X_2}(x_1,x_2)=\\begin{cases}\n\\begin{array}{rl}\n\\int_{0}^1 8 x_1x_2 x_3  \\, dx_3\\\\ & =8x_1x_2\n\\left[\\frac{x_3^2}{2}\\right]_0^1 =4 x_1 x_2\\end{array},\n& \\mbox{ si }(x_1,x_2)\\in [0,1]\\times [0,1],\\\\\n0, & \\mbox{en caso contrario.}\n\\end{cases}\n\\]\n\n\n\n\n\n7.4.3 La distribución gaussiana \\(n\\)-dimensional\nVamos a generalizar la distribución normal a \\(n\\) dimensiones.\nDefinición de distribución gaussiana \\(n\\)-dimensional.  Diremos que la distribución de la variable aleatoria \\(n\\)-dimensional \\((X_1\\ldots,X_n)\\) es gaussiana \\(n\\)-dimensional dependiendo del vector de medias \\(\\mathbf{\\mu}\\) y de la matriz de covarianzas \\(\\Sigma\\) si su función de densidad conjunta es: \\[\n\\begin{array}{rl}\n& f_{X_1\\ldots X_n}(x_1,\\ldots,x_n)=\\frac{1}{(2\\pi)^{\\frac{n}{2}}\\sqrt{\\mathbf{|\\Sigma|}}}\\mathrm{e}^{-\\frac{1}{2}(\\mathbf{x-\\mu})^\\top\\mathbf{\\Sigma}^{-1}(\\mathbf{x-\\mu})},\\\\ & \\qquad  -\\infty &lt;x_1,\\ldots,x_n&lt;\\infty,\n\\end{array}\n\\] Se denota \\(\\mathbf{X}=(X_1,\\ldots,X_n)\\) con \\(\\mathbf{X}={\\cal N}(\\mathbf{\\mu},\\mathbf{\\Sigma})\\), donde \\(\\mathbf{x}=\\begin{pmatrix}x_1\\\\\\vdots\\\\ x_n\\end{pmatrix}\\), \\(\\mathbf{\\mu}=\\begin{pmatrix}\\mu_1\\\\\\vdots\\\\ \\mu_n\\end{pmatrix}\\) es el vector de medias de cada variable aleatoria \\(X_1,\\ldots, X_n\\) y \\(\\mathbf{\\Sigma}\\) es la denominada matriz de covarianzas que nos sirve para estudiar la relación lineal entre las variables \\(X_i\\), \\(i=1,\\ldots, n\\).\nDe hecho la componente \\((i,j)\\) de la matriz de covarianzas \\(\\mathbf{\\Sigma}\\), \\(\\sigma_{ij}\\) es la covarianza entre las variables \\(X_i\\) y \\(X_j\\).\nPor tanto, los elementos de la diagonal de la matriz de covarianzas \\(\\mathbf{\\Sigma}\\), \\(\\sigma_{ii}\\), es las varianzas de las variables \\(X_i\\), \\(i=1,\\ldots,n\\).\nPropiedades de la función de densidad de la variable gaussiana \\(n\\)-dimensional:\n\nPara cualquier punto \\((x_1,\\ldots,x_n)\\in\\mathbb{R}^n\\), la función de densidad es no nula: \\(f_{X_1\\ldots X_n}(x_1,\\ldots,x_n)&gt;0\\).\nLa función de densidad tiene un único máximo absoluto en el punto \\(\\mathbf{\\mu}\\) que vale \\(f_{X_1\\ldots X_n}(\\mathbf{\\mu})=\\frac{1}{(2\\pi)^{\\frac{n}{2}}\\sqrt{\\mathbf{|\\Sigma|}}}\\).\n\nAntes de estudiar cómo son las distribuciones de las marginales de una distribución normal \\(n\\)-dimensional, enunciemos el resultado siguiente:\nProposición (transformación afín de una normal \\(n\\)-dimensional) Sea \\(\\mathbf{X}={\\cal N}(\\mathbf{\\mu},\\mathbf{\\Sigma})\\) una distribución normal \\(n\\)-dimensional. Sea la variable \\(k\\)-dimensional \\(\\mathbf{Y}\\) construida como \\(\\mathbf{Y}=\\mathbf{c}+\\mathbf{C}\\mathbf{X}\\), con \\(\\mathbf{C}\\) una matriz \\(k\\times n\\) y \\(\\mathbf{c}\\) un vector \\(k\\)-dimensional. Entonces la variable \\(Y\\) se distribuye como una variable normal \\(k\\)-dimensional de media \\(\\mathbf{\\mu}_{\\mathbf{Y}}=\\mathbf{c}+\\mathbf{C}\\mathbf{\\mu}\\) y matriz de covarianzas \\(\\mathbf{\\Sigma}_{\\mathbf{Y}}=\\mathbf{C}\\mathbf{\\Sigma}\\mathbf{C}^\\top\\): \\(\\mathbf{Y}={\\cal N}(\\mathbf{c}+\\mathbf{C}\\mathbf{\\mu},\\mathbf{C}\\mathbf{\\Sigma}\\mathbf{C}^\\top)\\).\nUsando la proposición anterior, podemos afirmar:\nProposición (distribución marginal de una variable normal \\(n\\)-dimensional) Sea \\(\\mathbf{X}={\\cal N}(\\mathbf{\\mu},\\mathbf{\\Sigma})\\) una distribución normal \\(n\\)-dimensional. Sea \\((X_{s_1},\\ldots,X_{s_k})\\) la variable \\(k\\) dimensional con las componentes \\(s_1,\\ldots,s_k\\), con \\(s_i\\in\\{1,\\ldots,n\\}\\), entonces la variable \\((X_{s_1},\\ldots,X_{s_k})\\) se distribuye según una normal \\(k\\)-dimensional de media \\((\\mu_{s_1},\\ldots,\\mu_{s_k})\\) y matriz de covarianzas \\(\\mathbf{\\Sigma'}\\) formada por las \\(s_1,\\ldots,s_k\\) filas y columnas de la matriz de covarianzas de la variable \\(\\mathbf{X}\\), \\(\\mathbf{\\Sigma}\\).\nPara ver la proposición anterior a partir de la proposición de la transformación afín, hagamos un ejemplo concreto:\n\nEjemplo: normal multidemisional de dimensión 5\nConsideremos una variable normal \\(5\\)-dimensional de vector de medias general \\(\\mathbf{\\mu}=(\\mu_1,\\mu_2,\\mu_3,\\mu_4,\\mu_5)\\) y matriz de covarianzas \\[\n\\mathbf{\\Sigma}=\\begin{pmatrix}\n\\sigma_{11} & \\sigma_{12} & \\sigma_{13} & \\sigma_{14} & \\sigma_{15} \\\\\n\\sigma_{21} & \\sigma_{22} & \\sigma_{23} & \\sigma_{24} & \\sigma_{25} \\\\\n\\sigma_{31} & \\sigma_{32} & \\sigma_{33} & \\sigma_{34} & \\sigma_{35} \\\\\n\\sigma_{41} & \\sigma_{42} & \\sigma_{43} & \\sigma_{44} & \\sigma_{45} \\\\\n\\sigma_{51} & \\sigma_{52} & \\sigma_{53} & \\sigma_{54} & \\sigma_{55}\n\\end{pmatrix}\n\\]\nQueremos estudiar cuál es la distribución de la variable \\(3\\)-dimensional \\((X_2,X_4,X_5)\\).\n\nPara ello consideramos el vector \\(\\mathbf{c}=0\\) y la matriz \\(\\mathbf{C}\\) siguiente: \\[\n\\mathbf{C}=\\begin{pmatrix}\n0 & 1 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 0 & 1 \\\\\n\\end{pmatrix}\n\\] Esta matriz \\(\\mathbf{C}\\) es de dimensiones\\(3\\times 5\\) y vale 1 en los lugares \\((1,2)\\), \\((2,4)\\) y \\((3,5)\\). Fijémonos que las segundas componentes de los lugares anteriores son precisamente las componentes elegidas de la variable \\(\\mathbf{X}\\).\nLa matriz \\(\\mathbf{Y}=\\mathbf{C}\\cdot \\mathbf{X}=\\begin{pmatrix}X_2\\\\X_4\\\\X_5\\end{pmatrix}\\) vale precisamente la variable marginal que queremos estudiar.\nAplicando la proposición de la transformación afín, podemos afirmar que la distribución de la variable \\(\\mathbf{Y}=\\begin{pmatrix}X_2\\\\X_4\\\\X_5\\end{pmatrix}\\) es una normal \\(3\\) dimensional de vector de medias \\(\\mu_{\\mathbf{Y}}=\\mathbf{C}\\mathbf{\\mu}=\\begin{pmatrix}\\mu_2\\\\\\mu_4\\\\\\mu_5\\end{pmatrix}\\) y vector de covarianzas\n\\[\n\\mathbf{\\Sigma'}=\\mathbf{C}\\mathbf{\\Sigma}\\mathbf{C}^\\top = \\begin{pmatrix}\\sigma_{22} & \\sigma_{24} & \\sigma_{25}\\\\ \\sigma_{42} & \\sigma_{44} & \\sigma_{45} \\\\  \\sigma_{52} & \\sigma_{54} & \\sigma_{55}\\end{pmatrix},\n\\] tal como indica la última proposición sobre distribuciones marginales."
  },
  {
    "objectID": "6.html#independencia-de-variables-aleatorias",
    "href": "6.html#independencia-de-variables-aleatorias",
    "title": "7  Vectores aleatorios",
    "section": "7.5 Independencia de variables aleatorias",
    "text": "7.5 Independencia de variables aleatorias\n\n7.5.1 Independencia de variables aleatorias discretas\nLa generalización de independencia a variables aleatorias \\(n\\)-dimensionales es clara:\nDefinición de independencia para variables aleatorias multidimensionales discretas.\nSean \\((X_1\\ldots,X_n)\\) una variable aleatoria \\(n\\)-dimensional discreta con \\((X_1\\ldots,X_n)(\\Omega)=\\{(x_{i_1},\\ldots,x_{i_n}),\\ i_1=1,2,\\ldots,i_n=1,2,\\ldots\\}\\) y función de probabilidad conjunta \\(P_{X_1\\ldots X_n}\\) y funciones de probabilidad marginales \\(P_{X_1},\\ldots P_{X_n}\\). Entonces \\(X_1,\\ldots X_n\\) son independientes si:\n\\[\nP_{X_1\\ldots X_n}(x_{i_1},\\ldots,x_{i_n})=P_{X_1}(x_{i_1})\\cdots P_{X_n}(x_{i_n}),\\ i_1=1,2,\\ldots,i_n=1,2,\\ldots\n\\]\no dicho de otra forma:\n\\[\nP(X_1=x_{i_1},\\ X_n=x_{i_n})=P(X_1=x_{i_1})\\cdots P(X_n=x_{i_n}),\\ i_1=1,2,\\ldots,i_n=1,2,\\ldots\n\\]\n\nEjemplo del lanzamiento de un dado tres veces\nConsideremos el experimento aleatorio que consiste en lanzar un dado tres veces.\nRecordemos que hemos estudiado la variable aleatoria \\((X_1,X_2,X_3)\\) donde \\(X_1\\) nos daba el número de 1’s que han salido, \\(X_2\\), el número de 2’s y \\(X_3\\), el número de 3’s.\n\nLas variables aleatorias anteriores no son independientes ya que, por ejemplo: \\[\n\\scriptsize{\nP_{X_1,X_2,X_3}(1,1,3)=0\\neq P_{X_1}(1)\\cdot P_{X_2}(1)\\cdot P_{X_3}(3)=0.3472\\cdot 0.3472\\cdot 0.0046=6\\times 10^{-4}.}\n\\]\n\n\nObservación.  Al igual que pasaba con las variables bidimensionales, si la tabla de la función de probabilidad conjunta de \\((X_1,\\ldots,X_n)\\) contiene algún \\(0\\), las variables \\(X_1,\\ldots, X_n\\) no pueden ser independientes. ¿Podéis decir por qué?\n Observación.  Si \\(X_1,\\ldots, X_n\\) son variables aleatorias independientes, y consideramos una distribución marginal, por ejemplo \\((X_{s_1},\\ldots,X_{s_k})\\), entonces las variables \\(X_{s_1},\\ldots,X_{s_k}\\) también son independientes.\n\n\n7.5.2 Independencia de variables aleatorias continuas\nLa definición dada para variables aleatorias discretas se traslada de forma natural a las variables aleatorias continuas:\nDefinición de independencia para variables aleatorias multidimensionales continuas. \nSean \\((X_1\\ldots,X_n)\\) una variable aleatoria \\(n\\)-dimensional continua con función de densidad conjunta \\(f_{X_1\\ldots X_n}\\) y funciones de densidad marginales \\(f_{X_1},\\ldots,f_{X_n}\\). Entonces \\(X_1,\\ldots, X_n\\) son independientes si:\n\\[\nf_{X_1\\ldots X_n}(x_1,\\ldots,x_n)=f_{X_1}(x_1)\\cdots f_{X_n}(x_n),\\ \\mbox{para todo $x_1,\\ldots,x_n\\in\\mathbb{R}$.}\n\\]\n\nEjemplo: variable aleatoria tridimensional continua\nRecordemos el ejemplo siguiente visto donde teníamos una variable aleatoria \\(3\\)-dimensional continua \\((X_1,X_2,X_3)\\) con función de densidad conjunta: \\[\nf_{X_1X_2X_3}(x_1,x_2,x_3)=\\begin{cases}\n8 x_1\\cdot x_2\\cdot x_3, & \\mbox{si }0\\leq x_1\\leq 1,\\ 0\\leq x_2\\leq 1,\\ 0\\leq x_3\\leq 1, \\\\\n0, & \\mbox{en caso contrario.}\\\\\n\\end{cases}\n\\] y con densidad marginales: \\[\n\\begin{array}{rl}\nf_{X_1}(x_1) & =\\begin{cases}\n2x_1, & \\mbox{ si }0\\leq x\\leq 1,\\\\\n0, & \\mbox{en caso contrario.}\n\\end{cases}\\quad f_{X_2}(x_2)=\\begin{cases}\n2x_2, & \\mbox{ si }0\\leq x_2\\leq 1,\\\\\n0, & \\mbox{en caso contrario.}\n\\end{cases}\\\\ f_{X_3}(x_3) & =\\begin{cases}\n2x_3, & \\mbox{ si }0\\leq x_3\\leq 1,\\\\\n0, & \\mbox{en caso contrario.}\n\\end{cases}\n\\end{array}\n\\]\nVeamos que son independientes.\n\nConsideremos dos casos:\n\n\\((x_1,x_2,x_3)\\in [0,1]\\times [0,1]\\times [0,1]\\). En este caso: \\[\nf_{X_1X_2X_3}(x_1,x_2,x_3) =8 x_1 x_2 x_3 =2 x_1 2 x_2 2 x_3=f_{X_1}(x_1)\\cdot f_{X_2}(x_2)\\cdot f_{X_3}(x_3).\n\\]\n\\((x_1,x_2,x_3)\\not\\in [0,1]\\times [0,1]\\times [0,1]\\). En este caso: \\[\nf_{X_1X_2X_3}(x_1,x_2,x_3)  =0 = f_{X_1}(x_1)\\cdot f_{X_2}(x_2)\\cdot f_{X_3}(x_3),\n\\] ya que si \\((x_1,x_2,x_3)\\not\\in [0,1]\\times [0,1]\\times [0,1]\\), o \\(x_1\\not\\in [0,1]\\) o \\(x_2\\not\\in [0,1]\\), o \\(x_3\\not\\in [0,1]\\). Por tanto \\(f_{X_1}(x_1)=0\\) o \\(f_{X_2}(x_2)=0\\) o \\(f_{X_3}(x_3)=0\\). En cualquier caso, \\(f_{X_1}(x_1)\\cdot f_{X_2}(x_2)\\cdot f_{X_3}(x_3)=0\\).\n\n\n\n\n7.5.2.1 Ejemplo de la variable gaussiana \\(n\\)-dimensional\nEn este caso, recordemos que la función de densidad conjunta de \\((X_1,\\ldots,X_n)\\) es: \\[\n\\begin{array}{rl}\n& f_{X_1\\ldots X_n}(x_1,\\ldots,x_n)=\\frac{1}{(2\\pi)^{\\frac{n}{2}}\\sqrt{\\mathbf{|\\Sigma|}}}\\mathrm{e}^{-\\frac{1}{2}(\\mathbf{x-\\mu})^\\top\\mathbf{\\Sigma}^{-1}(\\mathbf{x-\\mu})},\\\\ & \\qquad  -\\infty &lt;x_1,\\ldots,x_n&lt;\\infty,\n\\end{array}\n\\] donde \\(\\mathbf{\\mu}\\) es el vector de medias y \\(\\mathbf{\\Sigma}\\), la matriz de covarianzas.\nRecordemos también que las funciones de densidad marginales de \\(X_1,\\ldots, X_n\\) correspondían a \\(N(\\mu_i,\\sqrt{\\sigma_{ii}})\\), con \\(i=1,\\ldots, n\\): \\[\nf_{X_i}(x_i)  =\\frac{1}{\\sqrt{2\\pi\\sigma_{ii}}}\\mathrm{e}^{-\\frac{(x_i-\\mu_i)^2}{2\\sigma_{ii}}},\\ -\\infty &lt;x_i&lt;\\infty,\\ i=1,\\ldots,n.\n\\]\n¿Cómo tiene que ser la matriz de covarianzas \\(\\mathbf{\\Sigma}\\) para que las variables \\(X_1,\\ldots,X_n\\) sean independientes?\nO, expresado matemáticamente, \\[\n\\begin{array}{rl}\nf_{X_1}(x_1)\\cdots f_{X_n}(x_n) & =\\frac{1}{\\left(2\\pi\\right)^{\\frac{n}{2}}\\sqrt{\\sigma_{11}\\cdots \\sigma_{nn}}}\\mathrm{e}^{-\\sum\\limits_{i=1}^n\\frac{(x_i-\\mu_i)^2}{2\\sigma_{ii}}}=f_{X_1\\ldots X_n}(x_1,\\ldots,x_n) \\\\ & =\\frac{1}{(2\\pi)^{\\frac{n}{2}}\\sqrt{\\mathbf{|\\Sigma|}}}\\mathrm{e}^{-\\frac{1}{2}(\\mathbf{x-\\mu})^\\top\\mathbf{\\Sigma}^{-1}(\\mathbf{x-\\mu})}.\n\\end{array}\n\\]\nLa respuesta es claramente cuando la matriz de covarianzas \\(\\mathbf{\\Sigma}\\) es diagonal, es decir, si \\(\\sigma_{ij}=0,\\) para \\(i\\neq j\\), para todo \\(i,j=1,\\ldots,n\\).\nEn resumen, cuando la covarianza entre dos variables cualesquiera \\(X_i\\) y \\(X_j\\) distintas es cero, las variables normales \\(X_1,\\ldots,X_n\\) son independientes.\n\n\n\n7.5.3 Relación de la independencia y la función de distribución\nEl siguiente resultado nos da la relación entre la independencia de variables aleatorias y su función de distribución conjunta:\nTeorema.  Sea \\((X_1\\ldots,X_n)\\) una variable aleatoria \\(n\\)-dimensional. Entonces \\(X_1,\\ldots,X_n\\) son independientes si, y sólo si, la función de distribución conjunta es el producto de las funciones de distribución marginales en todo valor \\((x_1,\\ldots,x_n)\\in\\mathbb{R}^n\\): \\[\nF_{X_1\\ldots X_n}(x_1,\\ldots,x_n)=F_{X_1}(x_1)\\cdots F_{X_n}(x_n),\\ (x_1,\\ldots,x_n)\\in\\mathbb{R}^n.\n\\]\n\n7.5.3.1 Ejemplos\n\nEjemplo: densidad tridimensional continuación\nRecordemos la variable aleatoria \\(3\\)-dimensional continua con función de densidad conjunta:\n\\[\nf_{X_1X_2X_3}(x_1,x_2,x_3)=\\begin{cases}\n8 x_1\\cdot x_2\\cdot x_3, & \\mbox{si }0\\leq x_1\\leq 1,\\ 0\\leq x_2\\leq 1,\\ 0\\leq x_3\\leq 1, \\\\\n0, & \\mbox{en caso contrario.}\\\\\n\\end{cases}\n\\] Recordemos también función de distribución conjunta es:\n\\[\nF_{X_1X_2X_3}(x_1,x_2,x_3)=\\begin{cases}\n0, & \\mbox{si }x_1&lt;0,\\mbox{ o }x_2&lt;0,\\mbox{ o }x_3 &lt;0\\\\\nx_1^2\\cdot x_2^2\\cdot x_3^2, & \\mbox{si }0\\leq x_1\\leq 1,\\ 0\\leq x_2\\leq 1,\\ 0\\leq x_3\\leq 1, \\\\\nx_2^2\\cdot x_3^2, & \\mbox{si }x_1&gt; 1,\\ 0\\leq x_2\\leq  1,\\ 0\\leq x_3\\leq  1, \\\\\nx_1^2\\cdot x_3^2, & \\mbox{si }0\\leq x_1\\leq  1,\\ x_2&gt; 1,\\ \\ 0\\leq x_3\\leq  1, \\\\\nx_3^2, & \\mbox{si }x_1&gt; 1,\\ x_2&gt; 1,\\ \\ 0\\leq x_3\\leq  1, \\\\\nx_1^2\\cdot x_2^2, & \\mbox{si }0\\leq x_1\\leq  1,\\ 0\\leq x_2\\leq  1,\\ x_3&gt; 1,\\\\\nx_1^2, & \\mbox{si }0\\leq x_1\\leq  1,\\ x_2 &gt;  1,\\ x_3&gt; 1,\\\\\nx_2^2, & \\mbox{si }x_1&gt;1,\\ 0\\leq x_2\\leq  1,\\ x_3&gt; 1,\\\\\n1, & \\mbox{si }x_1\\geq 1,\\ x_2\\geq 1,\\ x_3\\geq 1.\n\\end{cases}\n\\]\nRecordemos las funciones de densidad de las distribuciones marginales:\n\\[\n\\begin{array}{rl}\nf_{X_1}(x_1) & =\\begin{cases}\n2x_1, & \\mbox{ si }0\\leq x\\leq 1,\\\\\n0, & \\mbox{en caso contrario.}\n\\end{cases}\\\\\nf_{X_2}(x_2)&=\\begin{cases}\n2x_2, & \\mbox{ si }0\\leq x_2\\leq 1,\\\\\n0, & \\mbox{en caso contrario.}\n\\end{cases}\\\\ f_{X_3}(x_3) & =\\begin{cases}\n2x_3, & \\mbox{ si }0\\leq x_3\\leq 1,\\\\\n0, & \\mbox{en caso contrario.}\n\\end{cases}\n\\end{array}\n\\]\n\nDejamos como ejercicio verificar que las expresiones siguientes correspondes a las funciones de distribución marginales:\n\\[\n\\begin{array}{rl}\nF_{X_1}(x_1) & =\\begin{cases}\n0, & \\mbox{ si }x_1&lt;0, \\\\\nx_1^2, & \\mbox{ si }0\\leq x_1\\leq 1,\\\\\n1, & \\mbox{ si }x_1 &gt; 1.\n\\end{cases}\\\\\nF_{X_2}(x_2) & =\\begin{cases}\n0, & \\mbox{ si }x_2&lt;0, \\\\\nx_2^2, & \\mbox{ si }0\\leq x_2\\leq 1,\\\\\n1, & \\mbox{ si }x_2 &gt; 1.\n\\end{cases}\\\\ F_{X_3}(x_3) & =\\begin{cases}\n0, & \\mbox{ si }x_3&lt;0, \\\\\nx_3^2, & \\mbox{ si }0\\leq x_3\\leq 1,\\\\\n1, & \\mbox{ si }x_3 &gt; 1.\n\\end{cases}\n\\end{array}\n\\]\nRecordemos que \\(X_1\\), \\(X_2\\) y \\(X_3\\) son independientes. Por tanto verifiquemos que \\(F_{X_1X_2X_3}(x_1,x_2,x_3)=F_{X_1}(x_1)\\cdot F_{X_2}(x_2)\\cdot F_{X_3}(x_3)\\) para todos los valores \\(x_1,x_2,x_3\\in\\mathbb{R}\\).\nDistingamos los mismos casos que en la función de distribución conjunta:\n\n\\(x_1&lt;0\\), o \\(x_2&lt;0\\), o \\(x_3 &lt;0\\). En este caso, o \\(F_{X_1}(x_1)=0\\), o \\(F_{X_2}(x_2)=0\\) o \\(F_{X_3}(x_3)=0\\). En cualquier caso: \\[\nF_{X_1X_2X_3}(x_1,x_2,x_3)=0= F_{X_1}(x_1)\\cdot F_{X_2}(x_2)\\cdot F_{X_3}(x_3).\n\\]\n\\(0\\leq x_1\\leq 1\\), y \\(0\\leq x_2\\leq 1\\), y \\(0\\leq x_3\\leq 1\\). En este caso: \\[\nF_{X_1X_2X_3}(x_1,x_2,x_3)=x_1^2\\cdot x_2^2\\cdot x_3^2= F_{X_1}(x_1)\\cdot F_{X_2}(x_2)\\cdot F_{X_3}(x_3).\n\\]\n\\(x_1&gt; 1\\), y \\(0\\leq x_2\\leq 1\\), y \\(0\\leq x_3\\leq 1\\). En este caso: \\[\nF_{X_1X_2X_3}(x_1,x_2,x_3)=x_2^2\\cdot x_3^2=1\\cdot x_2^2\\cdot x_3^2= F_{X_1}(x_1)\\cdot F_{X_2}(x_2)\\cdot F_{X_3}(x_3).\n\\]\n\\(0\\leq x_1\\leq 1,\\) y \\(x_2&gt; 1\\), y \\(0\\leq x_3\\leq 1\\). En este caso: \\[\nF_{X_1X_2X_3}(x_1,x_2,x_3)=x_1^2\\cdot x_3^2=x_1^2\\cdot 1\\cdot x_3^2= F_{X_1}(x_1)\\cdot F_{X_2}(x_2)\\cdot F_{X_3}(x_3).\n\\]\n\\(x_1&gt; 1\\), y \\(x_2&gt; 1\\), y \\(0\\leq x_3\\leq 1\\). En este caso: \\[\nF_{X_1X_2X_3}(x_1,x_2,x_3)=x_3^2=1\\cdot 1\\cdot x_3^2= F_{X_1}(x_1)\\cdot F_{X_2}(x_2)\\cdot F_{X_3}(x_3).\n\\]\n\\(0\\leq x_1\\leq 1\\), y \\(0\\leq x_2\\leq 1\\), y \\(x_3&gt; 1\\). En este caso: \\[\nF_{X_1X_2X_3}(x_1,x_2,x_3)=x_1^2\\cdot x_2^2=x_1^2\\cdot x_2^2\\cdot 1= F_{X_1}(x_1)\\cdot F_{X_2}(x_2)\\cdot F_{X_3}(x_3).\n\\]\n\\(0\\leq x_1\\leq 1\\), y \\(x_2 &gt; 1\\), y \\(x_3&gt; 1\\). En este caso: \\[\nF_{X_1X_2X_3}(x_1,x_2,x_3)=x_1^2=x_1^2\\cdot 1\\cdot 1= F_{X_1}(x_1)\\cdot F_{X_2}(x_2)\\cdot F_{X_3}(x_3).\n\\]\n\\(x_1&gt; 1\\), y \\(0\\leq x_2 \\leq 1\\), y \\(x_3&gt; 1\\). En este caso: \\[\nF_{X_1X_2X_3}(x_1,x_2,x_3)=x_2^2=1\\cdot x_2^2\\cdot 1= F_{X_1}(x_1)\\cdot F_{X_2}(x_2)\\cdot F_{X_3}(x_3).\n\\]\n\\(x_1&gt; 1\\), y \\(x_2&gt;1\\), y \\(x_3&gt; 1\\). En este caso: \\[\nF_{X_1X_2X_3}(x_1,x_2,x_3)=1=1\\cdot 1\\cdot 1= F_{X_1}(x_1)\\cdot F_{X_2}(x_2)\\cdot F_{X_3}(x_3).\n\\]"
  },
  {
    "objectID": "6.html#momentos-conjuntos-y-valores-esperados-conjuntos",
    "href": "6.html#momentos-conjuntos-y-valores-esperados-conjuntos",
    "title": "7  Vectores aleatorios",
    "section": "7.6 Momentos conjuntos y valores esperados conjuntos",
    "text": "7.6 Momentos conjuntos y valores esperados conjuntos\n\n7.6.1 Valor esperado de una función de \\(n\\) variables aleatorias\nSea \\((X_1,\\ldots,X_n)\\) una variable aleatoria \\(n\\)-dimensional.\nSea \\(P_{X_1\\ldots X_n}\\) su función de probabilidad conjunta en el caso en que \\((X_1,\\ldots,X_n)\\) sea discreta y \\(f_{X_1\\ldots X_n}\\) su función de densidad conjunta en el caso en que \\((X_1,\\ldots,X_n)\\) sea continua.\nSea \\(Z=g(X_1,\\ldots,X_n)\\) una variable aleatoria unidimensional función de las variables \\(X_1,\\ldots,X_n\\). Por ejemplo:\n\nMedia aritmética de las \\(n\\) variables \\(g(x_1,\\ldots,x_n)=\\frac{x_1+\\cdots + x_n}{n}\\): \\(Z=\\frac{X_1+\\cdots +X_n}{n}\\).\nMedia geométrica de las \\(n\\) variables \\(g(x_1,\\ldots,x_n)=\\sqrt[n]{x_1\\cdots x_n}\\): \\(Z=\\sqrt[n]{X_1\\cdots X_n}\\).\nSuma de los cuadrados de las variables \\(g(x_1,\\ldots,x_n)=x_1^2+\\cdots +x_n^2\\): \\(Z=X_1^2+\\cdots +X_n^2\\).\n\nHay que tener en cuenta que \\(Z\\), como variable aleatoria unidimensional tiene una función de probabilidad \\(P_Z\\) en el caso en que \\((X_1,\\ldots,X_n)\\) sea discreta y una función de densidad \\(f_Z\\) en el caso en que \\((X_1,\\ldots,X_n)\\) sea continua.\nEl siguiente resultado nos dice cómo calcular el valor esperado de \\(Z\\) sin tener que calcular \\(P_Z\\) o \\(f_Z\\), sólo usando la información de la variable aleatoria conjunta \\((X_1,\\ldots,X_n)\\):\nProposición.  El valor esperado de \\(Z\\) se puede hallar usando la expresión siguiente:\n\nen el caso en que \\((X_1,\\ldots,X_n)\\) sea discreta con \\((X_1,\\ldots,X_n)(\\Omega)=\\{(x_{i_1},\\ldots,x_{i_n}),\\ i_1=1,2,\\ldots, i_n=1,2,\\ldots\\}\\), \\[\nE(Z)  = E(g(X_1\\ldots,X_n))  =\\sum_{x_{i_1}}\\cdots\\sum_{x_{i_n}}g(x_{i_1},\\ldots,x_{i_n})P(x_{i_1},\\ldots,x_{i_n}),\n\\]\nen el caso en que \\((X_1,\\ldots,X_n)\\) sea continua: \\[\n\\begin{array}{rl}\n& E(Z)=E(g(X_1\\ldots,X_n)) \\\\ & \\quad =\\int_{-\\infty}^\\infty\\cdots\\int_{-\\infty}^\\infty g(x_1,\\ldots,x_n)f_{X_1\\ldots X_n}(x_1,\\ldots,x_n)\\, dx_1\\ldots dx_n.\n\\end{array}\n\\]\n\n\n\n7.6.2 Ejemplos\n\nEjemplo del lanzamiento de un dado tres veces\nConsideremos el ejemplo de la variable \\((X_1,X_2,X_3)\\) que nos daba el número de 1’s, 2’s y 3’s en el lanzamiento de un dado tres veces.\nVamos a calcular \\(E\\left(X_1\\cdot X_2\\cdot X_3\\right)\\).\n\nEl valor esperado anterior se calcula como sigue: \\[\nE\\left(X_1\\cdot X_2\\cdot X_3\\right)\\displaystyle =\\sum_{i_1=0}^3\\sum_{i_2=0}^3\n\\sum_{i_3=0}^3 i_1\\cdot i_2\\cdot i_3\\cdot P_{X_1X_2X_3}(i_1,i_2,i_3),\n\\] en total \\(4^3=64\\) términos. Como el cálculo es tedioso, lo vamos a realizar con ayuda de R.\n\nvalor.esperado=0;\nfor (i1 in 0:3){\n  for (i2 in 0:3){\n    for (i3 in 0:3){\n      valor.esperado=valor.esperado+i1*i2*i3*fun.prob.con(i1,i2,i3)\n    }\n  }\n}\nvalor.esperado\n\n[1] 0.02777778\n\n\n\n\n\nEjemplo: densidad tridimensional (continuación)\nRecordemos la variable aleatoria \\(3\\)-dimensional \\((X_1,X_2,X_3)\\) con función de densidad conjunta:\n\\[\nf_{X_1X_2X_3}(x_1,x_2,x_3)=\\begin{cases}\n8 x_1\\cdot x_2\\cdot x_3, & \\mbox{si }0\\leq x_1\\leq 1,\\ 0\\leq x_2\\leq 1,\\ 0\\leq x_3\\leq 1, \\\\\n0, & \\mbox{en caso contrario.}\\\\\n\\end{cases}\n\\] Calculemos \\(E(X_1^2+X_2^2+X_3^2)\\):\n\n\\[\n\\scriptsize{\n\\begin{array}{rl}\nE(X_1^2+X_2^2+X_3^2) & \\displaystyle=\\int_{x_1=0}^{x_1=1} \\int_{x_2=0}^{x_2=1}\\int_{x_3=0}^{x_3=1} (x_1^2+x_2^2+x_3^2) 8\\cdot x_1\\cdot x_2\\cdot  x_3 \\,dx_1\\, dx_2\\, dx_3\\\\ & =8\\cdot \\left(\\int_{x_1=0}^{x_1=1} \\int_{x_2=0}^{x_2=1}\\int_{x_3=0}^{x_3=1}   x_1^3\\cdot  x_2\\cdot  x_3 \\,dx_1\\, dx_2\\, dx_3 + \\int_{x_1=0}^{x_1=1} \\int_{x_2=0}^{x_2=1}\\int_{x_3=0}^{x_3=1}   x_1 \\cdot x_2^3 \\cdot x_3 \\,dx_1\\, dx_2\\, dx_3 \\right.\\\\ & \\left. + \\int_{x_1=0}^{x_1=1} \\int_{x_2=0}^{x_2=1}\\int_{x_3=0}^{x_3=1}   x_1\\cdot  x_2\\cdot  x_3^3 \\,dx_1\\, dx_2\\, dx_3\\right) \\\\ & =\n8\\left(\\left[\\frac{x_1^4}{4}\\right]_0^1 \\left[\\frac{x_2^2}{2}\\right]_0^1 \\left[\\frac{x_3^2}{2}\\right]_0^1 + \\left[\\frac{x_1^2}{2}\\right]_0^1 \\left[\\frac{x_2^4}{4}\\right]_0^1 \\left[\\frac{x_3^2}{2}\\right]_0^1 + \\left[\\frac{x_1^2}{2}\\right]_0^1 \\left[\\frac{x_2^2}{2}\\right]_0^1 \\left[\\frac{x_3^4}{4}\\right]_0^1\\right) \\\\ & =8\\cdot 3\\cdot \\frac{1}{16}=\\frac{3}{2}=1.5.\n\\end{array}}\n\\]\n\n\n\n7.6.3 Propiedad del valor esperado de la suma de variables\nEn estos momentos estamos en condiciones de demostrar el resultado siguiente:\nProposición. Valor esperado de la suma de variables aleatorias. Sea \\((X_1,\\ldots,X_n)\\) una variable aleatoria \\(n\\)-dimensional. Entonces el valor esperado de la variable aleatoria suma de las variables es igual a la suma de los valores esperados de cada variable: \\[\nE(X_1+\\cdots + X_n)=E(X_1)+\\cdots + E(X_n).\n\\]\n\nDemostración\nHaremos la demostración para el caso continuo. Dejamos como ejercicio la demostración en el caso discreto.\nPara calcular valor esperado de la suma de variables necesitamos la función de densidad conjunta \\(f_{X_1\\ldots X_n}\\):\n\\[\n\\begin{array}{rl}\nE(X_1+\\cdots + X_n) & = \\int_{-\\infty}^\\infty\\cdots\\int_{-\\infty}^\\infty (x_1+\\cdots + x_n)f_{X_1\\ldots X_n}(x_1,\\ldots,x_n)\\,dx_1\\ldots dx_n \\\\ & = \\int_{-\\infty}^\\infty\\cdots\\int_{-\\infty}^\\infty x_1f_{X_1\\ldots X_n}(x_1,\\ldots,x_n)\\,dx_1\\ldots dx_n+ \\cdots \\\\ & + \\int_{-\\infty}^\\infty\\cdots\\int_{-\\infty}^\\infty x_n f_{X_1\\ldots X_n}(x_1,\\ldots,x_n)\\,dx_1\\ldots dx_n \\\\ & = E(X_1)+\\cdots + E(X_n).\n\\end{array}\n\\]\n\n\n7.6.3.1 Ejemplos\n\nEjemplo del lanzamiento de un dado tres veces\nConsideremos el ejemplo de la variable \\((X_1,X_2,X_3)\\) que nos daba el número de 1’s, 2’s y 3’s en el lanzamiento de un dado tres veces.\nComprobemos en este caso que \\(E(X_1+X_2+X_3)=E(X_1)+E(X_2)+E(X_3)\\).\nCalculemos \\(E(X_1+X_2+X_3)\\) con R usando la misma técnica que en el ejemplo donde calculábamos \\(E(X_1\\cdot X_2\\cdot X_3)\\):\n\nvalor.esperado=0;\nfor (i1 in 0:3){\n  for (i2 in 0:3){\n    for (i3 in 0:3){\n      valor.esperado=valor.esperado+(i1+i2+i3)*fun.prob.con(i1,i2,i3)\n    }\n  }\n}\nvalor.esperado\n\n[1] 1.5\n\n\nLa distribución marginal de cada una de las variables \\(X_1\\), \\(X_2\\) y \\(X_3\\) recordemos que es la siguiente:\n\n\n\n\n\\(X_1\\)\n0\n1\n2\n3\n\n\n\n\n\\(P_{X_1}\\)\n\\(0.5787\\)\n\\(0.3472\\)\n\\(0.0694\\)\n\\(0.0046\\)\n\n\n\n\nEl valor de \\(E(X_1)=E(X_2)=E(X_3)\\) es:\n\nesperanza.X1=0\nfor (i in 0:3){\n  esperanza.X1=esperanza.X1+i*fun.marginal.X1(i)\n}\nesperanza.X1\n\n[1] 0.5\n\n\nClaramente: \\[\nE(X_1+X_2+X_3)=1.5=E(X_1)+E(X_2)+E(X_3)=0.5+0.5+0.5.\n\\]\n\n\n\n\n7.6.4 Valor esperado de una función de \\(n\\) variables aleatorias independientes\nEl siguiente resultado nos simplifica el cálculo del valor esperado de una función de \\(n\\) variables aleatorias en el caso en que sean independientes:\nProposición: cálculo del valor esperado de una función de \\(n\\) variables aleatorias en el caso de independencia.  Sea \\((X_1,\\ldots,X_n)\\) una variable aleatoria \\(n\\)-dimensional donde suponemos que \\(X_1,\\ldots,X_n\\) son independientes. Sea \\(Z=g(X_1,\\ldots,X_n)\\) una variable aleatoria unidimensional función de \\(X_1,\\ldots,X_n\\) donde suponemos que podemos “separar” las variables \\(x_1,\\ldots, x_n\\) en la función \\(g\\). O lo que es lo mismo, existen \\(n\\) funciones \\(g_1,\\ldots, g_n\\) tal que \\(g(x_1,\\ldots,x_n)=g_1(x_1)\\cdots g_n(x_n)\\) para todo valor \\(x_1,\\ldots,x_n\\in\\mathbb{R}\\). En este caso, el valor esperado de \\(Z\\) se puede calcular como: \\[\nE(Z)=E(g(X_1\\ldots,X_n))=E_{X_1}(g_1(X_1))\\cdots E_{X_n}(g_n(X_n)).\n\\]\nEl cálculo de \\(E(g(X_1\\ldots,X_n))\\) que es una suma múltiple en el caso de que \\((X_1,\\ldots,X_n)\\) sea discreta o una integral múltiple en el caso en que \\((X_1,\\ldots,X_n)\\) sea continua se transforma en el producto de \\(n\\) sumas simples (caso discreto) o el producto de \\(n\\) integrales simples (caso continuo):\n\\[\n\\begin{array}{rl}\nE(Z) & =E(g(X_1\\ldots,X_n))\\\\ & =\\left(\\sum_{x_{i_1}} g_1(x_{i_1})\\cdot P_{X_1}(x_{i_1})\\right)\\cdots \\left(\\sum_{x_{i_n}} g_n(x_{i_n})\\cdot P_{X_n}(x_{i_n})\\right), \\ \\mbox{caso discreto},\\\\\nE(Z) & =E(g(X_1\\ldots,X_n)) \\\\ & =\\left(\\int_{-\\infty}^\\infty g_1(x_1)\\cdot f_{X_1}(x_1)\\, dx_1\\right)\\cdots \\left(\\int_{-\\infty}^\\infty g_n(x_n)\\cdot f_{X_n}(x_n)\\right), \\ \\mbox{caso continuo}.\n\\end{array}\n\\]\nUn caso particular de aplicación de la proposición anterior se produce cuando queramos calcular \\(E(X_1\\cdots X_n)\\). En este caso \\(g(x_1,\\ldots,x_n)=x_1\\cdots x_n\\), \\(g_1(x_1)=x_1\\), y \\(g_n(x_n)=x_n\\).\nPodemos escribir, por tanto: \\[\nE(X_1\\cdots X_n)=E_{X_1}(X_1)\\cdots E_{X_n}(X_n),\n\\] si \\(X_1,\\ldots,X_n\\) son independientes.\n\n7.6.4.1 Ejemplos\n\nEjempo: densidad tridimensional (continuación)\nRecordemos el ejemplo con función de densidad conjunta:\n\\[\nf_{X_1X_2X_3}(x_1,x_2,x_3)=\\begin{cases}\n8 x_1\\cdot x_2\\cdot x_3, & \\mbox{si }0\\leq x_1\\leq 1,\\ 0\\leq x_2\\leq 1,\\ 0\\leq x_3\\leq 1, \\\\\n0, & \\mbox{en caso contrario.}\\\\\n\\end{cases}\n\\] del que ya sebemos que ls tres variables son independientes. Comprobemos que \\(E(X_1\\cdot X_2\\cdot X_3)=E(X_1)\\cdot E(X_2)\\cdot E(X_3)\\).\nCalculemos \\(E(X_1\\cdot X_2\\cdot X_3)\\):\n\n\\[\n\\begin{array}{rl}\nE(X_1\\cdot X_2\\cdot X_3)  & = \\int_0^1\\int_0^1\\int_0^1 x_1\\cdot x_2\\cdot x_3\\cdot 8 x_1\\cdot x_2\\cdot x_3\\,dx_1\\, dx_2\\, dx_3 \\\\ & = 8\\int_0^1\\int_0^1\\int_0^1 x_1^2\\cdot x_2^2\\cdot x_3^2\\,dx_1\\, dx_2\\, dx_3 =8\\left[\\frac{x_1^3}{3}\\right]_0^1\\left[\\frac{x_2^3}{3}\\right]_0^1\n\\left[\\frac{x_3^3}{3}\\right]_0^1 \\\\\n& = 8\\cdot \\frac{1}{3^3}=\\frac{8}{27}=0.2963.\n\\end{array}\n\\]\nRecordemos que las densidades marginales son:\n\\[\n\\begin{array}{rl}\nf_{X_1}(x_1) & =\\begin{cases}\n2x_1, & \\mbox{ si }0\\leq x\\leq 1,\\\\\n0, & \\mbox{en caso contrario.}\n\\end{cases}\\quad f_{X_2}(x_2)=\\begin{cases}\n2x_2, & \\mbox{ si }0\\leq x_2\\leq 1,\\\\\n0, & \\mbox{en caso contrario.}\n\\end{cases}\\\\ f_{X_3}(x_3) & =\\begin{cases}\n2x_3, & \\mbox{ si }0\\leq x_3\\leq 1,\\\\\n0, & \\mbox{en caso contrario.}\n\\end{cases}\n\\end{array}\n\\]\nPor tanto: \\[\nE(X_1)=E(X_2)=E(X_3)=\\int_0^1 x\\cdot 2 x\\, dx =2 \\int_0^1 x^2\\, dx=2\\left[\\frac{x^3}{3}\\right]_0^1=\\frac{2}{3}.\n\\] Claramente, se verifica: \\[\nE(X_1\\cdot X_2\\cdot X_3)=\\frac{8}{27}=E(X_1)\\cdot E(X_2)\\cdot E(X_3)=\\left(\\frac{2}{3}\\right)^3.\n\\]\n\n\n\n\n7.6.5 Propiedades de la covarianza\nVeamos cómo se calcula la covarianza de dos combinaciones lineales de variables aleatorias:\nProposición (covarianza de dos combinaciones lineales de variables aleatorias).  Sean \\((X_1,\\ldots,X_n)\\) e \\((Y_1,\\ldots, Y_n)\\) dos variables aleatorias \\(n\\)-dimensionales. Sean \\(a_1, \\ldots, a_n\\) y \\(b_1,\\ldots, b_n\\) \\(n\\) parejas de valores reales. Sean \\(U\\) y \\(V\\) las variables aleatorias siguientes: \\(U=\\sum\\limits_{i=1}^n a_i X_i,\\  V=\\sum\\limits_{i=1}^n b_i Y_i.\\) Entonces la covarianza de las variables \\(U\\) y \\(V\\) se calcula usando la expresión siguiente: \\[\n\\mathrm{Cov}(U,V)=\\sum_{i=1}^n\\sum_{j=1}^n a_i b_j \\mathrm{Cov}(X_i,Y_j).\n\\]\n\nDemostración\nUsando la expresión de la covarianza podemos calcular la covarianza entre las variables \\(U\\) y \\(V\\) como: \\[\n\\scriptsize{\n\\begin{array}{rl}\n\\mathrm{Cov}(U,V) & =E(UV)-E(U)\\cdot E(V)=E\\left(\\sum\\limits_{i=1}^n a_i \\cdot X_i\\cdot \\sum\\limits_{j=1}^n b_j \\cdot Y_j\\right)- E\\left(\\sum\\limits_{i=1}^n a_i \\cdot X_i\\right)E\\left(\\sum\\limits_{j=1}^n b_j \\cdot Y_j\\right) \\\\ & =\\sum\\limits_{i=1}^n \\sum\\limits_{j=1}^n a_i\\cdot  b_j \\cdot E\\left(X_i Y_j\\right)-\\sum\\limits_{i=1}^n a_i\\cdot  E(X_i)\\sum\\limits_{j=1}^n b_j \\cdot E(Y_j) \\\\\n&= \\sum\\limits_{i=1}^n \\sum\\limits_{j=1}^n a_i\\cdot  b_j\\cdot E\\left(X_i Y_j\\right)-\\sum\\limits_{i=1}^n\\sum\\limits_{j=1}^n a_i\\cdot  b_j\\cdot  E(X_i)\\cdot  E(Y_j) \\\\ & = \\sum\\limits_{i=1}^n \\sum\\limits_{j=1}^n a_i b_j \\left(E\\left(X_i Y_j\\right) - E(X_i)\\cdot  E(Y_j)\\right) = \\sum\\limits_{i=1}^n \\sum\\limits_{j=1}^n a_i\\cdot  b_j\\cdot  \\mathrm{Cov}(X_i,Y_j),\n\\end{array}}\n\\] tal como queríamos ver.\n\nSi en la combinación lineal sólo hay las componentes de una sola variable aleatoria \\(n\\)-dimensional, la proposición anterior se convierte en la proposición siguiente:\nProposición (covarianza de una combinación lineal de variables aleatorias).  Sean \\((X_1,\\ldots,X_n)\\) una variable aleatoria \\(n\\)-dimensional. Sean \\(a_1, \\ldots, a_n\\) y \\(b_1,\\ldots, b_n\\) \\(n\\) parejas de valores reales. Sean \\(U\\) y \\(V\\) las variables aleatorias siguientes: \\(U=\\sum\\limits_{i=1}^n a_i X_i,\\  V=\\sum\\limits_{i=1}^n b_i X_i.\\) Entonces la covarianza de las variables \\(U\\) y \\(V\\) se calcula usando la expresión siguiente: \\[\n\\mathrm{Cov}(U,V)=\\sum_{i=1}^n a_i b_i \\mathrm{Var}(X_i)+\\sum_{i=1}^n\\sum_{j=1,j\\neq i}^n a_i b_j \\mathrm{Cov}(X_i,X_j).\n\\]\n\nDemostración\nUsando la misma técnica de demostración que en la proposición anterior, podemos calcular la covarianza entre las variables \\(U\\) y \\(V\\) como: \\[\n\\begin{array}{rl}\n\\mathrm{Cov}(U,V) & = \\sum\\limits_{i=1}^n \\sum\\limits_{j=1}^n a_i\\cdot  b_j\\cdot  \\left(E\\left(X_i \\cdot X_j\\right) - E(X_i)\\cdot  E(X_j)\\right) \\\\\n&= \\sum\\limits_{i=1}^n a_i\\cdot  b_i \\cdot \\left(E\\left(X_i^2\\right) - E(X_i)^2\\right)+\\sum\\limits_{i=1}^n \\sum\\limits_{j=1,j\\neq i}^n a_i \\cdot b_j\\cdot  \\mathrm{Cov}(X_i,X_j) \\\\ & = \\sum\\limits_{i=1}^n a_i\\cdot  b_i\\cdot  \\mathrm{Var}(X_i)+\\sum\\limits_{i=1}^n \\sum\\limits_{j=1,j\\neq i}^n a_i b_j \\mathrm{Cov}(X_i,X_j),\n\\end{array}\n\\] tal como queríamos ver.\n\n Observación.  Una forma equivalente de escribir la covarianza anterior es: \\[\n\\mathrm{Cov}(U,V)=\\sum_{i=1}^n a_i\\cdot  b_i\\cdot  \\mathrm{Var}(X_i)+2\\sum_{i=1}^n\\sum_{j=1,j&gt;i}^n a_i\\cdot  b_j\\cdot  \\mathrm{Cov}(X_i,X_j).\n\\]\nComo, en general \\(\\mathrm{Var}(U)=\\mathrm{Cov}(U,U)\\), una consecuencia directa de la proposición anterior es la expresión de la varianza de una combinación lineal de variables aleatorias:\nProposición (varianza de una combinación lineal de variables aleatorias).  Sean \\((X_1,\\ldots,X_n)\\) una variable aleatoria \\(n\\)-dimensional. Sean \\(a_1, \\ldots, a_n\\) \\(n\\) valores reales. Sea \\(U\\) la variable aleatoria siguiente: \\(U=\\sum\\limits_{i=1}^n a_i X_i.\\) Entonces la varianza de la variable \\(U\\) se calcula usando la expresión siguiente: \\[\n\\mathrm{Var}(U)=\\sum_{i=1}^n a_i^2 \\mathrm{Var}(X_i)+\\sum_{i=1}^n\\sum_{j=1,j\\neq i}^n a_i a_j \\mathrm{Cov}(X_i,X_j).\n\\]\n\nDemostración\nPara la demostración, basta tener en cuenta que \\(\\mathrm{Var}(U)=\\mathrm{Cov}(U,U)\\) y aplicar la expresión de la covariancia entre \\(U\\) y \\(V\\) en la proposición que nos da la covarianza de una combinación lineal de variables aleatorias.\n\n Observación.  En este caso, también existe una forma equivalente de escribir la varianza anterior: \\[\n\\mathrm{Var}(U)=\\sum_{i=1}^n a_i^2 \\mathrm{Var}(X_i)+2\\sum_{i=1}^n\\sum_{j=1,j&gt;i}^n a_i a_j \\mathrm{Cov}(X_i,X_j).\n\\]\nUna consecuencia de la proposición anterior es que si las variables son independientes, la varianza de la suma es la suma de varianzas:\nProposición (varianza de la suma de variables aleatorias independientes).  Sean \\((X_1,\\ldots,X_n)\\) una variable aleatoria \\(n\\)-dimensional donde \\(X_1,\\ldots, X_n\\) son independientes.\nEntonces la varianza de la variable \\(X_1+\\cdots X_n\\) es la suma de las varianzas de cada variable aleatoria: \\[\n\\mathrm{Var}(X_1+\\cdots + X_n)=\\sum_{i=1}^n \\mathrm{Var}(X_i).\n\\]\n\nDemostración\nPara la demostración, basta aplicar la proposición anterior de la varianza de una combinación lineal de variables aleatorias con \\(a_i=1\\), para todo \\(i=1,\\ldots,n\\) y tener en cuenta que como son independientes, \\(\\mathrm{Cov}(X_i,X_j)=0\\), para todo \\(i\\neq j\\)."
  },
  {
    "objectID": "7.html#muestras-aleatorias-simples",
    "href": "7.html#muestras-aleatorias-simples",
    "title": "8  Ley de los grandes números y Teorema Central del Límite",
    "section": "8.1 Muestras aleatorias simples",
    "text": "8.1 Muestras aleatorias simples\nEl pilar básico sobre el que se sustenta la estadística inferencial es el concepto de muestra aleatoria simple.\nUna muestra aleatoria simple, desde el punto de vista de la probabilidad es una distribución \\(n\\) variables aleatorias, \\(X_1,\\ldots, X_n\\) todas independientes entre sí e idénticamente distribuidas ya que queremos simular la repetición de un experimento \\(n\\) veces de forma independiente.\nPor tanto, estudiar una muestra aleatoria simple equivale a estudiar su distribución.\nEn muchos casos, nos bastará estudiar la distribución de una variable que “represente” a dicha muestra aleatoria simple: la media muestral definida como \\(\\overline{X}=\\frac{X_1+\\cdots + X_n}{n}\\).\nLas leyes de los grandes números nos dicen que, de alguna manera (que concretaremos más adelante), la media muestral y la media poblacional se “parecen” a la larga o cuando el número de repeticiones \\(n\\) tiende a infinito.\nEl Teorema Central del Límite nos dice que la distribución de la media muestral tiende, sea cual sea la distribución de las variables \\(X_i\\), a una normal. De ahí que la distribución normal sea la más importante en probabilidades y estadística.\n\n8.1.1 La distribución de la media muestral\nVamos cómo se distribuye la media de un conjunto de variables normales e idénticamente distribuidas:\nProposición. Distribución de la media muestral de \\(n\\) variables normales independientes e idénticamente distribuidas.  Sean \\(X_1,\\ldots, X_n\\) \\(n\\) variables normales de media \\(\\mu\\) y varianza \\(\\sigma^2\\), todas normales e independientes. Consideramos la variable \\(\\overline{X}=\\frac{X_1+\\cdots + X_n}{n}\\) la media muestral. Entonces la distribución de la variable aleatoria \\(\\overline{X}\\) es normal de la misma media \\(\\mu\\) de las \\(X_i\\) y varianza \\(\\frac{\\sigma^2}{n}\\).\n\nDemostración\nConsideramos la variable aleatoria \\(n\\)-dimensional \\(\\mathbf{X}=(X_1,\\ldots,X_n)\\). Dicha variable tendrá la distribución normal \\(n\\)-dimensional con vector de medias \\(\\mathbf{\\mu}=(\\mu,\\ldots,\\mu)^\\top\\) y matriz de covarianzas \\(\\mathbf{\\Sigma}\\) diagonal ya que recordemos que las \\(X_i\\) son independientes y, por tanto, incorreladas o de covarianza nula: \\[\n\\mathbf{\\Sigma}=\\begin{pmatrix}\n\\sigma^2 & 0 & \\ldots & 0 \\\\\n0 & \\sigma^2 & \\ldots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\vdots \\\\\n0 & 0 & \\ldots & \\sigma^2\n\\end{pmatrix}.\n\\]\nPara hallar la variable \\(\\overline{X}\\), realizamos la transformación afín siguiente: \\[\n\\overline{X}=\\left(\\frac{1}{n},\\ldots,\\frac{1}{n}\\right)\\cdot\\begin{pmatrix} X_1 \\\\ X_2\\\\\\vdots \\\\ X_n \\end{pmatrix}.\n\\] Aplicando la proposición sobre la transformación afín sobre una variable normal \\(n\\)-dimensional que vimos en el capítulo de distribuciones \\(n\\)-dimensionales con matriz de cambio \\(\\mathbf{C}=\\left(\\frac{1}{n},\\ldots,\\frac{1}{n}\\right)\\) y \\(\\mathbf{c}=0\\), tenemos que la distribución de \\(\\overline{X}\\) será normal de media \\(\\mathbf{c}+\\mathbf{C}\\mathbf{\\mu} = \\mu\\) y varianza (o matriz de covarianzas \\(1\\times 1\\)): \\[\n\\mathbf{C}\\cdot\\mathbf{\\Sigma}\\cdot\\mathbf{C}^\\top =\\left(\\frac{1}{n},\\ldots,\\frac{1}{n}\\right)\\cdot\\begin{pmatrix}\n\\sigma^2 & 0 & \\ldots & 0 \\\\\n0 & \\sigma^2 & \\ldots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\vdots \\\\\n0 & 0 & \\ldots & \\sigma^2\n\\end{pmatrix}\\cdot \\begin{pmatrix}\\frac{1}{n}\\\\\\frac{1}{n}\\\\\\vdots\\\\\\frac{1}{n}\\end{pmatrix} =\\frac{\\sigma^2}{n}.\n\\]"
  },
  {
    "objectID": "7.html#convergencia-de-sucesiones-de-variables-aleatorias",
    "href": "7.html#convergencia-de-sucesiones-de-variables-aleatorias",
    "title": "8  Ley de los grandes números y Teorema Central del Límite",
    "section": "8.2 Convergencia de sucesiones de variables aleatorias",
    "text": "8.2 Convergencia de sucesiones de variables aleatorias\nEn esta sección vamos a intentar concretar cómo la media muestral y la media poblacional de una muestra aleatoria simple se van pareciendo, así como la distribución de la media muestral se va “acercando” a la normalidad.\nPara ello, necesitamos introducir un conjunto de conceptos relacionados con la convergencia de variables aleatorias.\nEn primer lugar, introduciremos el concepto de sucesión de variables aleatorias:\n Definición de sucesión de variables aleatorias.  Consideremos un experimento aleatorio sobre un espacio muestral \\(\\Omega\\). Sea \\(P\\) una probabilidad definida sobre el conjunto de sucesos de \\(\\Omega\\). Entonces, si \\(X_1,X_2,\\ldots,X_n,\\ldots\\) son variables aleatorias definidas sobre \\(\\Omega,P\\), diremos que forman una sucesión de variables aleatorias y lo denotaremos por \\(\\{X_n\\}_{n=1}^\\infty\\).\n\nEjemplo: lanzamiento de un dado\nConsideremos el experimento aleatorio de ir lanzando un dado no trucado. Definimos la variable aleatoria \\(X_n\\) como el resultado del dado el lanzamiento \\(n\\)-ésimo.\nEntonces, la sucesión de variables aleatorias \\(X_1,\\ldots,X_n,\\ldots\\) es la asociada al lanzamiento del dado.\n\n¡Ojo! no confundir la sucesión de variables aleatorias \\(X_1,\\ldots,X_n,\\ldots\\) con la sucesión de resultados de dichas variables aleatorias \\(x_1,\\ldots, x_n,\\ldots\\). Lo primero correspondería a variables aleatorias con su función de probabilidad, esperanza, varianza, etc., y lo segundo sería simplemente una sucesión numérica de valores enteros entre 1 y 6.\n\n\n\n8.2.1 Convergencia casi segura\n Definición de convergencia casi segura.  Sea \\(X_1,\\ldots,X_n,\\ldots\\) una sucesión de variables aleatorias y sea \\(X\\) una variable aleatoria definida sobre el mismo espacio muestral \\(\\Omega\\) y con la misma probabilidad de sucesos. Diremos que la sucesión \\(\\{X_n\\}_{n=1}^\\infty\\) converge casi seguramente hacia \\(X\\) si \\[\nP\\left(\\{w\\in \\Omega\\ |\\ \\lim_{n\\to\\infty} X_n(w)=X(w)\\}\\right)=1.\n\\] Lo denotaremos por \\(X_n\\stackrel{c.s.}{\\longrightarrow}X\\).\nEs decir, si el conjunto de elementos \\(w\\) del espacio muestral \\(\\Omega\\) que cumplen que el límite de la sucesión de números reales \\((X_n(w))_n\\) tiende a \\(X(w)\\) tiene probabilidad \\(1\\).\nDe ahí viene el nombre de casi segura: el conjunto de valores \\(w\\) del espacio muestral tal que la sucesión numérica \\((X_n(w))_n\\) no converge a \\(X(w)\\) tiene probabilidad 0.\nComprobar la convergencia casi segura a partir de la definición puede ser muy complicado. Por suerte, existe la proposición siguiente que nos hace la vida más fácil:\n Proposición.  Sea \\(X_1,\\ldots,X_n,\\ldots\\) una sucesión de variables aleatorias y sea \\(X\\) una variable aleatoria definida sobre el mismo espacio muestral \\(\\Omega\\) y con la misma probabilidad de sucesos. Entonces \\(X_n\\stackrel{c.s.}{\\longrightarrow}X\\) si, y sólo si, para todo valor \\(\\epsilon &gt;0\\), la serie siguiente \\[\n\\sum_{n=1}^\\infty P(|X_n-X|&gt;\\epsilon),\n\\] es convergente.\n** Ejemplo: convergencia casi segura frecuencias de un dado**\n\nVeamos si la sucesión \\(\\{X_n\\}_{n=1}^\\infty\\) tiene convergencia casi segura hacia la variable \\(X\\) cuya función de probabilidad es:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(X\\)\n1\n2\n3\n4\n5\n6\n\n\n\n\n\\(P_X\\)\n\\(\\frac{1}{6}\\)\n\\(\\frac{1}{6}\\)\n\\(\\frac{1}{6}\\)\n\\(\\frac{1}{6}\\)\n\\(\\frac{1}{6}\\)\n\\(\\frac{1}{6}\\)\n\n\n\n\n\nEn este caso el espacio muestral \\(\\Omega\\) es \\(\\Omega=\\{1,2,3,4,5,6\\}\\) y la función de probabilidad de cada \\(X_i\\) corresponde con la tabla anterior.\nSeguidamente, de cara a aplica la proposición anterior, vamos a hallar la función de probabilidad de la variable \\(D_n=X_n-X\\). Los valores de la variable anterior son: \\(D_n(\\Omega)=\\{-5,-4,-3,-2,-1,0,1,2,3,4,5\\}\\).\nLa función de probabilidad conjunta de la variable \\((X_n,X)\\) será al ser \\(X_n\\) y \\(X\\) independientes: \\[\nP_{X_nX}(x_n,x)=P_{X_n}(x_n)\\cdot P_X(x)=\\frac{1}{6}\\cdot \\frac{1}{6}=\\frac{1}{36},\n\\] para todo \\(x_n=1,2,3,4,5,6\\) y para todo \\(x=1,2,3,4,5,6\\).\nLa función de probabilidad de la variable \\(D_n\\) será: \\[\n\\scriptsize{\n\\begin{array}{rl}\nP_{D_n}(-5) & =P_{X_nX}(1,6)=\\frac{1}{36}, \\\\\nP_{D_n}(-4) & =P_{X_nX}(2,6)+P_{X_nX}(1,5)=\\frac{2}{36}, \\\\\nP_{D_n}(-3) & =P_{X_nX}(3,6)+P_{X_nX}(2,5)+P_{X_nX}(1,4)=\\frac{3}{36}, \\\\\nP_{D_n}(-2) & =P_{X_nX}(4,6)+P_{X_nX}(3,5)+P_{X_nX}(2,4)+P_{X_nX}(1,3)=\\frac{4}{36}, \\\\\nP_{D_n}(-1) & =P_{X_nX}(5,6)+P_{X_nX}(4,5)+P_{X_nX}(3,4)+P_{X_nX}(2,3)+P_{X_nX}(1,2)=\\frac{5}{36}, \\\\\nP_{D_n}(0) & =P_{X_nX}(6,6)+P_{X_nX}(5,5)+P_{X_nX}(4,4)+P_{X_nX}(3,3)+P_{X_nX}(2,2)+P_{X_nX}(1,1)=\\frac{6}{36}, \\\\\nP_{D_n}(1) & =P_{X_nX}(6,5)+P_{X_nX}(5,4)+P_{X_nX}(4,3)+P_{X_nX}(3,2)+P_{X_nX}(2,1)=\\frac{5}{36}, \\\\\nP_{D_n}(2) & =P_{X_nX}(6,4)+P_{X_nX}(5,3)+P_{X_nX}(4,2)+P_{X_nX}(3,1)=\\frac{4}{36}, \\\\\nP_{D_n}(3) & =P_{X_nX}(6,3)+P_{X_nX}(5,2)+P_{X_nX}(4,1)=\\frac{3}{36}, \\\\\nP_{D_n}(4) & =P_{X_nX}(6,2)+P_{X_nX}(5,1)=\\frac{2}{36}, \\\\\nP_{D_n}(5) & =P_{X_nX}(6,1)=\\frac{1}{36}.\n\\end{array}\n}\n\\]\nSea \\(\\epsilon\\) un valor real entre 0 y 1: \\(0&lt;\\epsilon &lt;1\\). Entonces el suceso \\(\\{|D_n|&gt;\\epsilon\\}\\) será el complementario del suceso \\(\\{D_n=0\\}\\) ya que el único valor entre \\(-5\\) y \\(5\\) que no cumple \\(|D_n|&gt;\\epsilon\\) es el valor \\(D_n=0\\). Por tanto: \\[\nP(|D_n|&gt;\\epsilon)=1-P(D_n=0)=1-P_{D_n}(0)=1-\\frac{1}{6}=\\frac{5}{6}.\n\\] La serie \\(\\sum\\limits_{n=1}^\\infty \\frac{5}{6}\\) no es convergente de forma obvia. Por tanto, deducimos que la sucesión \\(\\{X_n\\}_{n=1}^\\infty\\) no converge casi seguramente hacia la variable \\(X\\).\n\n\n\n\n8.2.2 Convergencia en probabilidad\n Definición de convergencia en probabilidad.  Sea \\(X_1,\\ldots,X_n,\\ldots\\) una sucesión de variables aleatorias y sea \\(X\\) una variable aleatoria definida sobre el mismo espacio muestral \\(\\Omega\\) y con la misma probabilidad de sucesos. Diremos que la sucesión \\(\\{X_n\\}_{n=1}^\\infty\\) converge en probabilidad hacia \\(X\\) si para cualquier valor \\(\\epsilon &gt;0\\), \\[\n\\lim_{n\\to\\infty} P(|X_n-X|&gt;\\epsilon \\})=0.\n\\] Lo denotaremos por \\(X_n\\stackrel{c.p.}{\\longrightarrow}X\\).\nEl límite de la probabilidad de los sucesos formados por los \\(w\\in\\Omega\\) tal que \\(|X_n(w)-X(w)|&gt;\\epsilon\\) vale 0.\nObservación. Una definición equivalente de convergencia en probabilidad es que para todo valor \\(\\epsilon &gt;0\\), \\[\n\\lim_{n\\to\\infty} P(|X_n(w)-X(w)|\\leq \\epsilon \\})=1.\n\\] Observación.  La convergencia casi segura implica la convergencia en probabilidad ya que si la sucesión \\(\\{X_n\\}_{n=1}^\\infty\\) converge casi seguramente hacia \\(X\\), la serie \\(\\sum_{n=1}^\\infty P(|X_n-X|&gt;\\epsilon)\\) será convergente y, por tanto, el límite de su término \\(P(|X_n-X|&gt;\\epsilon)\\) tenderá a cero, hecho que equivale a la convergencia en probabilidad.\nEl siguiente resultado nos puede ayudar algunas veces a comprobar la convergencia en probabilidad:\nProposición.  Sea \\(X_1,\\ldots,X_n,\\ldots\\) una sucesión de variables aleatorias. Sea \\(\\mu_n\\) el valor medio de la variable \\(X_n\\), \\(E(X_n)=\\mu_n\\) y \\(\\sigma_n^2\\) su varianza: \\(\\mathrm{Var}(X_n)=\\sigma_n^2\\). Supongamos que \\(\\lim_{n\\to\\infty}\\sigma_n^2=0\\). Entonces, \\[\nX_n-\\mu_n\\stackrel{c.p.}{\\longrightarrow} 0.\n\\]\n\nDemostración\nUsando la desigualdad de Chebyschev, podemos escribir: \\[\nP(|X_n-\\mu_n|&gt;\\epsilon \\}) \\leq \\frac{\\sigma_n^2}{\\epsilon^2}.\n\\] Tomando límite a cada parte de la desigualdad anterior tenemos: \\[\n0\\leq \\lim_{n\\to\\infty} P(|X_n-X|&gt;\\epsilon \\}) \\leq \\lim_{n\\to\\infty}\\frac{\\sigma_n^2}{\\epsilon^2}=0,\n\\] de donde deducimos que \\(\\lim_{n\\to\\infty} P(|X_n-X|&gt;\\epsilon \\})=0\\), tal como queríamos ver.\n\n\nEjemplo del lanzamiento de un dado (continuación)\nEn el ejemplo anterior del lanzamiento de un dado, no hay convergencia en probabilidad ya que comprobamos que para \\(0&lt;\\epsilon&lt;1\\),\n\n\\[\nP(|X_n-X|&gt;\\epsilon)=\\frac{5}{6}.\n\\] Por tanto, para \\(0&lt;\\epsilon&lt;1\\), \\(\\lim_{n\\to\\infty} P(|X_n-X|&gt;\\epsilon \\})=\\frac{5}{6}\\neq 0.\\)\n\n\n\nEjemplo: covergencia en probabilidad distribucione exponenciales\nConsideremos las variables aleatorias \\(X_n\\) con función de densidad: \\[\nf_{X_n}(x)=\\begin{cases}\n\\lambda\\cdot  n\\cdot\\mathrm{e}^{-\\lambda\\cdot n\\cdot  x}, & \\mbox{si }x&gt;0,\\\\\n0, & \\mbox{en caso contrario.}\n\\end{cases}\n\\] Estas variables \\(X_n\\) tienen distribución exponencial de parámetro \\(\\lambda\\cdot n\\).\nVeamos que \\(\\{X_n\\}_{n=1}^\\infty\\stackrel{c.p}{\\longrightarrow} 0\\).\n\nDado \\(\\epsilon &gt;0\\), calculemos \\(P(|X_n|&gt;\\epsilon \\})\\): \\[\nP(|X_n|&gt;\\epsilon \\}) = \\int_\\epsilon^\\infty \\lambda\\cdot  n\\cdot \\mathrm{e}^{-\\lambda\\cdot  n x}\\, dx =\\lambda\\cdot  n\\cdot  \\left[\\frac{1}{-\\lambda \\cdot n}\\cdot\\mathrm{e}^{-\\lambda\\cdot  n\\cdot  x}\\right]_\\epsilon^\\infty =\\mathrm{e}^{-\\lambda\\cdot  n\\cdot  \\epsilon}\\stackrel{n\\to\\infty}{\\longrightarrow} 0,\n\\] tal como queríamos ver.\n\n\n\n\n8.2.3 Convergencia en ley o en distribución\n Definición de convergencia en ley o distribución.  Sea \\(X_1,\\ldots,X_n,\\ldots\\) una sucesión de variables aleatorias y sea \\(X\\) una variable aleatoria definida sobre el mismo espacio muestral \\(\\Omega\\) y con la misma probabilidad de sucesos. Sea \\(F_{X_n}\\) y \\(F_X\\) las funciones de distribución de la variable \\(X_n\\) y \\(X\\), respectivamente. Diremos que la sucesión \\(\\{X_n\\}_{n=1}^\\infty\\) converge en ley, o en distribución hacia \\(X\\) si, \\[\n\\lim_{n\\to\\infty} F_{X_n}(x)=F(x),\n\\] para todo valor \\(x\\in\\mathbb{R}\\).\nLo denotaremos por \\(X_n\\stackrel{{\\cal L}}{\\longrightarrow}X\\).\nEl resultado siguiente simplifica algunas veces comprobar que la sucesión \\(X_n\\) converge en ley hacia \\(X\\):\n Proposición.  Sea \\(\\{X_n\\}_{n=1}^\\infty\\) una sucesión de variables aleatorias y sea \\(X\\) una variable aleatoria definida sobre el mismo espacio muestral \\(\\Omega\\) y con la misma probabilidad de sucesos. Sean \\(\\phi_{X_n}\\) y \\(\\phi_X\\) las funciones características de \\(X_n\\) y \\(X\\), respectivamente. Entonces, la sucesión converge en ley hacia \\(X\\), \\(X_n\\stackrel{{\\cal L}}{\\longrightarrow}X\\), si, y sólo si, \\[\n\\lim_{n} \\phi_{X_n}(t) = \\phi_X(t),\n\\] para cualquier número \\(t\\in\\mathbb{R}\\).\n\nEjemplo de la distribución binomial \\(B(n,p)\\)\nVeamos que si \\(X_n=B(n,p_n)\\) tiene distribución binomial de parámetros \\(n\\) y \\(p_n\\), con \\(p_n=\\frac{\\lambda}{n}\\), con \\(\\lambda\\) fijo, \\[\nB(n,p)\\stackrel{{\\cal L}}{\\longrightarrow}Poiss(\\lambda).\n\\]\n\nEn el tema de distribuciones notables demostramos que para todo \\(k\\in\\{0,\\ldots,n\\}\\), \\[\nP(X_n = k)=\\binom{n}{k}\\cdot p_n^k\\cdot (1-p_n)^{n-k}\\stackrel{n\\to\\infty}{\\longrightarrow} P(X=k)=\\frac{\\lambda^k}{k!}\\cdot\\mathrm{e}^{-\\lambda}.\n\\] Entonces tenemos que dado \\(x\\in\\mathbb{R}\\), existe \\(k\\in\\{0,\\ldots,n\\}\\), tal que \\(k\\leq x&lt; k+1\\). Por tanto, \\[\n\\begin{array}{rl}\n\\lim\\limits_{n\\to\\infty} F_{X_n}(x)\n& = \\lim\\limits_{n\\to\\infty} F_{X_n}(k)=\\lim\\limits_{n\\to\\infty} P(X_n=0)+\\cdots + P(X_n=k) \\\\\n& =\\lim\\limits_{n\\to\\infty} P(X_n=0)+\\cdots + \\lim\\limits_{n\\to\\infty} P(X_n=k)\\\\\n& = P(X=0)+\\cdots + P(X=k)\\\\ &  =F_X(k)=F_X(x),\n\\end{array}\n\\] tal como queríamos demostrar.\n\n\n\n8.2.4 Relaciones entre las distintas convergencias\nEl resultado siguiente nos dice cuando un tipo de convergencia implica la otra:\n Proposición.  Sea \\(\\{X_n\\}_{n=1}^\\infty\\) una sucesión de variables aleatorias y sea \\(X\\) una variable aleatoria definida sobre el mismo espacio muestral \\(\\Omega\\) y con la misma probabilidad de sucesos. Entonces:\n\nSi \\(X_n\\stackrel{c.s.}{\\longrightarrow} X\\), entonces \\(X_n\\stackrel{c.p.}{\\longrightarrow} X\\).\nSi \\(X_n\\stackrel{c.p.}{\\longrightarrow} X\\), entonces \\(X_n\\stackrel{{\\cal L}}{\\longrightarrow} X\\).\n\nEn resumen, la convergencia más fuerte es la casi segura, luego vendría la convergencia en probabilidad y, por último, la convergencia en ley:\n\\[\n\\mbox{Conv. casi segura }\\Rightarrow \\mbox{ Conv. en probabilidad }\\Rightarrow\\mbox{ Conv. en ley.}\n\\]"
  },
  {
    "objectID": "7.html#leyes-de-los-grandes-números",
    "href": "7.html#leyes-de-los-grandes-números",
    "title": "8  Ley de los grandes números y Teorema Central del Límite",
    "section": "8.3 Leyes de los grandes números",
    "text": "8.3 Leyes de los grandes números\nComo ya comentamos al principio del tema, las leyes de los grandes números estudian el comportamiento de la media muestral \\(\\overline{X}_n\\) cuando la sucesión de variables aleatorias \\(\\{X_n\\}_{n=1}^\\infty\\) se va hacia infinito.\nMás concretamente, diremos que una sucesión de variables aleatorias \\(\\{X_n\\}_{n=1}^\\infty\\) cumple una ley de los grandes números si existe un sucesión numérica \\((a_n)_n\\) tal que la sucesión de variables aleatorias \\(\\{\\overline{X}_n-a_n\\}\\) converge “de alguna manera” de las que hemos visto hacia 0.\nSi este “alguna manera” es la convergencia más fuerte, o la casi segura, tendremos la ley fuerte de los grandes números.\nEn cambio, si la convergencia es en probabilidad, tendremos la ley débil de los grandes números.\n\n8.3.1 Leyes débiles de los grandes números\n Teorema. Ley débil de los grandes números.  Sea \\(\\{X_n\\}_{n=1}^\\infty\\) una sucesión de variables aleatorias independientes dos a dos tal que sus varianzas existen y están acotadas por una constante independiente de \\(n\\). Entonces, \\[\n\\overline{X}_n-\\frac{1}{n}\\sum_{i=1}^n \\mu_i\\stackrel{c.p.}{\\longrightarrow} 0,\n\\] donde \\(\\mu_i = E(X_i)\\).\nDicho en otras palabras: en las condiciones de la proposición anterior, la diferencia entre la sucesión de medias muestrales como variables aleatorias y la sucesión numérica de la medias poblacionales de dichas variables aleatorias tiende en probabilidad hacia 0.\n\nDemostración\nComo las variables son independientes dos a dos la varianza de la suma es la suma de las varianzas: \\[\n\\mathrm{Var}(\\overline{X}_n)=\\frac{1}{n^2}\\mathrm{Var}(\\sum_{i=1}^n X_i)=\\frac{1}{n^2}\\sum_{i=1}^n \\sigma_i^2,\n\\] donde \\(\\sigma_i^2 = \\mathrm{Var}(X_i)\\).\nSabemos por hipótesis que existe una constante \\(M\\) tal que \\(\\sigma_i^2\\leq M\\) para todo \\(i\\). Por tanto, \\[\n\\mathrm{Var}(\\overline{X}_n)=\\frac{1}{n^2}\\sum_{i=1}^n \\sigma_i^2\\leq \\frac{1}{n^2}Mn =\\frac{M}{n}.\n\\]\nEl valor del valor medio de la media muestral será: \\[\nE(\\overline{X}_n)=\\frac{1}{n}\\sum_{i=1}^n E(X_i)=\\frac{1}{n}\\sum_{i=1}^n \\mu_i.\n\\] Usando la desigualdad de Chebyschev, deducimos, dado un \\(\\epsilon &gt;0\\): \\[\nP\\left(\\left|\\overline{X}_n-\\frac{1}{n}\\sum_{i=1}^n \\mu_i\\right|&gt;\\epsilon\\right) \\leq \\frac{\\mathrm{Var}(\\overline{X}_n)}{\\epsilon^2}\\leq \\frac{M}{n\\epsilon^2}.\n\\] Por tanto, tomando límites en las dos partes de la desigualdad anterior, deducimos \\[\n\\lim_{n\\to \\infty}P\\left(\\left|\\overline{X}_n-\\frac{1}{n}\\sum_{i=1}^n \\mu_i\\right|&gt;\\epsilon\\right) =0,\n\\] tal como queríamos ver.\n\nDel teorema anterior obtenemos las consecuencias siguientes:\nCorolario.  Sea \\(\\{X_n\\}_{n=1}^\\infty\\) una sucesión de variables aleatorias independientes dos a dos tal que todas tienes la misma esperanza \\(\\mu\\) y la misma varianza \\(\\sigma^2\\). Entonces, \\[\n\\overline{X}_n\\stackrel{c.p.}{\\longrightarrow} \\mu,\n\\]\n\nDemostración\nEn este caso tenemos que \\(\\mu_i=\\mu\\) y, por tanto, \\(\\frac{1}{n}\\sum\\limits_{i=1}^n \\mu_i =\\frac{1}{n}\\cdot n\\mu=\\mu\\). Si aplicamos el teorema de la ley débil de los grandes números nos sale el resultado enunciado.\n\nCorolario.  Sea \\(\\{X_n\\}_{n=1}^\\infty\\) una sucesión de variables aleatorias independientes dos a dos e idénticamente distribuidas tal que todas tienes la misma esperanza \\(\\mu\\). Entonces, \\[\n\\overline{X}_n\\stackrel{c.p.}{\\longrightarrow} \\mu,\n\\]\n\nDemostración\nTrivial a partir del Corolario anterior.\n\n\nEjemplo: lanzamiento de una moneda\nVamos a simular la ley débil de los grandes números en el caso en que el experimento aleatorio sea el lanzamiento de una moneda.\nEn este caso, tendremos que las variables aleatorias \\(X_n\\) tendrán distribución de Bernoulli de parámetro \\(p=\\frac{1}{2}\\).\nLa variable \\(\\overline{X}_n\\) representa la proporción de caras (\\(X_n=1\\)) en el lanzamiento de la moneda \\(n\\) veces. Nos preguntamos si dicha proporción de caras tiende al parámetro \\(p\\) en probabilidad.\n\nVamos a hallar una muestra para cada variable \\(\\overline{X}_n=\\frac{\\sum\\limits_{i=1}^n X_i}{n}\\).\nPara ello, vamos a repetir el experimento de lanzar la moneda \\(N=100\\) veces y lo repetimos \\(k=500\\) ocasiones.\nLos resultados estarán en una matriz \\(k\\times N =500\\times 100\\) donde cada fila de la matriz simulará una repetición del experimento de lanzar la moneda \\(N=100\\) veces.\nDada la fila \\(i\\)-ésima, iremos calculando \\(\\overline{X}_1^{(i)},\\overline{X}_2^{(i)},\\ldots,\\overline{X}_{N=100}^{(i)}\\).\nLuego, fijado un \\(\\epsilon\\), para cada \\(n\\), aproximaremos la probabilidad \\(P\\left(\\left|\\overline{X}_n-\\frac{1}{2}\\right|&gt;\\epsilon\\right)\\) usando la fórmula de Laplace: \\[\np_n=P\\left(\\left|\\overline{X}_n-\\frac{1}{2}\\right|&gt;\\epsilon\\right) \\approx\\frac{\\#\\left\\{\\mbox{$i$ tal que  $\\left|\\overline{X}_n^{(i)}-\\frac{1}{2}\\right|&gt;\\epsilon$}\\right\\}}{k}.\n\\]\nPara comprobar dicha afirmación, la idea es hallar para cada valor \\(n\\), una muestra para cada variable \\(\\overline{X}_n=\\frac{\\sum\\limits_{i=1}^n X_i}{n}\\).\nPara hallar una muestra de cada variable \\(\\overline{X}_n\\), seguimos los pasos siguientes:\n\nEn primer lugar, simulamos la repetición del experimento de lanzar la moneda \\(N=100\\) veces y lo repetimos \\(k=500\\) ocasiones. Los resultados estarán en una matriz \\(k\\times N =500\\times 100\\) donde cada fila de la matriz simulará una repetición del experimento de lanzar la moneda \\(N=100\\) veces:\n\n\nN=100\nk=500\nset.seed(2019) \n## fijamos la semila de aleatoriedad\n## por reproducibilidad\nvalores.experimento=matrix(sample(c(0,1),N*k,replace=TRUE),k,N)\n\nLos primeros resultados son:\n\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12]\n[1,]    0    1    1    1    1    1    1    0    1     1     0     0\n[2,]    0    0    0    0    0    1    0    1    1     0     1     1\n[3,]    1    1    1    0    0    1    1    0    0     1     0     1\n[4,]    0    0    1    1    1    0    0    0    1     1     0     1\n[5,]    0    1    1    1    1    1    0    1    0     1     1     1\n\n\n…\n\nEn segundo lugar, dada la fila \\(i\\)-ésima de la matriz anterior, iremos calculando \\(\\overline{X}_1^{(i)},\\overline{X}_2^{(i)},\\ldots,\\overline{X}_{N=100}^{(i)}\\) guardando los resultados en una matriz de medias muestrales. Antes de nada, creamos la función que nos realizará la operación anterior dado un vector cualquiera x:\n\n\ncálculo.xnbarra = function(x){\n  return(cumsum(x)/(1:length(x)))\n}\n\nA partir de la matriz de los resultados, aplicamos la función anterior a cada fila y hallaremos una matriz con todas las \\(\\overline{X}_n^{(i)}\\):\n\nmatriz.medias.muestrales = t(apply(valores.experimento,\n                                   1,cálculo.xnbarra))\n\nLa columna \\(j\\)-ésima de la matriz matriz.medias.muestrales contiene una muestra de \\(k=500\\) valores de la variable \\(\\overline{X}_j\\).\n\nEn último lugar, fijado un \\(\\epsilon\\), para cada \\(n\\), aproximaremos la probabilidad \\(P\\left(\\left|\\overline{X}_n-\\frac{1}{2}\\right|&gt;\\epsilon\\right)\\) usando la fórmula de Laplace: \\[\np_n=P\\left(\\left|\\overline{X}_n-\\frac{1}{2}\\right|&gt;\\epsilon\\right) \\approx\\frac{\\#\\left\\{\\mbox{$i$ tal que  $\\left|\\overline{X}_n^{(i)}-\\frac{1}{2}\\right|&gt;\\epsilon$}\\right\\}}{k}.\n\\] La columna \\(j\\)-ésima de la matriz matriz.medias.muestrales es una muestra de la variable \\(\\overline{X}_j\\). Por tanto, para hallar la aproximación de \\(p_n\\), miramos cuántos valores de la columna \\(j\\)-ésima de la matriz anterior verifican \\(\\left|\\overline{X}_j^{l}-\\frac{1}{2}\\right|&gt;\\epsilon\\), para \\(l=1,\\ldots, k\\):\n\n\nepsilon=0.1\nprobabilidades.pn= colSums(abs(matriz.medias.muestrales-0.5) &gt; epsilon)/k\n\nPara ver los resultados, dibujamos el gráfico \\(n\\) vs. \\(p_n\\):\n\n\n\n\n\n\n\nObservamos que las probabilidades tienden a cero tal como nos dice el Teorema de la ley débil de los grandes números.\n\n\n\n\n8.3.2 Convergencia de los momentos muestrales\n´Dada una sucesión de variables aleatorias, definimos los momentos muestrales de la forma siguiente:\n Definición de los momentos muestrales de una sucesión de variables aleatorias. Sea \\(\\{X_n\\}_{n=1}^\\infty\\) una sucesión de variables aleatorias. Dado \\(k\\) valor entero positivo, definimos el momento muestral de orden \\(k\\) como la sucesión de variables aleatorias siguientes: \\[\nM_k^{(n)} = \\frac{1}{n}\\sum_{i=1}^n X_i^k.\n\\]\n Observación:  el momento muestral de orden \\(k=1\\) es la media muestral \\(\\overline{X}_n\\).\n Definición de los momentos muestrales centrados de una sucesión de variables aleatorias. Sea \\(\\{X_n\\}_{n=1}^\\infty\\) una sucesión de variables aleatorias. Dado \\(k\\) valor entero positivo, definimos el momento muestral centrado en la media de orden \\(k\\) como la sucesión de variables aleatorias siguientes: \\[\nMC_k^{(n)} = \\frac{1}{n}\\sum_{i=1}^n (X_i-\\overline{X}_n)^k.\n\\]\n Observación:  el momento muestral centrado en la media de orden \\(k=2\\) es la varianza muestral \\(S_{X_n}^2\\).\n Definición de la covarianza y el coeficiente de correlación muestral de una sucesión de variables aleatorias. Sea \\(\\{(X_n,Y_n)\\}_{n=1}^\\infty\\) una sucesión de variables aleatorias bidimensionales. Definimos la covarianza muestral como la sucesión de variables aleatorias siguientes: \\[\nS_{X_n,Y_n} = \\frac{1}{n}\\sum_{i=1}^n (X_i-\\overline{X}_n)(Y_i-\\overline{Y}_n),\n\\] y el coeficiente de correlación muestral como la sucesión de variables aleatorias siguientes: \\[\nR_{X_n,Y_n}=\\frac{S_{X_n,Y_n}}{\\sqrt{S_{X_n}^2 S_{Y_n}^2}}.\n\\]\nDada \\(\\{X_n\\}_{n=1}^\\infty\\) una sucesión de variables aleatorias y \\(k\\) un valor entero positivo, en el tema de Complementos de variables aleatorias, definimos los momentos y los momentos centrales de orden \\(k\\) para cada de dichas variables como: \\[\nm_k^{(n)} = E\\left(X_n^k\\right),\\quad\\mu_k^{(n)}=E\\left(\\left(X_n-\\mu_n\\right)^k\\right),\n\\] donde \\(\\mu_n\\) es el valor medio de la variable \\(X_n\\): \\(\\mu_n = E(X_n)\\).\nAsí mismo, dada \\(\\{(X_n,Y_n)\\}_{n=1}^\\infty\\) una sucesión de variables aleatorias bidimensionales, en el tema de variables aleatorias bidimensionales definimos para cada variable \\((X_n,Y_n)\\) la covarianza \\(\\sigma_{X_nY_n}\\) y el coeficiente de correlación \\(\\rho_{X_nY_n}\\): \\[\n\\sigma_{X_nY_n}=E((X_n-\\mu_{X_n})(Y_n-\\mu_{Y_n})),\\quad \\rho_{X_nY_n}=\\frac{\\sigma_{X_nY_n}}{\\sqrt{\\sigma_{X_n}^2\\sigma_{Y_n}^2}}.\n\\]\nDada una sucesión de variables aleatorias \\(\\{X_n\\}_{n=1}^\\infty\\), el resultado siguiente nos relaciona los momentos muestrales y los momentos muestrales centrados en la media con los momentos y los momentos centrales de cada variable:\n Teorema. Convergencia de los momentos muestrales y los momentos muestrales centrados en la media.  Sea \\(\\{X_n\\}_{n=1}^\\infty\\) una sucesión de variables aleatorias independientes dos a dos e idénticamente distribuidas y dado un entero positivo \\(k\\), supongamos que para cada \\(n\\), existe el momento de orden \\(k\\), \\(m_k\\) y el momento central de orden \\(k\\), \\(\\mu_k\\), que no dependerán de \\(n\\) al ser idénticamente distribuidas. Entonces las sucesiones de variables aleatorias \\(\\{M_n^{(k)}\\}_{n=1}^\\infty\\) y \\(\\{MC_n^{(k)}\\}_{n=1}^\\infty\\) tienden a \\(m_k\\) y \\(\\mu_k\\), respectivamente, en probabilidad \\[\nM_n^{(k)}\\stackrel{c.p.}{\\longrightarrow} m_k,\\quad MC_n^{(k)}\\stackrel{c.p.}{\\longrightarrow} \\mu_k.\n\\]\n\nDemostración\nConsideremos la sucesión de variables aleatorias \\(\\{X_n^k\\}_{n=1}^\\infty\\). Como las variables aleatorias de la sucesión \\(\\{X_n\\}_{n=1}^\\infty\\) son independientes dos a dos e idénticamente distribuidas, las variables de la sucesión \\(\\{X_n^k\\}_{n=1}^\\infty\\) también lo serán.\nLa idea es aplicar la ley débil de los grandes números a la sucesión anterior.\nEl valor medio de cada variable de la sucesión \\(\\{X_n^k\\}_{n=1}^\\infty\\) será: \\(\\tilde{\\mu}_n^{(k)}= E(X_n^{k})=m_k\\) el momento de orden \\(k\\).\nEntonces, si hacemos \\(\\frac{1}{n}\\sum\\limits_{i=1}^n \\tilde{\\mu}_n^{(k)}\\) obtenemos: \\(\\frac{1}{n} n\\cdot m_k=m_k.\\)\nAplicando la ley débil de los grandes números a la sucesión \\(\\{X_n^k\\}_{n=1}^\\infty\\), tendremos que \\[\n\\overline{X^k}_n \\stackrel{c.p.}{\\longrightarrow}m_k,\n\\] pero \\(\\overline{X^k}_n\\) vale: \\[\n\\overline{X^k}_n=\\frac{1}{n}\\sum_{i=1}^n X_i^k,\n\\] variable aleatoria que coincide con el momento muestral de orden \\(k\\), \\(M_n^{(k)}\\), tal como queríamos demostrar.\nDejamos como ejercicio la demostración de los momentos centrales. Razonando de la misma manera, no tiene dificultad alguna.\n\nEnunciemos ahora el resultado para las covarianzas y las correlaciones muestrales:\n Teorema: convergencia de la covarianza y el coeficiente de correlación muestrales.  Sea \\(\\{(X_n,Y_n)\\}_{n=1}^\\infty\\) una sucesión de variables aleatorias bidimensionales independientes dos a dos e idénticamente distribuidas. Sea \\(\\sigma_{X,Y}, \\rho_{XY}\\) la covarianza y el coeficiente de correlación de cada par de variables que, al ser idénticamente distribuidas, no dependen de \\(n\\). Entonces las sucesiones de las covarianzas muestrales \\(\\{S_{X_n,Y_n}\\}_{n=1}^\\infty\\) y los coeficientes de correlación muestrales \\(\\{R_{X_nY_n}\\}_{n=1}^\\infty\\) tienden en probabilidad hacia \\(\\sigma_{XY}\\) y \\(\\rho_{XY}\\), respectivamente: \\[\nS_{X_n,Y_n}\\stackrel{c.p.}{\\longrightarrow}\\sigma_{XY},\\quad R_{X_nY_n}\\stackrel{c.p.}{\\longrightarrow}\\rho_{XY}.\n\\]\n\nDemostración\nPara la demostración basta aplicar la ley débil de los grandes números a las sucesiones \\(\\{S_{X_n,Y_n}\\}_{n=1}^\\infty\\) y \\(\\{R_{X_nY_n}\\}_{n=1}^\\infty\\). Dejamos los detalles como ejercicio.\n\n\n\n8.3.3 Leyes fuertes de los grandes números\nVamos a dar una versión de la ley débil de los grandes números pero en lugar de tener convergencia en probabilidad, tendremos convergencia casi segura.\n Teorema de Kolmogorov. Ley fuerte de los grandes números.  Sea \\(\\{X_n\\}_{n=1}^\\infty\\) una sucesión de variables aleatorias independientes y con varianza \\(\\sigma_n^2\\). Supongamos que la serie \\(\\sum\\limits_{n=1}^\\infty \\frac{\\sigma_n^2}{n^2},\\) es convergente. Entonces la sucesión de las medias muestrales \\(\\{\\overline{X}_n\\}_{n=1}^\\infty\\) cumplen la llamada ley fuerte de los grandes números: \\[\n\\overline{X}_n-\\frac{1}{n}\\sum_{i=1}^n \\mu_i \\stackrel{c.s.}{\\longrightarrow} 0.\n\\]\nAsociados al resultado anterior tenemos los corolarios siguientes:\n Corolario.  Sea \\(\\{X_n\\}_{n=1}^\\infty\\) una sucesión de variables aleatorias independientes y con varianza \\(\\sigma_n^2\\). Supongamos que existe una constante \\(M\\) tal que todas las varianzas están acotadas por \\(M\\): \\(\\sigma_n^2\\leq M\\), para todo \\(n\\). Entonces la sucesión de las medias muestrales \\(\\{\\overline{X}_n\\}_{n=1}^\\infty\\) cumplen la llamada ley fuerte de los grandes números: \\[\n\\overline{X}_n-\\frac{1}{n}\\sum_{i=1}^n \\mu_i \\stackrel{c.s.}{\\longrightarrow} 0.\n\\]\n\nDemostración\nSi \\(\\sigma_n^2\\leq M\\) para todo \\(n\\), la serie numérica \\(\\sum\\limits_{n=1}^\\infty \\frac{\\sigma_n^2}{n^2}\\) será convergente ya que, por el criterio de acotación, \\[\n\\sum\\limits_{n=1}^\\infty \\frac{\\sigma_n^2}{n^2}\\leq M\\sum\\limits_{n=1}^\\infty \\frac{1}{n^2},\n\\] que es convergente.\nEntonces aplicando el Teorema de Kolmogorov o la ley fuerte de los grandes números, tenemos el resultado.\n\n Corolario.  Sea \\(\\{X_n\\}_{n=1}^\\infty\\) una sucesión de variables aleatorias independientes de Bernoulli con el mismo parámetro \\(p\\) que es lo mismo que decir que son idénticamente distribuidas. Entonces la sucesión de las medias muestrales convergen casi seguramente hacia \\(p\\): \\[\n\\overline{X}_n \\stackrel{c.s.}{\\longrightarrow} p.\n\\]\n\nDemostración\nEn este caso: \\[\n\\frac{1}{n}\\sum_{i=1}^n \\mu_i = \\frac{1}{n}\\cdot n\\cdot p=p.\n\\] También se verifica que \\(\\sigma_n^2 =p(1-p)\\). Por tanto, existe una constante \\(M\\) (\\(M=p(1-p)\\)) tal que \\(\\sigma_n^2\\leq M\\). Aplicando el Corolario anterior, obtenemos el resultado.\n\n\nEjemplo: lanzamiento moneda (continuación)\nVamos a repetir el ejemplo de las variables aleatorias de Bernoulli \\(X_n\\), todas de parámetro \\(p=\\frac{1}{2}\\) y comprobar que las proporciones de caras cuando lanzamos la moneda \\(n\\) veces, es decir, las medias muestrales \\(\\overline{X}_n\\) tienden casi seguramente hacia \\(p=\\frac{1}{2}\\).\n\nLa comprobación anterior es equivalente a ver que la serie: \\[\n\\sum_{n=1}^\\infty P(|\\overline{X}_n-p|&gt;\\epsilon),\n\\] es convergente fijado \\(\\epsilon &gt;0\\).\nRecordemos que en la variable probabilidades.pn calculábamos las probabilidades \\(P(|\\overline{X}_n-p|&gt;\\epsilon)\\) para un \\(\\epsilon =0.1\\).\nComprobar que la serie anterior es convergente es equivalente a comprobar que las sumas parciales convergen:\n\ncumsum(probabilidades.pn)\n\nEl problema es que la n y la N escogidas son demasiado pequeñas. Para realizar el experimento actual tenéis que considerar n=1000 y N=5000. Id con cuidado que el programa os tardará un rato.\nEl gráfico de las sumas parciales se muestra a continuación:\n\n\nN=1000\nplot(1:N,cumsum(probabilidades.pn),xlab=expression(n),\n     ylab=\"Sumas parciales\",col='red', type='l')\n\n\n\n\n\nComo se puede observar, la serie parece que converge."
  },
  {
    "objectID": "7.html#teorema-central-del-límite",
    "href": "7.html#teorema-central-del-límite",
    "title": "8  Ley de los grandes números y Teorema Central del Límite",
    "section": "8.4 Teorema Central del Límite",
    "text": "8.4 Teorema Central del Límite\nSabemos que si una sucesión \\(\\{X_n\\}\\) está formada por variables normales, la sucesión de medias muestrales \\(\\left\\{\\overline{X}_n=\\frac{\\sum\\limits_{i=1}^n X_i}{n}\\right\\}_{n=1}^\\infty\\) también son normales ya que vimos en el tema de variables multidimensionales que si aplicamos una transformación afín (y, en particular, lineal) a una variable normal multidimensional, el resultado es una normal.\nPara calcular la variable \\(\\overline{X}_n\\), es obvio que la transformación lineal es la siguiente: \\[\n\\overline{X}_n = \\left(\\frac{1}{n},\\ldots,\\frac{1}{n}\\right)\\cdot \\begin{pmatrix}X_1 \\\\\\vdots\\\\ X_n\\end{pmatrix}.\n\\] Si además la sucesión de variables \\(X_n\\) son normales todas con media \\(\\mu\\) y varianza \\(\\sigma^2\\), la sucesión \\(\\left\\{\\overline{X}_n\\right\\}_{n=1}^\\infty\\) serán normales de media \\(\\mu\\) y varianza \\(\\frac{\\sigma^2}{n}\\).\nEstandarizando las variables anteriores, podemos concluir que las variables medias estandarizadas \\(Z_n =\\left\\{\\frac{\\overline{X}_n-\\mu}{\\frac{\\sigma}{\\sqrt{n}}}\\right\\}_{n=1}^\\infty\\) todas son \\(N(0,1)\\).\nEl Teorema Central del Límite generaliza el resultado anterior en el sentido de que si las variables \\(X_n\\) no tienen por qué tener la distribución normal pero son independientes e idénticamente distribuidas, las variables \\(Z_n\\) correspondientes tienden en ley a una distribución normal estándar \\(N(0,1)\\).\nEn general, se dice que los valores medios de cualquier secuencia de números aproximadamente corresponde a una muestra de una normal.\n\n8.4.1 Teorema Central del Límite\n Teorema Central del Límite  Sea \\(\\{X_n\\}_{n=1}^\\infty\\) una sucesión de variables aleatorias independientes e idénticamente distribuidas con \\(E(X_n)=\\mu\\) y \\(\\mathrm{Var}(X_n)=\\sigma^2\\) para todo \\(n\\). Entonces: \\[\n\\frac{\\sum\\limits_{i=1}^n X_i-n\\mu}{\\sigma\\sqrt{n}}\\stackrel{{\\cal L}}{\\longrightarrow} N(0,1).\n\\]\n Observación.  Una condición equivalente a la tesis del Teorema Central del Límite es: \\[\n\\frac{\\overline{X}_n-\\mu}{\\frac{\\sigma}{\\sqrt{n}}}\\stackrel{{\\cal L}}{\\longrightarrow} N(0,1).\n\\] Basta dividir por \\(n\\) el numerador y el denominador de la tesis original del Teorema Central del Límite.\nPara la demostración, usaremos dos propiedades de la función característica:\n Proposición.  Sea \\(Y_1,\\ldots, Y_n\\) \\(n\\) variables aleatorias independientes. Sea \\(S_n\\) la variable aleatoria suma de las variables anteriores, \\(S_n=\\sum\\limits_{i=1}^n Y_i\\). Entonces, para calcular \\(\\phi_{S_n}\\), podemos usar la expresión siguiente:: \\[\n\\phi_{S_n}(w)=\\phi_{Y_1}(w)\\cdots \\phi_{Y_n}(w),\n\\] donde \\(w\\) es cualquier valor real.\n\nDemostración de la proposición\nPor definición: \\[\n\\begin{array}{rl}\n\\phi_{S_n}(w) & =E\\left(\\mathrm{e}^{\\mathrm{i} w S_n}\\right)=E\\left(\\mathrm{e}^{\\mathrm{i} w \\sum\\limits_{i=1}^n Y_i}\\right) = E\\left(\\mathrm{e}^{i w Y_1}\\cdots \\mathrm{e}^{i w Y_n}\\right)\\\\ & \\stackrel{\\mbox{$Y_1,\\ldots,Y_n$ son independientes}}{=} E\\left(\\mathrm{e}^{i w Y_1}\\right)\\cdots E\\left(\\mathrm{e}^{i w Y_n}\\right) \\\\\n& =\\phi_{Y_1}(w)\\cdots \\phi_{Y_n}(w).\n\\end{array}\n\\]\n\n Proposición.  Sea \\(Y\\) una variable aleatoria. Sea \\(U=kY\\) la variable aleatoria \\(Y\\) multiplicada por un valor real \\(k\\). Entonces, para calcular \\(\\phi_{U}\\), podemos usar la expresión siguiente: \\[\n\\phi_{U}(w)=\\phi_Y(kw),\n\\] donde \\(w\\) es cualquier valor real.\n\nDemostración de la proposición\nPor definición: \\[\n\\phi_{U}(w)=E\\left(\\mathrm{e}^{\\mathrm{i} w U}\\right) = E\\left(\\mathrm{e}^{\\mathrm{i} w k Y}\\right)=\\phi_Y(kw).\n\\]\n\n\nDemostración del Teorema Central del Límite\nUsando la proposición que vimos al introducir la convergencia en ley que dice que una sucesión \\(\\{X_n\\}\\) converge en ley hacia \\(X\\) si, y sólo si, \\(\\lim\\limits_{\\phi_{X_n}(t)}=\\phi_{X}(t)\\), donde \\(\\phi\\) representa la función característica y la condición anterior tiene que verificarse para todo valor \\(t\\in\\mathbb{R}\\), basta demostrar que, si tomamos \\(Z_n = \\frac{\\sum\\limits_{i=1}^n X_i-n\\mu}{\\sigma\\sqrt{n}},\\)\n\\[\n\\lim_{n\\to \\infty}\\phi_{Z_n}(w)=\\phi_Z(w),\n\\] para cualquier valor \\(w\\in\\mathbb{R}\\), siendo \\(Z=N(0,1)\\).\nSeguidamente, simplifiquemos la expresión \\(\\phi_{Z_n}(w)=\\phi_{\\frac{\\sum\\limits_{i=1}^n X_i-n\\mu}{\\sigma\\sqrt{n}}}(w)\\) usando las dos proposiciones anteriores. En primer lugar, teniendo en cuenta que las variables \\(\\left\\{\\frac{X_i-\\mu}{\\sigma\\sqrt{n}}\\right\\}\\) son independientes e idénticamente distribuidas, usando la primera proposición podemos escribir: \\[\n\\phi_{Z_n}(w) = \\left(\\phi_{\\frac{X-\\mu}{\\sigma\\sqrt{n}}}(w)\\right)^n,\n\\] donde \\(X\\) representa cualquiera de las variables \\(X_i\\).\nUsando la segunda proposición, podemos simplificar la expresión anterior aún más: \\[\n\\phi_{Z_n}(w) = \\left(\\phi_{\\frac{X-\\mu}{\\sigma\\sqrt{n}}}(w)\\right)^n = \\left(\\phi_{X-\\mu}\\left(\\frac{w}{\\sigma\\sqrt{n}}\\right)\\right)^n.\n\\]\nSi desarrollamos por Taylor alrededor del valor \\(\\hat{w}=0\\) la función característica \\(\\phi_{X-\\mu}\\left(\\hat{w}\\right)\\) hasta segundo orden, obtenemos: \\[\n\\scriptsize{\n\\phi_{X-\\mu}\\left(\\hat{w}\\right) = \\phi_{X-\\mu}\\left(0\\right)+ \\phi_{X-\\mu}'\\left(0\\right) \\hat{w}+ \\phi_{X-\\mu}''\\left(0\\right)\\frac{\\hat{w}^2}{2}+O(\\hat{w}^3),\n}\n\\] donde \\(O(\\hat{w}^3)\\) simboliza los términos de orden \\(\\hat{w}^3\\) y superiores.\nLos valores \\(\\phi_{X-\\mu}\\left(0\\right)\\), \\(\\phi_{X-\\mu}'\\left(0\\right)\\) y \\(\\phi_{X-\\mu}''\\left(0\\right)\\) valen: (ver tema de Complementos de variables aleatorias)\n\\[\n\\scriptsize{\n\\phi_{X-\\mu}\\left(0\\right)=1, \\ \\phi_{X-\\mu}'\\left(0\\right)=\\frac{1}{\\mathrm{i}}E(X-\\mu)=0,\\ \\phi_{X-\\mu}''\\left(0\\right)=\\frac{1}{\\mathrm{i}^2}E\\left((X-\\mu)^2\\right)=-\\sigma^2.\n}\n\\] El desarrollo anterior será: \\[\n\\phi_{X-\\mu}\\left(\\hat{w}\\right) =1 - \\frac{1}{2}\\hat{w}^2\\sigma^2+O(\\hat{w}^3),\n\\]\nAplicando la expresión anterior para \\(\\hat{w}=\\frac{w}{\\sigma\\sqrt{n}}\\), obtenemos:\n\\[\n\\scriptsize{\n\\phi_{X-\\mu}\\left(\\frac{w}{\\sigma\\sqrt{n}}\\right) =1 - \\frac{1}{2}\\left(\\frac{w}{\\sigma\\sqrt{n}}\\right)^2\\sigma^2+O\\left(\\frac{w}{\\sigma\\sqrt{n}}\\right)^3= 1-\\frac{w^2}{2n}+O\\left(\\frac{w^3}{n^{\\frac{3}{2}}}\\right),\n}\n\\]\nLa función característica de la variable \\(Z_n\\) será usando la expresión anterior: \\[\n\\scriptsize{\\phi_{Z_n}(w)=\\left(\\phi_{X-\\mu}\\left(\\frac{w}{\\sigma\\sqrt{n}}\\right)\\right)^n = \\left(1-\\frac{w^2}{2n}+O\\left(\\frac{w^3}{n^{\\frac{3}{2}}}\\right)\\right)^n}\n\\] El objetivo es calcular el límite de la expresión anterior: \\[\n\\lim_{n\\to \\infty}\\phi_{Z_n}(w) = \\lim_{n\\to\\infty} \\left(1-\\frac{w^2}{2n}+O\\left(\\frac{w^3}{n^{\\frac{3}{2}}}\\right)\\right)^n =\n\\lim_{n\\to \\infty}\\mathrm{e}^{n\\cdot \\ln \\left(1-\\frac{w^2}{2n}+O\\left(\\frac{w^3}{n^{\\frac{3}{2}}}\\right)\\right)}.\n\\]\nUsando que para \\(z\\approx 0\\), \\(\\ln(1-z)=z+O(z^2)\\), el límite anterior será: \\[\n\\lim_{n\\to \\infty}\\phi_{Z_n}(w) =\n\\lim_{n\\to \\infty}\\mathrm{e}^{n\\cdot \\left(-\\frac{w^2}{2n}+O\\left(\\frac{w^4}{n^{2}}\\right)\\right)} = \\lim_{n\\to \\infty}\\mathrm{e}^{ \\left(-\\frac{w^2}{2}+O\\left(\\frac{w^4}{n}\\right)\\right)} = \\mathrm{e}^{-\\frac{w^2}{2}},\n\\] y dicha expresión coincide con la función característica de la variable \\(N(0,1)\\), \\(\\phi_{Z}(w)\\).\nRecordad que en el tema de Complementos de variables aleatorias vimos que si la variable \\(U\\) era \\(N(\\mu,\\sigma)\\), \\(\\phi_{U}(w)=\\mathrm{e}^{\\mathrm{i}w\\mu-\\frac{w^2\\sigma^2}{2}}\\). Aplicando la fórmula anterior para \\(\\mu=0\\) y \\(\\sigma=1\\), obtenemos \\(\\phi_{Z}(w)=\\mathrm{e}^{-\\frac{w^2}{2}}.\\)\n\n\n\n8.4.2 Teorema Central del Límite en la práctica\nEl Teorema Central del Límite se aplica a la práctica en la forma siguiente:\nSea \\(\\{X_n\\}_{n=1}^\\infty\\) una sucesión de variables aleatorias independientes e idénticamente distribuidas con \\(E(X_i)=\\mu\\) y \\(\\mathrm{Var}(X_i)=\\sigma^2\\). Entonces, podemos aproximar para \\(n\\) grande (\\(n\\geq 30\\)), la media muestral \\(\\overline{X}_n\\) por: \\[\n\\overline{X}_n =\\frac{1}{n}\\sum_{i=1}^n X_i \\approx N\\left(\\mu,\\frac{\\sigma}{\\sqrt{n}}\\right),\n\\] o también: \\[\n\\sum_{i=1}^n X_i \\approx N\\left(n\\mu,\\sigma\\sqrt{n}\\right),\n\\]\nLas aproximaciones anteriores se pueden obtener teniendo en cuenta que el Teorema Central del Límite nos dice que la variable \\(Z_n= \\frac{\\sum\\limits_{i=1}^n X_i-n\\mu}{\\sigma\\sqrt{n}}\\) es aproximadamente una \\(N(0,1)\\). Por tanto, \\[\n\\frac{\\sum\\limits_{i=1}^n X_i-n\\mu}{\\sigma\\sqrt{n}} \\approx N(0,1),\\ \\Rightarrow \\sum_{i=1}^n X_i\\approx \\sigma\\sqrt{n}\\cdot N(0,1)+n\\mu = N\\left(n\\mu,\\sigma\\sqrt{n}\\right).\n\\]\nDividiendo por \\(n\\) la aproximación anterior, obtenemos: \\[\n\\overline{X}_n =\\frac{1}{n}\\sum_{i=1}^n X_i \\approx \\frac{1}{n}N\\left(n\\mu,\\sigma\\sqrt{n}\\right) =N\\left(\\mu,\\frac{\\sigma}{\\sqrt{n}}\\right).\n\\]\n\n\n8.4.3 Teorema de Moivre-Laplace\nSi aplicamos el Teorema Central del Límite en el caso en que las variables \\(X_n\\) son de Bernoulli de parámetro \\(p\\), obtenemos el llamado Teorema de Moivre-Laplace:\n Teorema de Moivre-Laplace  Sea \\(\\{X_n\\}_{n=1}^\\infty\\) una sucesión de variables aleatorias independientes Bernoulli de parámetro \\(p\\). La variable \\(\\sum\\limits_{i=1}^n X_i\\) será binomial de parámetros \\(n\\) y \\(p\\), \\(B(n,p)\\). Entonces: \\[\n\\frac{B(n,p)-np}{\\sqrt{n\\cdot p\\cdot (1-p)}}\\stackrel{{\\cal L}}{\\longrightarrow} N(0,1).\n\\]\nEn la práctica, decimos que podemos aproximar una variable binomial de parámetros \\(n\\) y \\(p\\) por una distribución normal de parámetros \\(\\mu=np\\) y \\(\\sigma =\\sqrt{n\\cdot p\\cdot (1-p)}\\): \\[\nB(n,p)\\approx N(np,\\sqrt{n\\cdot p\\cdot (1-p)}).\n\\]\n\n\n8.4.4 Aproximación de una suma de variables Poisson\nSi aplicamos el Teorema Central del Límite en el caso en que las variables \\(X_n\\) son de Poisson de parámetro \\(\\lambda\\), obtenemos el resultado siguiente:\n Proposición.  Sea \\(\\{X_n\\}_{n=1}^\\infty\\) una sucesión de variables aleatorias independientes Poisson de parámetro \\(\\lambda\\). Entonces: \\[\n\\frac{\\sum\\limits_{i=1}^n X_i -n\\lambda}{\\sqrt{n\\cdot \\lambda}}\\stackrel{{\\cal L}}{\\longrightarrow} N(0,1).\n\\]\nAntes de ver la aplicación práctica del resultado anterior, veamos que suma de variables Poisson independientes de parámetro \\(\\lambda\\) es una variable Poisson de parámetro \\(n\\lambda\\):\n Proposición.  Sea \\(\\{X_n\\}_{n=1}^\\infty\\) una sucesión de variables aleatorias independientes Poisson de parámetro \\(\\lambda\\). Entonces la variable \\(\\sum\\limits_{i=1}^n X_i\\) sigue la distribución de Poisson de parámetro \\(n\\lambda\\).\n\nDemostración\nEn primer lugar, hallemos la función característica de la distribución de Poisson de parámetro \\(\\lambda\\). Sea \\(X=Poiss(\\lambda)\\). Su función característica en un valor \\(w\\) será: \\[\n\\scriptsize{\n\\phi_X(w)=E\\left(\\mathrm{e}^{\\mathrm{i} w X}\\right)=\\sum_{k=0}^\\infty \\mathrm{e}^{i w k}\\frac{\\lambda^k}{k!}\\mathrm{e}^{-\\lambda}=\\mathrm{e}^{-\\lambda} \\sum_{k=0}^\\infty \\frac{\\left(\\lambda\\mathrm{e}^{iw}\\right)^k}{k!}=\\mathrm{e}^{-\\lambda}\\cdot \\mathrm{e}^{\\lambda\\mathrm{e}^{iw}}=\\mathrm{e}^{\\lambda \\left(\\mathrm{e}^{iw}-1\\right)}.\n}\n\\] Sea ahora la variable \\(S_n=\\sum\\limits_{i=1}^n X_i\\). Usando la proposición anterior que nos calcula la función característica de sumas de variables independientes, podemos escribir: \\[\n\\phi_{S_n}(w)=\\phi_{X_1}(w)\\cdots \\phi_{X_n}(w)=\\left(\\mathrm{e}^{\\lambda \\left(\\mathrm{e}^{iw}-1\\right)}\\right)^n =\\mathrm{e}^{n\\lambda \\left(\\mathrm{e}^{iw}-1\\right)},\n\\] función característica que corresponde a una variable de Poisson de parámetro \\(n\\lambda\\), \\(Poiss(n\\lambda)\\).\n\nUsando la proposición anterior, tenemos que la suma de variables Poisson independientes de parámetro \\(\\lambda\\) sigue una distribución Poisson de parámetro \\(n\\lambda\\). Por tanto, podemos escribir usando el corolario del Teorema Central del Límite aplicado a variables Poisson: \\[\nPoiss(n\\lambda)\\approx N(n\\lambda,\\sqrt{n\\lambda}).\n\\]\n\nEjemplo: aplicación del Teorema de Moivre-Laplace\nSea \\(X\\) una distribución binomial de parámetros \\(n=50\\) y \\(p=\\frac{1}{3}\\).\nQueremos conocer \\(P(X &lt; 15)\\) y \\(P(10\\leq X\\leq 20)\\).\nVamos a calcular las probabilidades anteriores usando el Teorema de Moivre-Laplace.\n\nLa variable \\(X\\) es aproximadamente una distribución normal \\(X_N\\) de parámetros \\(\\mu = np=\\frac{50}{3}=16.6667\\) y \\(\\sigma=\\sqrt{50\\cdot\\frac{1}{3}\\cdot \\frac{2}{3}}=3.3333\\).\nPor tanto: \\[\n\\begin{array}{rl}\nP(X&lt; 15)\n& = P(X\\leq 14) \\approx P(X_N \\leq 14)=P\\left(Z\\leq \\frac{14-16.6667}{3.3333}\\right)\\\\&\n=P(Z\\leq -0.8) = 0.2119,\\\\\n\\end{array}\n\\] \\[\n\\begin{array}{rl}\nP(10\\leq X\\leq 20)\n& \\approx P(10\\leq X_N \\leq 20) = P\\left(\\frac{10-16.6667}{3.3333}\\leq  Z\\leq \\frac{20-16.6667}{3.3333}\\right) \\\\\n& = P(-2\\leq Z\\leq 1) \\\\\n& = P(Z\\leq 1)-P(Z\\leq -2)=0.8413447-0.0227501 = 0.8186,\n\\end{array}\n\\] donde \\(Z=N(0,1)\\).\nComparemos los valores aproximados anteriores con los valores “exactos” proporcionados por R:\n\npbinom(14,50,1/3)\n\n[1] 0.2612386\n\npbinom(20,50,1/3)-pbinom(9,50,1/3)\n\n[1] 0.8613685\n\n\nTenemos errores de 0.04938 y 0.04277, respectivamente.\nAunque \\(n\\) no es pequeño, \\(n=50\\), los errores anteriores no son demasiado pequeños.\nUna razón por la que dichos errores no son pequeños es que aproximamos una distribución discreta (Binomial) cuyos valores van de 1 en 1 por una distribución normal, que es continua.\nLa corrección de continuidad de Fisher nos mejora la aproximación disminuyendo dichos errores.\n\n\n\n8.4.5 Corrección de continuidad de Fisher\nCuando aplicamos el Teorema Central del Límite y aproximamos una distribución discreta que tiene valores enteros por una normal, hemos de aplicar lo que se llama corrección de continuidad de Fisher.\nSea \\(X\\) la variable discreta que queremos aproximar y \\(X_N\\) la variable normal que nos aparece cuando aplicamos el Teorema Central del Límite. Supongamos que queremos calcular \\(P(X\\leq k)\\), para un \\(k\\) entero. Entonces debemos hacer: \\[\nP(X\\leq k)\\approx P(X_N\\leq k+0.5).\n\\]\nEs decir, para tener en cuenta el valor \\(k\\) en la aproximación \\(X_N\\) hay que sumarle la mitad entre dos valores consecutivos (0.5 si los valores son enteros) de la variable \\(X\\).\nId con cuidado, si queremos calcular \\(P(X&lt;k)\\), hay que hacer \\(P(X&lt;k) =P(X\\leq k-1)\\approx P(X_N \\leq k-1+0.5)=P(X_N\\leq k-0.5)\\).\n\nEjemplo: continuación ejemplo anterior\nSi aplicamos la continuidad de Fisher en el ejemplo anterior, obtenemos:\n\n\\[\n\\begin{array}{rl}\nP(X&lt; 15) & = P(X\\leq 14) \\approx P(X_N \\leq 14.5)=P\\left(Z\\leq \\frac{14.5-16.6667}{3.3333}\\right) =P(Z\\leq -0.65) = 0.2578,\n\\end{array}\n\\]\n\\[\n\\begin{array}{rl}\nP(10\\leq X\\leq 20) &= P(X\\leq 20)-P(X\\leq 9)\\approx P(X_N \\leq 20.5)-P(X_N\\leq 9.5) \\\\ & = P\\left(Z\\leq \\frac{20.5-16.6667}{3.3333}\\right) - P\\left(Z\\leq \\frac{9.5-16.6667}{3.3333}\\right)=  P(Z\\leq 1.15)-P(Z\\leq -2.15)\\\\ & =0.8749281-0.0157776 = 0.8592,\n\\end{array}\n\\] obteniendo unos errores de sólo 0.00339 y 0.00222, respectivamente.\n\n\n\n8.4.6 Simulación del Teorema Central del Límite\n\nEjemplo de simulación de la aproximación de una variable binomial a una distribución normal\nPara realizar la simulación anterior, consideremos una distribución binomial de parámetros \\(n=100\\) y \\(p=\\frac{1}{2}\\).\nSegún el Teorema Central del Límite, tenemos que \\[\n\\overline{X}_n=\\frac{1}{n}B\\left(n=100,p=\\frac{1}{2}\\right)\\approx N\\left(\\mu = p=\\frac{1}{2}=0.5,\\sigma=\\sqrt{\\frac{\\frac{1}{2}\\cdot \\frac{1}{2}}{100}}=0.05\\right).\n\\]\nPara ver dicha aproximación, en primer lugar vamos a generar una muestra de \\(N=1000\\) valores de una binomial de parámetros \\(n=100\\) y \\(p=\\frac{1}{2}\\) y dividiendo por \\(n=100\\), tenemos una muestra de \\(\\overline{X}_n\\):\n\nn=100\np=1/2\nsigma=p*(1-p)\nset.seed(2019)\nmuestra.binomial = rbinom(1000,n,p)\nmuestra.xnbarra = muestra.binomial/n\n\nPara ver si la aproximación funciona, dibujaremos en una misma gráfica el histograma de frecuencias relativas de la muestra anterior y la curva de la función de densidad de la distribución normal de parámetros \\(\\mu =\\frac{1}{2}\\) y \\(\\sigma = 0.05\\):\n\nhist(muestra.xnbarra,freq=FALSE,\n     breaks=seq(from=min(muestra.xnbarra)-0.1,\n                to=max(muestra.xnbarra)+0.1,by=0.01),\n     main=\"Histograma de la distribución de las medias muestrales\",\n     xlab=\"valores variable\",ylab=\"frecuencias relativas\")\nmu=p\nsigma.xnbarra=sqrt(p*(1-p)/n)\nx=seq(from=min(muestra.xnbarra),to=max(muestra.xnbarra),by=0.01)\nlines(x,dnorm(x,mu,sigma.xnbarra),col='red')\n\n\n\n\nObservamos que la aproximación es bastante buena."
  }
]